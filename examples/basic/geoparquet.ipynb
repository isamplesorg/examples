{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2edcc8d6",
   "metadata": {},
   "source": [
    "# Memory-Efficient Visualization Strategies for Large Datasets\n",
    "\n",
    "When working with 6M+ points, memory management becomes crucial. Here are several strategies to optimize Lonboard performance:\n",
    "\n",
    "## Memory-Efficient Approaches\n",
    "\n",
    "1. **Smart Sampling**: Use statistical sampling to maintain data representativeness\n",
    "2. **Spatial Decimation**: Reduce point density based on zoom level\n",
    "3. **Data Streaming**: Load data progressively based on viewport\n",
    "4. **Hierarchical Display**: Show different detail levels at different zoom levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d644cd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available sampling methods:\n",
      "  - random: '\n",
      "  - stratified: '\n",
      "  - spatial_grid: '\n",
      "  - adaptive: '\n",
      "    random_state : int\n",
      "        Random seed for reproducibility\n",
      "    \n",
      "    Returns:\n",
      "    --------\n",
      "    GeoDataFrame\n",
      "        Sampled geodataframe\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from typing import Union, Optional\n",
    "\n",
    "def smart_sample_geodataframe(\n",
    "    gdf: gpd.GeoDataFrame, \n",
    "    target_size: int = 100000,\n",
    "    method: str = 'stratified',\n",
    "    random_state: int = 42\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Intelligently sample a large GeoDataFrame to reduce memory usage while maintaining representativeness.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        The input geodataframe to sample\n",
    "    target_size : int\n",
    "        Target number of points to retain\n",
    "    method : str\n",
    "        Sampling method: 'random', 'stratified', 'spatial_grid', or 'adaptive'\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    GeoDataFrame\n",
    "        Sampled geodataframe\n",
    "    \"\"\"\n",
    "    if len(gdf) <= target_size:\n",
    "        return gdf.copy()\n",
    "    \n",
    "    print(f\"Sampling {len(gdf):,} points down to {target_size:,} ({target_size/len(gdf)*100:.1f}%)\")\n",
    "    \n",
    "    if method == 'random':\n",
    "        # Simple random sampling\n",
    "        return gdf.sample(n=target_size, random_state=random_state)\n",
    "    \n",
    "    elif method == 'stratified':\n",
    "        # Stratified sampling by source_collection to maintain proportions\n",
    "        if 'source_collection' in gdf.columns:\n",
    "            # Calculate sample sizes for each stratum\n",
    "            proportions = gdf['source_collection'].value_counts(normalize=True)\n",
    "            sample_sizes = (proportions * target_size).round().astype(int)\n",
    "            \n",
    "            # Ensure we don't exceed available data in any stratum\n",
    "            sample_sizes = sample_sizes.clip(upper=gdf['source_collection'].value_counts())\n",
    "            \n",
    "            # Sample from each stratum\n",
    "            sampled_parts = []\n",
    "            for source, size in sample_sizes.items():\n",
    "                if size > 0:\n",
    "                    stratum = gdf[gdf['source_collection'] == source]\n",
    "                    if len(stratum) > 0:\n",
    "                        sample_size = min(size, len(stratum))\n",
    "                        sampled_parts.append(stratum.sample(n=sample_size, random_state=random_state))\n",
    "            \n",
    "            return pd.concat(sampled_parts, ignore_index=True) if sampled_parts else gdf.sample(n=target_size, random_state=random_state)\n",
    "        else:\n",
    "            return gdf.sample(n=target_size, random_state=random_state)\n",
    "    \n",
    "    elif method == 'spatial_grid':\n",
    "        # Grid-based spatial sampling to ensure geographic coverage\n",
    "        bounds = gdf.total_bounds  # minx, miny, maxx, maxy\n",
    "        \n",
    "        # Calculate grid size based on target sample size\n",
    "        grid_size = int(np.sqrt(target_size))\n",
    "        x_step = (bounds[2] - bounds[0]) / grid_size\n",
    "        y_step = (bounds[3] - bounds[1]) / grid_size\n",
    "        \n",
    "        sampled_points = []\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                # Define grid cell bounds\n",
    "                minx = bounds[0] + i * x_step\n",
    "                maxx = bounds[0] + (i + 1) * x_step\n",
    "                miny = bounds[1] + j * y_step\n",
    "                maxy = bounds[1] + (j + 1) * y_step\n",
    "                \n",
    "                # Find points in this grid cell\n",
    "                mask = ((gdf.geometry.x >= minx) & (gdf.geometry.x < maxx) & \n",
    "                       (gdf.geometry.y >= miny) & (gdf.geometry.y < maxy))\n",
    "                cell_points = gdf[mask]\n",
    "                \n",
    "                # Sample one point from this cell (if any exist)\n",
    "                if len(cell_points) > 0:\n",
    "                    sampled_points.append(cell_points.sample(n=1, random_state=random_state + i*grid_size + j))\n",
    "        \n",
    "        return pd.concat(sampled_points, ignore_index=True) if sampled_points else gdf.sample(n=target_size, random_state=random_state)\n",
    "    \n",
    "    elif method == 'adaptive':\n",
    "        # Adaptive sampling: more points in dense areas, fewer in sparse areas\n",
    "        # This is a simplified version - could be much more sophisticated\n",
    "        bounds = gdf.total_bounds\n",
    "        grid_size = 50  # Use a finer grid for density calculation\n",
    "        \n",
    "        x_step = (bounds[2] - bounds[0]) / grid_size\n",
    "        y_step = (bounds[3] - bounds[1]) / grid_size\n",
    "        \n",
    "        # Calculate density for each grid cell\n",
    "        densities = np.zeros((grid_size, grid_size))\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                minx = bounds[0] + i * x_step\n",
    "                maxx = bounds[0] + (i + 1) * x_step\n",
    "                miny = bounds[1] + j * y_step\n",
    "                maxy = bounds[1] + (j + 1) * y_step\n",
    "                \n",
    "                mask = ((gdf.geometry.x >= minx) & (gdf.geometry.x < maxx) & \n",
    "                       (gdf.geometry.y >= miny) & (gdf.geometry.y < maxy))\n",
    "                densities[i, j] = mask.sum()\n",
    "        \n",
    "        # Normalize densities and use as sampling probabilities\n",
    "        total_density = densities.sum()\n",
    "        if total_density > 0:\n",
    "            # Create sampling weights based on inverse density (more samples from sparser areas)\n",
    "            weights = np.zeros(len(gdf))\n",
    "            for idx, (x, y) in enumerate(zip(gdf.geometry.x, gdf.geometry.y)):\n",
    "                i = int((x - bounds[0]) / x_step)\n",
    "                j = int((y - bounds[1]) / y_step)\n",
    "                i = min(i, grid_size - 1)  # Handle edge case\n",
    "                j = min(j, grid_size - 1)\n",
    "                weights[idx] = 1.0 / (densities[i, j] + 1)  # +1 to avoid division by zero\n",
    "            \n",
    "            # Normalize weights\n",
    "            weights = weights / weights.sum()\n",
    "            \n",
    "            # Sample based on weights\n",
    "            indices = np.random.choice(len(gdf), size=min(target_size, len(gdf)), \n",
    "                                     replace=False, p=weights)\n",
    "            return gdf.iloc[indices].copy()\n",
    "        else:\n",
    "            return gdf.sample(n=target_size, random_state=random_state)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown sampling method: {method}\")\n",
    "\n",
    "def get_memory_usage_mb(gdf: gpd.GeoDataFrame) -> float:\n",
    "    \"\"\"Calculate approximate memory usage of a GeoDataFrame in MB.\"\"\"\n",
    "    return gdf.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "\n",
    "# Example usage and comparison\n",
    "print(\"Available sampling methods:\")\n",
    "methods = ['random', 'stratified', 'spatial_grid', 'adaptive']\n",
    "for method in methods:\n",
    "    print(f\"  - {method}: {smart_sample_geodataframe.__doc__.split(method)[1].split(',')[0] if method in smart_sample_geodataframe.__doc__ else 'See docstring'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "445fbbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_zoom_adaptive_layer(\n",
    "    gdf: gpd.GeoDataFrame,\n",
    "    zoom_levels: dict = None,\n",
    "    color_map: dict = None,\n",
    "    radius_base: int = 300\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Create multiple ScatterplotLayers with different levels of detail for zoom-based display.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        The input geodataframe\n",
    "    zoom_levels : dict\n",
    "        Dictionary mapping zoom ranges to sample sizes\n",
    "        e.g., {(0, 3): 1000, (4, 6): 10000, (7, 18): 100000}\n",
    "    color_map : dict\n",
    "        Color mapping for different categories\n",
    "    radius_base : int\n",
    "        Base radius for points\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with zoom ranges as keys and layers as values\n",
    "    \"\"\"\n",
    "    if zoom_levels is None:\n",
    "        zoom_levels = {\n",
    "            (0, 3): 1000,    # World view: 1K points\n",
    "            (4, 6): 10000,   # Continental view: 10K points  \n",
    "            (7, 9): 50000,   # Regional view: 50K points\n",
    "            (10, 18): 200000  # Local view: 200K points\n",
    "        }\n",
    "    \n",
    "    layers = {}\n",
    "    \n",
    "    for (min_zoom, max_zoom), target_size in zoom_levels.items():\n",
    "        print(f\"Creating layer for zoom {min_zoom}-{max_zoom} with {target_size:,} points...\")\n",
    "        \n",
    "        # Sample the data for this zoom level\n",
    "        if len(gdf) > target_size:\n",
    "            sampled_gdf = smart_sample_geodataframe(gdf, target_size, method='stratified')\n",
    "        else:\n",
    "            sampled_gdf = gdf.copy()\n",
    "        \n",
    "        # Create colors if color_map is provided\n",
    "        if color_map:\n",
    "            colors = create_color_map(sampled_gdf, color_map)\n",
    "        else:\n",
    "            colors = [100, 150, 200, 255]  # Default blue\n",
    "        \n",
    "        # Adjust radius based on zoom level (smaller points for higher zoom)\n",
    "        radius = radius_base * (1 + (max_zoom - min_zoom) * 0.1)\n",
    "        \n",
    "        # Create the layer\n",
    "        from lonboard import ScatterplotLayer\n",
    "        layer = ScatterplotLayer.from_geopandas(\n",
    "            sampled_gdf,\n",
    "            get_fill_color=colors,\n",
    "            get_radius=radius,\n",
    "            radius_units='meters',\n",
    "            pickable=True,\n",
    "            min_zoom=min_zoom,\n",
    "            max_zoom=max_zoom\n",
    "        )\n",
    "        \n",
    "        layers[(min_zoom, max_zoom)] = {\n",
    "            'layer': layer,\n",
    "            'data': sampled_gdf,\n",
    "            'point_count': len(sampled_gdf),\n",
    "            'memory_mb': get_memory_usage_mb(sampled_gdf)\n",
    "        }\n",
    "    \n",
    "    return layers\n",
    "\n",
    "def monitor_memory_usage():\n",
    "    \"\"\"Monitor current memory usage of the Python process.\"\"\"\n",
    "    import psutil\n",
    "    import os\n",
    "    \n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    \n",
    "    print(f\"Current memory usage:\")\n",
    "    print(f\"  RSS (Resident Set Size): {memory_info.rss / 1024 / 1024:.1f} MB\")\n",
    "    print(f\"  VMS (Virtual Memory Size): {memory_info.vms / 1024 / 1024:.1f} MB\")\n",
    "    \n",
    "    # Get system memory info\n",
    "    system_memory = psutil.virtual_memory()\n",
    "    print(f\"System memory:\")\n",
    "    print(f\"  Total: {system_memory.total / 1024 / 1024 / 1024:.1f} GB\")\n",
    "    print(f\"  Available: {system_memory.available / 1024 / 1024 / 1024:.1f} GB\")\n",
    "    print(f\"  Used: {system_memory.percent:.1f}%\")\n",
    "\n",
    "def optimize_gdf_memory(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Optimize memory usage of a GeoDataFrame by converting data types.\n",
    "    \"\"\"\n",
    "    gdf_optimized = gdf.copy()\n",
    "    \n",
    "    # Convert string columns to categorical where beneficial\n",
    "    for col in gdf_optimized.select_dtypes(include=['object']).columns:\n",
    "        if col != 'geometry':  # Don't convert geometry column\n",
    "            unique_ratio = gdf_optimized[col].nunique() / len(gdf_optimized)\n",
    "            if unique_ratio < 0.5:  # If less than 50% unique values, convert to categorical\n",
    "                gdf_optimized[col] = gdf_optimized[col].astype('category')\n",
    "                print(f\"Converted {col} to categorical (unique ratio: {unique_ratio:.3f})\")\n",
    "    \n",
    "    # Downcast numeric types where possible\n",
    "    for col in gdf_optimized.select_dtypes(include=['int64']).columns:\n",
    "        gdf_optimized[col] = pd.to_numeric(gdf_optimized[col], downcast='integer')\n",
    "    \n",
    "    for col in gdf_optimized.select_dtypes(include=['float64']).columns:\n",
    "        gdf_optimized[col] = pd.to_numeric(gdf_optimized[col], downcast='float')\n",
    "    \n",
    "    original_memory = get_memory_usage_mb(gdf)\n",
    "    optimized_memory = get_memory_usage_mb(gdf_optimized)\n",
    "    \n",
    "    print(f\"Memory optimization:\")\n",
    "    print(f\"  Original: {original_memory:.1f} MB\")\n",
    "    print(f\"  Optimized: {optimized_memory:.1f} MB\")\n",
    "    print(f\"  Savings: {original_memory - optimized_memory:.1f} MB ({(1 - optimized_memory/original_memory)*100:.1f}%)\")\n",
    "    \n",
    "    return gdf_optimized\n",
    "\n",
    "# Install psutil if not available\n",
    "try:\n",
    "    import psutil\n",
    "except ImportError:\n",
    "    print(\"Installing psutil for memory monitoring...\")\n",
    "    import subprocess\n",
    "    subprocess.run(['pip', 'install', 'psutil'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0460c268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Memory Usage Analysis ===\n",
      "Current memory usage:\n",
      "  RSS (Resident Set Size): 142.6 MB\n",
      "  VMS (Virtual Memory Size): 402881.2 MB\n",
      "System memory:\n",
      "  Total: 128.0 GB\n",
      "  Available: 34.8 GB\n",
      "  Used: 72.8%\n",
      "\n",
      "=== Dataset Information ===\n",
      "Please run the data loading cells first to create gdf_valid\n",
      "\n",
      "=== Memory Management Tips ===\n",
      "1. Monitor memory usage regularly with monitor_memory_usage()\n",
      "2. Use sampling for initial exploration, full dataset for final analysis\n",
      "3. Consider using DuckDB/Ibis for aggregations before visualization\n",
      "4. Clear unused variables with del and gc.collect()\n",
      "5. For production, consider pre-processing data at different zoom levels\n"
     ]
    }
   ],
   "source": [
    "# Practical example: Apply memory-efficient techniques to your dataset\n",
    "\n",
    "print(\"=== Memory Usage Analysis ===\")\n",
    "monitor_memory_usage()\n",
    "\n",
    "print(\"\\n=== Dataset Information ===\")\n",
    "# Check current dataset size (assuming you have gdf_valid from earlier)\n",
    "if 'gdf_valid' in locals():\n",
    "    print(f\"Current dataset: {len(gdf_valid):,} points\")\n",
    "    print(f\"Memory usage: {get_memory_usage_mb(gdf_valid):.1f} MB\")\n",
    "    \n",
    "    # Optimize memory usage\n",
    "    print(\"\\n=== Memory Optimization ===\")\n",
    "    gdf_optimized = optimize_gdf_memory(gdf_valid)\n",
    "    \n",
    "    # Test different sampling strategies\n",
    "    print(\"\\n=== Sampling Strategy Comparison ===\")\n",
    "    target_sizes = [10000, 50000, 100000]\n",
    "    methods = ['random', 'stratified', 'spatial_grid']\n",
    "    \n",
    "    sampling_results = {}\n",
    "    \n",
    "    for target_size in target_sizes:\n",
    "        print(f\"\\nTarget size: {target_size:,} points\")\n",
    "        for method in methods:\n",
    "            try:\n",
    "                sampled = smart_sample_geodataframe(gdf_optimized, target_size, method=method)\n",
    "                memory_mb = get_memory_usage_mb(sampled)\n",
    "                \n",
    "                # Check source collection distribution\n",
    "                if 'source_collection' in sampled.columns:\n",
    "                    dist = sampled['source_collection'].value_counts(normalize=True)\n",
    "                    print(f\"  {method:12s}: {len(sampled):6,} points, {memory_mb:5.1f} MB, collections: {len(dist)}\")\n",
    "                else:\n",
    "                    print(f\"  {method:12s}: {len(sampled):6,} points, {memory_mb:5.1f} MB\")\n",
    "                \n",
    "                sampling_results[(target_size, method)] = {\n",
    "                    'data': sampled,\n",
    "                    'memory_mb': memory_mb,\n",
    "                    'point_count': len(sampled)\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"  {method:12s}: Error - {str(e)}\")\n",
    "    \n",
    "    print(\"\\n=== Recommended Approach ===\")\n",
    "    print(\"For 6M+ points with Lonboard:\")\n",
    "    print(\"1. Use stratified sampling to maintain data representativeness\")\n",
    "    print(\"2. Start with 50K-100K points for interactive exploration\")\n",
    "    print(\"3. Use zoom-adaptive layers for better performance\")\n",
    "    print(\"4. Consider spatial decimation for very dense areas\")\n",
    "    \n",
    "    # Demonstrate zoom-adaptive approach\n",
    "    print(\"\\n=== Creating Zoom-Adaptive Layers ===\")\n",
    "    # Use a smaller subset for demonstration\n",
    "    demo_data = smart_sample_geodataframe(gdf_optimized, 25000, method='stratified')\n",
    "    \n",
    "    # Create zoom-adaptive layers\n",
    "    zoom_layers = create_zoom_adaptive_layer(\n",
    "        demo_data,\n",
    "        zoom_levels={\n",
    "            (0, 4): 1000,   # World view\n",
    "            (5, 8): 5000,   # Regional view  \n",
    "            (9, 18): 15000  # Local view\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"\\nZoom layers created:\")\n",
    "    total_memory = 0\n",
    "    for (min_zoom, max_zoom), layer_info in zoom_layers.items():\n",
    "        memory = layer_info['memory_mb']\n",
    "        total_memory += memory\n",
    "        print(f\"  Zoom {min_zoom:2d}-{max_zoom:2d}: {layer_info['point_count']:5,} points, {memory:5.1f} MB\")\n",
    "    \n",
    "    print(f\"Total memory for all zoom layers: {total_memory:.1f} MB\")\n",
    "    \n",
    "else:\n",
    "    print(\"Please run the data loading cells first to create gdf_valid\")\n",
    "\n",
    "print(\"\\n=== Memory Management Tips ===\")\n",
    "print(\"1. Monitor memory usage regularly with monitor_memory_usage()\")\n",
    "print(\"2. Use sampling for initial exploration, full dataset for final analysis\")\n",
    "print(\"3. Consider using DuckDB/Ibis for aggregations before visualization\")\n",
    "print(\"4. Clear unused variables with del and gc.collect()\")\n",
    "print(\"5. For production, consider pre-processing data at different zoom levels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a4769d",
   "metadata": {},
   "source": [
    "\n",
    "# Visualizing iSamples Data with Lonboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158479cc",
   "metadata": {},
   "source": [
    "\n",
    "This notebook demonstrates how to use the `lonboard` library to visualize iSamples data. It covers loading the data, cleaning it, and creating an interactive map with various controls.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaf43d4",
   "metadata": {},
   "source": [
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup and Imports](#Setup-and-Imports)\n",
    "2. [Load Data](#Load-Data)\n",
    "3. [Data Exploration with Ibis](#Data-Exploration-with-Ibis)\n",
    "4. [Data Cleaning and Preparation](#Data-Cleaning-and-Preparation)\n",
    "5. [Interactive Map](#Interactive-Map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eea41027-29e5-4a71-868f-dd2853d24379",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import shapely\n",
    "from palettable.colorbrewer.diverging import BrBG_10\n",
    "# from sidecar import Sidecar\n",
    "\n",
    "from lonboard import Map, ScatterplotLayer\n",
    "from lonboard.colormap import apply_continuous_cmap\n",
    "\n",
    "import ibis\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import Layout, Button, HBox, VBox, HTML\n",
    "from ipywidgets import Output, HTMLMath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e7d3a8",
   "metadata": {},
   "source": [
    "\n",
    "<a id='Setup-and-Imports'></a>\n",
    "## 1. Setup and Imports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dc6442",
   "metadata": {},
   "source": [
    "Two files to analyze:\n",
    "\n",
    "* [iSamples Complete Export Dataset - April 2025](https://zenodo.org/records/15278211)\n",
    "\n",
    "* [Open Context Database SQL Dump and Parquet Exports](https://zenodo.org/records/15732000) -- [https://zenodo.org/records/15732000](https://zenodo.org/records/15732000)Â \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a1acf75-46a8-4353-a0b2-71e123412eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_path = Path(\"/Users/raymondyee/Data/iSample/2025_02_20_10_30_49/isamples_export_2025_02_20_10_30_49_geo.parquet\")\n",
    "# local_path = Path(\"/Users/raymondyee/Data/iSample/OPENCONTEXT.parquet\")\n",
    "# local_path = Path(\"/Users/raymondyee/Data/iSample/pqg_refining/oc_isamples_pqg.parquet\")\n",
    "# LOCAL_PATH = \"isamples_export_2025_04_21_16_23_46_geo.parquet\"\n",
    "LOCAL_PATH = \"/Users/raymondyee/Data/iSample/2025_04_21_16_23_46/isamples_export_2025_04_21_16_23_46_geo.parquet\"\n",
    "local_path = Path(LOCAL_PATH)\n",
    "if not local_path.exists():\n",
    "    remote_url = \"https://zenodo.org/records/15278211/files/isamples_export_2025_04_21_16_23_46_geo.parquet\"\n",
    "    # retrieve the file and store to local_path\n",
    "    response = requests.get(remote_url)\n",
    "    with open(local_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c3b71b",
   "metadata": {},
   "source": [
    "\n",
    "<a id='Load-Data'></a>\n",
    "## 2. Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2194a64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local file: /Users/raymondyee/Data/iSample/2025_04_21_16_23_46/isamples_export_2025_04_21_16_23_46_geo.parquet\n",
      "File size: 283.28 MB\n"
     ]
    }
   ],
   "source": [
    "# write out some info about the local file\n",
    "# how big is it?\n",
    "print(f\"Local file: {local_path}\")\n",
    "print(f\"File size: {local_path.stat().st_size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c7661a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in geoparquet file: ['sample_identifier', '@id', 'label', 'description', 'source_collection', 'has_sample_object_type', 'has_material_category', 'has_context_category', 'informal_classification', 'keywords', 'produced_by', 'last_modified_time', 'curation', 'registrant', 'related_resource', 'sampling_purpose', 'sample_location_longitude', 'sample_location_latitude', 'geometry']\n"
     ]
    }
   ],
   "source": [
    "# any quick way to read off the columns for a geoparquet file without having to load the whole thing?\n",
    "import pyarrow.parquet as pq\n",
    "schema = pq.read_schema(local_path)\n",
    "print(f\"Columns in geoparquet file: {schema.names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c80fb35",
   "metadata": {},
   "source": [
    "\n",
    "A GeoParquet file contains standard Parquet data types as well as a special geometry column. Here's a breakdown of what you can expect:\n",
    "\n",
    "*   **Standard Data Types**: These are the fundamental data types from the Apache Parquet format. They include:\n",
    "    *   `BOOLEAN`: True or false values.\n",
    "    *   `INT32`, `INT64`: 32-bit and 64-bit signed integers.\n",
    "    *   `FLOAT`, `DOUBLE`: 32-bit and 64-bit floating-point numbers.\n",
    "    *   `BYTE_ARRAY`: Variable-length byte arrays, which can be used to store strings (with UTF-8 encoding), binary data, or complex types.\n",
    "    *   `FIXED_LEN_BYTE_ARRAY`: Fixed-length byte arrays.\n",
    "\n",
    "*   **Logical Types**: These are annotations that add semantic meaning to the underlying primitive types. For example, a `BYTE_ARRAY` can be annotated as a `STRING` or `JSON`. Common logical types include:\n",
    "    *   `STRING`: UTF-8 encoded character strings.\n",
    "    *   `DECIMAL`: Arbitrary-precision signed decimal numbers.\n",
    "    *   `DATE`, `TIME`, `TIMESTAMP`: Date and time values with various precisions.\n",
    "    *   `UUID`: Universally unique identifiers.\n",
    "\n",
    "*   **Geometry Column**: This is the defining feature of a GeoParquet file. It's a column that stores geographic features in a binary format, typically Well-Known Binary (WKB). This column is what allows geospatial libraries like `geopandas` to interpret the data as points, lines, or polygons. The schema metadata will specify which column is the geometry column.\n",
    "\n",
    "*   **GeoParquet Metadata**: In addition to the standard Parquet metadata, a GeoParquet file includes specific metadata that describes the geospatial information. This includes:\n",
    "    *   The name of the geometry column.\n",
    "    *   The Coordinate Reference System (CRS) of the geometries (e.g., WGS84).\n",
    "    *   The bounding box of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2ec8524-f36f-4ec0-8d3e-04b94a3d3d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = ['sample_identifier',\n",
    " 'label',\n",
    " 'description',\n",
    " 'source_collection',\n",
    " 'has_sample_object_type',\n",
    " 'has_material_category',\n",
    " 'has_context_category',\n",
    " 'informal_classification',\n",
    " 'keywords',\n",
    " 'produced_by',\n",
    " 'curation',\n",
    " 'registrant',\n",
    " 'related_resource',\n",
    " 'sampling_purpose',\n",
    " 'sample_location_longitude',\n",
    " 'sample_location_latitude',\n",
    " 'geometry']\n",
    "\n",
    "# read a subset of columns\n",
    "columns = ['sample_identifier', 'source_collection', 'geometry']\n",
    "# columns = all_columns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60c33c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'@id', 'last_modified_time'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(schema.names) - set(all_columns)  # what columns are not in the subset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90535a62",
   "metadata": {},
   "source": [
    "\n",
    "The fields `@id` and `last_modified_names` are indeed part of the Parquet file's schema, which is why `schema.names` includes them. Here's a breakdown of what they likely represent:\n",
    "\n",
    "*   **`@id`**: This is a convention often used in Linked Data formats like JSON-LD. It typically represents a unique identifier for each record, often in the form of a URI (Uniform Resource Identifier). This allows each sample in your dataset to be uniquely referenced in a global context, which is very useful for data integration and interoperability. Given that one of the data sources is Open Context, which heavily uses Linked Open Data principles, this is a very likely explanation.\n",
    "\n",
    "*   **`last_modified_names`**: This field is less standard, but it's almost certainly related to data provenance and tracking. It likely stores information about when the data for that specific record was last modified. This is crucial for understanding the version and history of the data.\n",
    "\n",
    "In short, while they may not be \"data\" columns in the same way as `latitude` or `description`, they are important metadata fields that have been stored as columns within the Parquet file's schema. `pyarrow.read_schema` gives you a raw look at this schema, so it includes everything defined at that level. When you load the data with `geopandas` and specify a subset of columns, these metadata columns are simply ignored, but they are still present in the file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35bb9e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "if local_path.exists():\n",
    "    gdf = gpd.read_parquet(local_path, columns=columns)\n",
    "    # Get a sample if the dataset is too large\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc092c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that the columns are as expected\n",
    "assert set(gdf.columns) == set(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1178957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  source_collection  source_collection_count\n",
      "0             GEOME                   605554\n",
      "1       SMITHSONIAN                   322161\n",
      "2       OPENCONTEXT                  1064831\n",
      "3             SESAR                  4688386\n"
     ]
    }
   ],
   "source": [
    "# use ibis to read the parquet file and compute some basic stats\n",
    "\n",
    "table = ibis.read_parquet(local_path)\n",
    "result = table[\"source_collection\"].value_counts().execute()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdadf31d",
   "metadata": {},
   "source": [
    "\n",
    "<a id='Data-Exploration-with-Ibis'></a>\n",
    "## 3. Data Exploration with Ibis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd39600",
   "metadata": {},
   "source": [
    "Ibis uses DuckDB as its default backend for working with Parquet files, which makes it really efficient and convenient for handling large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b7631d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('sample_identifier', '@id', 'label', 'description', 'source_collection', 'has_sample_object_type', 'has_material_category', 'has_context_category', 'informal_classification', 'keywords', 'produced_by', 'last_modified_time', 'curation', 'registrant', 'related_resource', 'sampling_purpose', 'sample_location_longitude', 'sample_location_latitude', 'geometry')\n",
      "ibis.Schema {\n",
      "  sample_identifier          string\n",
      "  @id                        string\n",
      "  label                      string\n",
      "  description                string\n",
      "  source_collection          string\n",
      "  has_sample_object_type     array<struct<identifier: string>>\n",
      "  has_material_category      array<struct<identifier: string>>\n",
      "  has_context_category       array<struct<identifier: string>>\n",
      "  informal_classification    array<string>\n",
      "  keywords                   array<struct<keyword: string>>\n",
      "  produced_by                struct<description: string, has_feature_of_interest: string, identifier: string, label: string, responsibility: array<struct<name: string, role: string>>, result_time: string, sampling_site: struct<description: string, label: string, place_name: array<string>, sample_location: struct<elevation: float64, latitude: float64, longitude: float64>>>\n",
      "  last_modified_time         timestamp('UTC', 6)\n",
      "  curation                   struct<access_constraints: array<string>, curation_location: string, description: string, label: string, responsibility: array<struct<name: string, role: string>>>\n",
      "  registrant                 struct<name: string>\n",
      "  related_resource           array<struct<target: string>>\n",
      "  sampling_purpose           array<string>\n",
      "  sample_location_longitude  float64\n",
      "  sample_location_latitude   float64\n",
      "  geometry                   binary\n",
      "}\n",
      "6680932\n",
      "    sample_identifier                     @id label  \\\n",
      "0  ark:/21547/DSz2757  metadata/21547/DSz2757   757   \n",
      "1  ark:/21547/DSz2779  metadata/21547/DSz2779   779   \n",
      "2  ark:/21547/DSz2806  metadata/21547/DSz2806   806   \n",
      "3  ark:/21547/DSz2807  metadata/21547/DSz2807   807   \n",
      "4  ark:/21547/DSz2759  metadata/21547/DSz2759   759   \n",
      "\n",
      "                        description source_collection  \\\n",
      "0  basisOfRecord: PreservedSpecimen             GEOME   \n",
      "1  basisOfRecord: PreservedSpecimen             GEOME   \n",
      "2  basisOfRecord: PreservedSpecimen             GEOME   \n",
      "3  basisOfRecord: PreservedSpecimen             GEOME   \n",
      "4  basisOfRecord: PreservedSpecimen             GEOME   \n",
      "\n",
      "                              has_sample_object_type  \\\n",
      "0  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "1  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "2  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "3  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "4  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "\n",
      "                               has_material_category  \\\n",
      "0  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "1  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "2  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "3  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "4  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "\n",
      "                                has_context_category informal_classification  \\\n",
      "0  [{'identifier': 'https://w3id.org/isample/biol...    [Taricha, granulosa]   \n",
      "1  [{'identifier': 'https://w3id.org/isample/biol...    [Taricha, granulosa]   \n",
      "2  [{'identifier': 'https://w3id.org/isample/biol...    [Taricha, granulosa]   \n",
      "3  [{'identifier': 'https://w3id.org/isample/biol...    [Taricha, granulosa]   \n",
      "4  [{'identifier': 'https://w3id.org/isample/biol...    [Taricha, granulosa]   \n",
      "\n",
      "                                          keywords  \\\n",
      "0  [{'keyword': 'California'}, {'keyword': 'USA'}]   \n",
      "1  [{'keyword': 'California'}, {'keyword': 'USA'}]   \n",
      "2  [{'keyword': 'California'}, {'keyword': 'USA'}]   \n",
      "3  [{'keyword': 'California'}, {'keyword': 'USA'}]   \n",
      "4  [{'keyword': 'California'}, {'keyword': 'USA'}]   \n",
      "\n",
      "                                         produced_by  \\\n",
      "0  {'description': 'expeditionCode: newts | proje...   \n",
      "1  {'description': 'expeditionCode: newts | proje...   \n",
      "2  {'description': 'expeditionCode: newts | proje...   \n",
      "3  {'description': 'expeditionCode: newts | proje...   \n",
      "4  {'description': 'expeditionCode: newts | proje...   \n",
      "\n",
      "         last_modified_time curation registrant related_resource  \\\n",
      "0 1894-01-01 00:00:00+00:00     None       None             None   \n",
      "1 1893-01-01 00:00:00+00:00     None       None             None   \n",
      "2 1893-01-01 00:00:00+00:00     None       None             None   \n",
      "3 1893-01-01 00:00:00+00:00     None       None             None   \n",
      "4 1894-01-01 00:00:00+00:00     None       None             None   \n",
      "\n",
      "  sampling_purpose  sample_location_longitude  sample_location_latitude  \\\n",
      "0             None                -122.578610                 38.578888   \n",
      "1             None                -122.373055                 37.385277   \n",
      "2             None                -122.117050                 37.365490   \n",
      "3             None                -122.117050                 37.365490   \n",
      "4             None                -122.578610                 38.578888   \n",
      "\n",
      "                                            geometry  \n",
      "0  b'\\x01\\x01\\x00\\x00\\x00\\xde\\xc8<\\xf2\\x07\\xa5^\\x...  \n",
      "1  b'\\x01\\x01\\x00\\x00\\x00\\xfe&\\x14\"\\xe0\\x97^\\xc0T...  \n",
      "2  b'\\x01\\x01\\x00\\x00\\x00\\xcc\\x7fH\\xbf}\\x87^\\xc0\\...  \n",
      "3  b'\\x01\\x01\\x00\\x00\\x00\\xcc\\x7fH\\xbf}\\x87^\\xc0\\...  \n",
      "4  b'\\x01\\x01\\x00\\x00\\x00\\xde\\xc8<\\xf2\\x07\\xa5^\\x...  \n"
     ]
    }
   ],
   "source": [
    "# Get all column names\n",
    "print(table.columns)\n",
    "\n",
    "# Display table schema/structure with data types\n",
    "print(table.schema())\n",
    "\n",
    "# Get number of rows\n",
    "print(table.count().execute())\n",
    "\n",
    "# Preview first few rows (similar to pandas head())\n",
    "print(table.limit(5).execute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "651c32c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source collections:\n",
      "  source_collection  source_collection_count\n",
      "0             SESAR                  4688386\n",
      "1       SMITHSONIAN                   322161\n",
      "2             GEOME                   605554\n",
      "3       OPENCONTEXT                  1064831\n",
      "Sample object types:\n",
      "                              has_sample_object_type  \\\n",
      "0  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "1  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "2  [{'identifier': 'https://w3id.org/isample/open...   \n",
      "3  [{'identifier': 'https://w3id.org/isample/open...   \n",
      "4  [{'identifier': 'https://w3id.org/isample/open...   \n",
      "5  [{'identifier': 'https://w3id.org/isample/open...   \n",
      "6  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "7  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "8  [{'identifier': 'https://w3id.org/isample/open...   \n",
      "9  [{'identifier': 'https://w3id.org/isample/open...   \n",
      "\n",
      "   has_sample_object_type_count  \n",
      "0                           645  \n",
      "1                           230  \n",
      "2                          4659  \n",
      "3                            20  \n",
      "4                         10787  \n",
      "5                            26  \n",
      "6                            14  \n",
      "7                             1  \n",
      "8                        436015  \n",
      "9                          4038  \n",
      "Material categories:\n",
      "                               has_material_category  \\\n",
      "0  [{'identifier': 'https://w3id.org/isample/open...   \n",
      "1  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "2  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "3  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "4  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "5  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "6  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "7  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "8  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "9  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "\n",
      "   has_material_category_count  \n",
      "0                          223  \n",
      "1                       210546  \n",
      "2                       422889  \n",
      "3                          563  \n",
      "4                           25  \n",
      "5                            2  \n",
      "6                          160  \n",
      "7                          173  \n",
      "8                        12173  \n",
      "9                           26  \n",
      "Null counts per column:\n",
      "sample_identifier: 0\n",
      "@id: 0\n",
      "label: 3170\n",
      "description: 5074323\n",
      "source_collection: 0\n",
      "has_sample_object_type: 0\n",
      "has_material_category: 8333\n",
      "has_context_category: 148100\n",
      "informal_classification: 1448922\n",
      "keywords: 157442\n",
      "produced_by: 326761\n",
      "last_modified_time: 0\n",
      "curation: 5960678\n",
      "registrant: 834356\n",
      "related_resource: 6179013\n",
      "sampling_purpose: 6419244\n",
      "sample_location_longitude: 700650\n",
      "sample_location_latitude: 700650\n",
      "geometry: 0\n"
     ]
    }
   ],
   "source": [
    "# Value counts for categorical columns\n",
    "print(\"Source collections:\")\n",
    "print(table[\"source_collection\"].value_counts().execute())\n",
    "\n",
    "print(\"Sample object types:\")\n",
    "print(table[\"has_sample_object_type\"].value_counts().limit(10).execute())\n",
    "\n",
    "print(\"Material categories:\")\n",
    "print(table[\"has_material_category\"].value_counts().limit(10).execute())\n",
    "\n",
    "# Check for null values in important columns\n",
    "null_counts = {col: table[col].isnull().sum().execute() for col in table.columns}\n",
    "print(\"Null counts per column:\")\n",
    "for col, count in null_counts.items():\n",
    "    print(f\"{col}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d48f3b",
   "metadata": {},
   "source": [
    "\n",
    "### Column Analysis\n",
    "\n",
    "Here is a breakdown of the columns in the dataset, their data types as interpreted by Ibis, and some notes on their content.\n",
    "\n",
    "| Column Name | Data Type | Notes |\n",
    "|---|---|---|\n",
    "| `sample_identifier` | `string` | Unique identifier for the sample. |\n",
    "| `label` | `string` | A human-readable label for the sample. |\n",
    "| `description` | `string` | A description of the sample. |\n",
    "| `source_collection` | `string` | The collection that the sample belongs to (e.g., SESAR, OPENCONTEXT). |\n",
    "| `has_sample_object_type` | `string` | The type of object that was sampled (e.g., 'Core', 'Individual Sample'). |\n",
    "| `has_material_category` | `string` | The category of material that the sample is composed of (e.g., 'Rock', 'Sediment'). |\n",
    "| `has_context_category` | `string` | The environmental context from which the sample was taken (e.g., 'Marine', 'Terrestrial'). |\n",
    "| `informal_classification` | `string` | An informal classification of the sample. |\n",
    "| `keywords` | `string` | Keywords associated with the sample. |\n",
    "| `produced_by` | `string` | Information about who produced the data. |\n",
    "| `curation` | `string` | Information about the curation of the sample. |\n",
    "| `registrant` | `string` | The person or organization that registered the sample. |\n",
    "| `related_resource` | `string` | Links to related resources. |\n",
    "| `sampling_purpose` | `string` | The purpose for which the sample was collected. |\n",
    "| `sample_location_longitude` | `float64` | The longitude of the sample location. |\n",
    "| `sample_location_latitude` | `float64` | The latitude of the sample location. |\n",
    "| `geometry` | `geospatial` | The geographic coordinates of the sample, stored in WKB format. This is the primary geometry column. |\n",
    "| `@id` | `string` | A unique Linked Data identifier (URI) for the record. |\n",
    "| `last_modified_timestamp` | `string` | Timestamp of when the record was last modified. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6afb5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_identifier</th>\n",
       "      <th>@id</th>\n",
       "      <th>label</th>\n",
       "      <th>description</th>\n",
       "      <th>source_collection</th>\n",
       "      <th>has_sample_object_type</th>\n",
       "      <th>has_material_category</th>\n",
       "      <th>has_context_category</th>\n",
       "      <th>informal_classification</th>\n",
       "      <th>keywords</th>\n",
       "      <th>produced_by</th>\n",
       "      <th>last_modified_time</th>\n",
       "      <th>curation</th>\n",
       "      <th>registrant</th>\n",
       "      <th>related_resource</th>\n",
       "      <th>sampling_purpose</th>\n",
       "      <th>sample_location_longitude</th>\n",
       "      <th>sample_location_latitude</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ark:/21547/DSz2757</td>\n",
       "      <td>metadata/21547/DSz2757</td>\n",
       "      <td>757</td>\n",
       "      <td>basisOfRecord: PreservedSpecimen</td>\n",
       "      <td>GEOME</td>\n",
       "      <td>[{'identifier': 'https://w3id.org/isample/voca...</td>\n",
       "      <td>[{'identifier': 'https://w3id.org/isample/voca...</td>\n",
       "      <td>[{'identifier': 'https://w3id.org/isample/biol...</td>\n",
       "      <td>[Taricha, granulosa]</td>\n",
       "      <td>[{'keyword': 'California'}, {'keyword': 'USA'}]</td>\n",
       "      <td>{'description': 'expeditionCode: newts | proje...</td>\n",
       "      <td>1894-01-01 00:00:00+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>-122.57861</td>\n",
       "      <td>38.578888</td>\n",
       "      <td>b'\\x01\\x01\\x00\\x00\\x00\\xde\\xc8&lt;\\xf2\\x07\\xa5^\\x...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sample_identifier                     @id label  \\\n",
       "0  ark:/21547/DSz2757  metadata/21547/DSz2757   757   \n",
       "\n",
       "                        description source_collection  \\\n",
       "0  basisOfRecord: PreservedSpecimen             GEOME   \n",
       "\n",
       "                              has_sample_object_type  \\\n",
       "0  [{'identifier': 'https://w3id.org/isample/voca...   \n",
       "\n",
       "                               has_material_category  \\\n",
       "0  [{'identifier': 'https://w3id.org/isample/voca...   \n",
       "\n",
       "                                has_context_category informal_classification  \\\n",
       "0  [{'identifier': 'https://w3id.org/isample/biol...    [Taricha, granulosa]   \n",
       "\n",
       "                                          keywords  \\\n",
       "0  [{'keyword': 'California'}, {'keyword': 'USA'}]   \n",
       "\n",
       "                                         produced_by  \\\n",
       "0  {'description': 'expeditionCode: newts | proje...   \n",
       "\n",
       "         last_modified_time curation registrant related_resource  \\\n",
       "0 1894-01-01 00:00:00+00:00     None       None             None   \n",
       "\n",
       "  sampling_purpose  sample_location_longitude  sample_location_latitude  \\\n",
       "0             None                 -122.57861                 38.578888   \n",
       "\n",
       "                                            geometry  \n",
       "0  b'\\x01\\x01\\x00\\x00\\x00\\xde\\xc8<\\xf2\\x07\\xa5^\\x...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "local_path\n",
    "# pull out the first 100 rows and convert to a geopandas dataframe\n",
    "k = table.limit(100).to_pandas()\n",
    "# pandas\n",
    "k[k['sample_identifier'] == 'ark:/21547/DSz2757']\n",
    "\n",
    "# how to do this using ibis?\n",
    "\n",
    "# table[table['sample_identifier'] == 'ark:/21547/DSz275']\n",
    "table.filter(table['sample_identifier'] == 'ark:/21547/DSz2757').execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fea2ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AthenaCompiler',\n",
       " 'BigQueryCompiler',\n",
       " 'ClickHouseCompiler',\n",
       " 'DataFusionCompiler',\n",
       " 'DatabricksCompiler',\n",
       " 'DruidCompiler',\n",
       " 'DuckDBCompiler',\n",
       " 'ExasolCompiler',\n",
       " 'FlinkCompiler',\n",
       " 'ImpalaCompiler',\n",
       " 'MSSQLCompiler',\n",
       " 'MySQLCompiler',\n",
       " 'OracleCompiler',\n",
       " 'PostgresCompiler',\n",
       " 'PySparkCompiler',\n",
       " 'RisingWaveCompiler',\n",
       " 'SQLiteCompiler',\n",
       " 'SnowflakeCompiler',\n",
       " 'TrinoCompiler',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'annotations',\n",
       " 'athena',\n",
       " 'base',\n",
       " 'bigquery',\n",
       " 'clickhouse',\n",
       " 'databricks',\n",
       " 'datafusion',\n",
       " 'druid',\n",
       " 'duckdb',\n",
       " 'exasol',\n",
       " 'flink',\n",
       " 'impala',\n",
       " 'mssql',\n",
       " 'mysql',\n",
       " 'oracle',\n",
       " 'postgres',\n",
       " 'pyspark',\n",
       " 'risingwave',\n",
       " 'snowflake',\n",
       " 'sqlite',\n",
       " 'trino']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(ibis.backends.sql.compilers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0364b792",
   "metadata": {},
   "source": [
    "### Querying Nested Data: Pandas vs. Ibis\n",
    "\n",
    "When working with columns that contain nested, JSON-like data (such as dictionaries or structs), both pandas and Ibis provide powerful tools for querying. However, their approaches and the underlying performance can differ significantly.\n",
    "\n",
    "#### The Pandas Approach\n",
    "\n",
    "Let's assume we have a pandas DataFrame `k` with a column `produced_by` that contains dictionaries.\n",
    "\n",
    "**1. The `apply` method (less idiomatic):**\n",
    "\n",
    "A common but often inefficient approach is to use `apply` with a `lambda` function. This works but can be slow on large datasets as it's not a vectorized operation.\n",
    "\n",
    "```python\n",
    "k[k['produced_by'].apply(lambda x: x['identifier'] == 'ark:/21547/DSz2757' if x is not None else False)]\n",
    "```\n",
    "\n",
    "**2. The `.str.get()` accessor (idiomatic and fast):**\n",
    "\n",
    "A much more \"pandasonic\" and performant way is to use the `.str.get()` accessor, which is vectorized and gracefully handles missing values.\n",
    "\n",
    "```python\n",
    "k[k['produced_by'].str.get('identifier') == 'ark:/21547/DSz2757']\n",
    "```\n",
    "\n",
    "#### The Ibis Approach\n",
    "\n",
    "Now, let's consider an Ibis table `table` connected to a database like DuckDB. Ibis translates your Python code into efficient SQL.\n",
    "\n",
    "**1. Direct Filtering:**\n",
    "\n",
    "Ibis allows you to access fields in a struct-like column directly. This is clean and highly readable.\n",
    "\n",
    "```python\n",
    "e = table.filter(table.produced_by['identifier'] == 'ark:/21547/DSz2757')\n",
    "```\n",
    "\n",
    "**2. Filtering with `lambda` (more idiomatic):**\n",
    "\n",
    "For even cleaner code that avoids repeating the table variable, you can pass a `lambda` function to `filter`. This is considered a best practice in the Ibis community as it makes complex data pipelines easier to read and maintain.\n",
    "\n",
    "```python\n",
    "e = table.filter(lambda t: t.produced_by['identifier'] == 'ark:/21547/DSz2757')\n",
    "```\n",
    "\n",
    "Both Ibis expressions produce the same efficient SQL query. For DuckDB, the generated SQL would look something like this, using dot notation to access the nested field:\n",
    "\n",
    "```sql\n",
    "SELECT *\n",
    "FROM my_table t0\n",
    "WHERE \"t0\".\"produced_by\".\"identifier\" = 'ark:/21547/DSz2757'\n",
    "```\n",
    "\n",
    "This demonstrates how Ibis lets you write high-level, Pythonic code while leveraging the full power of the underlying database engine for scalable, high-performance queries on complex data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0987089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sample_identifier': 'ark:/21547/DSz2757',\n",
       " '@id': 'metadata/21547/DSz2757',\n",
       " 'label': '757',\n",
       " 'description': 'basisOfRecord: PreservedSpecimen',\n",
       " 'source_collection': 'GEOME',\n",
       " 'has_sample_object_type': [{'identifier': 'https://w3id.org/isample/vocabulary/materialsampleobjecttype/1.0/wholeorganism'}],\n",
       " 'has_material_category': [{'identifier': 'https://w3id.org/isample/vocabulary/material/1.0/organicmaterial'}],\n",
       " 'has_context_category': [{'identifier': 'https://w3id.org/isample/biology/biosampledfeature/1.0/Animalia'}],\n",
       " 'informal_classification': ['Taricha', 'granulosa'],\n",
       " 'keywords': [{'keyword': 'California'}, {'keyword': 'USA'}],\n",
       " 'produced_by': {'description': 'expeditionCode: newts | projectId: 244',\n",
       "  'has_feature_of_interest': '',\n",
       "  'identifier': 'ark:/21547/DSz2757',\n",
       "  'label': 'a22d568d303a95c622a9409871e562d7 newts',\n",
       "  'responsibility': [{'name': 'Vance Vredenburg', 'role': 'collector '},\n",
       "   {'name': ' Vance Vredenburg', 'role': 'principalInvestigator'}],\n",
       "  'result_time': '1894-01-01',\n",
       "  'sampling_site': {'description': None,\n",
       "   'label': 'California',\n",
       "   'place_name': ['California', 'USA'],\n",
       "   'sample_location': {'elevation': None,\n",
       "    'latitude': 38.578888,\n",
       "    'longitude': -122.57861}}},\n",
       " 'last_modified_time': Timestamp('1894-01-01 00:00:00+0000', tz='UTC'),\n",
       " 'curation': None,\n",
       " 'registrant': None,\n",
       " 'related_resource': None,\n",
       " 'sampling_purpose': None,\n",
       " 'sample_location_longitude': -122.57861,\n",
       " 'sample_location_latitude': 38.578888,\n",
       " 'geometry': b'\\x01\\x01\\x00\\x00\\x00\\xde\\xc8<\\xf2\\x07\\xa5^\\xc0\\xff\\x05\\x82\\x00\\x19JC@'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = table.limit(1).execute()\n",
    "(rows.loc[0].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd86130a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                has_material_category  \\\n",
      "0   [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "1   [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "2   [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "3                                                None   \n",
      "4   [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "5   [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "6   [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "7   [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "8   [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "9   [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "10  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "11  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "12  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "13  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "14  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "15  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "16  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "17  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "18  [{'identifier': 'https://w3id.org/isample/open...   \n",
      "19  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "20  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "21  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "22  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "23  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "24  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "25  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "26  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "27  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "28  [{'identifier': 'https://w3id.org/isample/open...   \n",
      "29  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "30  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "31  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "32  [{'identifier': 'https://w3id.org/isample/open...   \n",
      "33  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "34  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "35  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "36  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "37  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "38  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "39  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "40  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "41  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "42  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "43  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "44  [{'identifier': 'https://w3id.org/isample/open...   \n",
      "45  [{'identifier': 'https://w3id.org/isample/open...   \n",
      "46  [{'identifier': 'https://w3id.org/isample/open...   \n",
      "47  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "48  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "49  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "50  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "51  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "52  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "53  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "54  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "55  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "56  [{'identifier': 'https://w3id.org/isample/voca...   \n",
      "\n",
      "    has_material_category_count  \n",
      "0                         14595  \n",
      "1                          1025  \n",
      "2                         18763  \n",
      "3                          8333  \n",
      "4                         12173  \n",
      "5                            26  \n",
      "6                           160  \n",
      "7                           173  \n",
      "8                         11830  \n",
      "9                           357  \n",
      "10                          212  \n",
      "11                       194165  \n",
      "12                        29948  \n",
      "13                       838805  \n",
      "14                         1634  \n",
      "15                           57  \n",
      "16                          124  \n",
      "17                            1  \n",
      "18                        46164  \n",
      "19                        84492  \n",
      "20                       210546  \n",
      "21                       422889  \n",
      "22                          563  \n",
      "23                           25  \n",
      "24                            2  \n",
      "25                      2233779  \n",
      "26                            6  \n",
      "27                          855  \n",
      "28                          223  \n",
      "29                           95  \n",
      "30                          570  \n",
      "31                            4  \n",
      "32                        54181  \n",
      "33                       495052  \n",
      "34                        25655  \n",
      "35                            1  \n",
      "36                            8  \n",
      "37                            6  \n",
      "38                          799  \n",
      "39                      1250932  \n",
      "40                         4554  \n",
      "41                            8  \n",
      "42                           40  \n",
      "43                           25  \n",
      "44                          261  \n",
      "45                            5  \n",
      "46                            1  \n",
      "47                         1066  \n",
      "48                       318190  \n",
      "49                       243477  \n",
      "50                         2140  \n",
      "51                         7164  \n",
      "52                          187  \n",
      "53                           46  \n",
      "54                            5  \n",
      "55                       131930  \n",
      "56                        12605  \n"
     ]
    }
   ],
   "source": [
    "# compute the value counts for has_material_category\n",
    "result = table[\"has_material_category\"].value_counts().execute()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa31efa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latitude statistics:\n",
      "     count     min     max       mean        std\n",
      "0  5980282 -89.983  89.981  16.281101  33.070944\n",
      "Longitude statistics:\n",
      "     count    min    max      mean        std\n",
      "0  5980282 -180.0  180.0 -8.264868  92.460269\n",
      "Latitude percentiles:\n",
      "      25%        50%      75%\n",
      "0 -0.6798  29.970606  38.9346\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics for numeric columns\n",
    "print(\"Latitude statistics:\")\n",
    "lat_stats = table.aggregate([\n",
    "    table[\"sample_location_latitude\"].count().name('count'),\n",
    "    table[\"sample_location_latitude\"].min().name('min'),\n",
    "    table[\"sample_location_latitude\"].max().name('max'),\n",
    "    table[\"sample_location_latitude\"].mean().name('mean'),\n",
    "    table[\"sample_location_latitude\"].std().name('std'),\n",
    "]).execute()\n",
    "print(lat_stats)\n",
    "\n",
    "print(\"Longitude statistics:\")\n",
    "lon_stats = table.aggregate([\n",
    "    table[\"sample_location_longitude\"].count().name('count'),\n",
    "    table[\"sample_location_longitude\"].min().name('min'),\n",
    "    table[\"sample_location_longitude\"].max().name('max'),\n",
    "    table[\"sample_location_longitude\"].mean().name('mean'),\n",
    "    table[\"sample_location_longitude\"].std().name('std'),\n",
    "]).execute()\n",
    "print(lon_stats)\n",
    "\n",
    "# For percentiles, you can use quantile:\n",
    "print(\"Latitude percentiles:\")\n",
    "lat_percentiles = table.aggregate([\n",
    "    table[\"sample_location_latitude\"].quantile(0.25).name('25%'),\n",
    "    table[\"sample_location_latitude\"].quantile(0.50).name('50%'),\n",
    "    table[\"sample_location_latitude\"].quantile(0.75).name('75%')\n",
    "]).execute()\n",
    "print(lat_percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "531fb74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records per source collection:\n",
      "  source_collection    count\n",
      "0             SESAR  4688386\n",
      "1       OPENCONTEXT  1064831\n",
      "2             GEOME   605554\n",
      "3       SMITHSONIAN   322161\n",
      "Geographic data availability by collection:\n",
      "  source_collection    total  with_coords  coord_percentage\n",
      "0       OPENCONTEXT  1064831      1064831             100.0\n",
      "1             GEOME   605554       605554             100.0\n",
      "2       SMITHSONIAN   322161       322161             100.0\n",
      "3             SESAR  4688386      4688386             100.0\n"
     ]
    }
   ],
   "source": [
    "# Group by source collection and count records\n",
    "collection_summary = (\n",
    "    table.group_by(\"source_collection\")\n",
    "    .aggregate(count=table.count())\n",
    "    .order_by(ibis.desc(\"count\"))\n",
    "    .execute()\n",
    ")\n",
    "print(\"Records per source collection:\")\n",
    "print(collection_summary)\n",
    "\n",
    "# Find records with geographic information\n",
    "geography_stats = (\n",
    "    table.group_by(\"source_collection\")\n",
    "    .aggregate(\n",
    "        total=table.count(),\n",
    "        with_coords=((~table[\"geometry\"].isnull()).sum()),\n",
    "        coord_percentage=(100 * (~table[\"geometry\"].isnull()).mean())\n",
    "    )\n",
    "    .execute()\n",
    ")\n",
    "print(\"Geographic data availability by collection:\")\n",
    "print(geography_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa443bbf",
   "metadata": {},
   "source": [
    "\n",
    "<a id='Data-Cleaning-and-Preparation'></a>\n",
    "## 4. Data Cleaning and Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d82882",
   "metadata": {},
   "source": [
    "\n",
    "### Ibis vs. Pandas: Selecting the First Row\n",
    "\n",
    "Your intuition is correct! The equivalent of `df.loc[0]` in pandas for an Ibis table is `table.limit(1)`.\n",
    "\n",
    "The key difference lies in their execution models:\n",
    "\n",
    "*   **Pandas (Eager Execution)**: When you have a pandas DataFrame (`df`), the data is already loaded into memory. `df.loc[0]` or `df.head(1)` directly accesses this in-memory data to retrieve the first row.\n",
    "\n",
    "*   **Ibis (Lazy Execution)**: Ibis works differently. When you create an Ibis table, you are creating a *pointer* to the data, not loading it. The code you write builds a query plan. \n",
    "    *   `table.limit(1)`: This adds a \"limit\" operation to the query plan. No data has been read yet.\n",
    "    *   `.execute()`: This is the command that sends the completed query plan to the backend (in this case, DuckDB reading the Parquet file) to actually retrieve the data.\n",
    "\n",
    "This lazy approach is what makes Ibis so powerful for large datasets, as you only pull the data you explicitly ask for into memory.\n",
    "\n",
    "Here is an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57579e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_identifier</th>\n",
       "      <th>source_collection</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ark:/21547/DSz2757</td>\n",
       "      <td>GEOME</td>\n",
       "      <td>POINT (-122.57861 38.57889)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ark:/21547/DSz2779</td>\n",
       "      <td>GEOME</td>\n",
       "      <td>POINT (-122.37306 37.38528)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ark:/21547/DSz2806</td>\n",
       "      <td>GEOME</td>\n",
       "      <td>POINT (-122.11705 37.36549)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ark:/21547/DSz2807</td>\n",
       "      <td>GEOME</td>\n",
       "      <td>POINT (-122.11705 37.36549)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ark:/21547/DSz2759</td>\n",
       "      <td>GEOME</td>\n",
       "      <td>POINT (-122.57861 38.57889)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6680927</th>\n",
       "      <td>ark:/65665/3fffcea63-19cd-478d-84fe-9914c6f55157</td>\n",
       "      <td>SMITHSONIAN</td>\n",
       "      <td>POINT EMPTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6680928</th>\n",
       "      <td>ark:/65665/3fffe3e56-ec61-4892-9237-497340ad56ae</td>\n",
       "      <td>SMITHSONIAN</td>\n",
       "      <td>POINT EMPTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6680929</th>\n",
       "      <td>ark:/65665/3fffe639f-69f4-451d-8aad-af6c9a0265d8</td>\n",
       "      <td>SMITHSONIAN</td>\n",
       "      <td>POINT (-95.4615 30.3353)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6680930</th>\n",
       "      <td>ark:/65665/3fffebe64-0849-4803-9cbc-a4129a927bf8</td>\n",
       "      <td>SMITHSONIAN</td>\n",
       "      <td>POINT EMPTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6680931</th>\n",
       "      <td>ark:/65665/3ffff4347-6508-40c5-b0b5-5e8b2236c25a</td>\n",
       "      <td>SMITHSONIAN</td>\n",
       "      <td>POINT (-122.674 47.1613)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6680932 rows Ã 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        sample_identifier source_collection  \\\n",
       "0                                      ark:/21547/DSz2757             GEOME   \n",
       "1                                      ark:/21547/DSz2779             GEOME   \n",
       "2                                      ark:/21547/DSz2806             GEOME   \n",
       "3                                      ark:/21547/DSz2807             GEOME   \n",
       "4                                      ark:/21547/DSz2759             GEOME   \n",
       "...                                                   ...               ...   \n",
       "6680927  ark:/65665/3fffcea63-19cd-478d-84fe-9914c6f55157       SMITHSONIAN   \n",
       "6680928  ark:/65665/3fffe3e56-ec61-4892-9237-497340ad56ae       SMITHSONIAN   \n",
       "6680929  ark:/65665/3fffe639f-69f4-451d-8aad-af6c9a0265d8       SMITHSONIAN   \n",
       "6680930  ark:/65665/3fffebe64-0849-4803-9cbc-a4129a927bf8       SMITHSONIAN   \n",
       "6680931  ark:/65665/3ffff4347-6508-40c5-b0b5-5e8b2236c25a       SMITHSONIAN   \n",
       "\n",
       "                            geometry  \n",
       "0        POINT (-122.57861 38.57889)  \n",
       "1        POINT (-122.37306 37.38528)  \n",
       "2        POINT (-122.11705 37.36549)  \n",
       "3        POINT (-122.11705 37.36549)  \n",
       "4        POINT (-122.57861 38.57889)  \n",
       "...                              ...  \n",
       "6680927                  POINT EMPTY  \n",
       "6680928                  POINT EMPTY  \n",
       "6680929     POINT (-95.4615 30.3353)  \n",
       "6680930                  POINT EMPTY  \n",
       "6680931     POINT (-122.674 47.1613)  \n",
       "\n",
       "[6680932 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67127b1f-47a0-45fe-91f8-5eea1b7953e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample_identifier', 'source_collection', 'geometry']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(gdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb2c25bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out the first few rows of table\n",
    "ibis.options.interactive = False\n",
    "k = table.head().execute()\n",
    "type(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a373b3d-f3f6-4d1e-a7fa-97e586867eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataframe: 6,680,932 records\n",
      "After removing empty geometries: 5,980,282 records\n",
      "Removed: 700,650 records (10.49%)\n"
     ]
    }
   ],
   "source": [
    "# Filter out null and empty geometries\n",
    "gdf_valid = gdf[~gdf.geometry.isna() & ~gdf.geometry.is_empty]\n",
    "\n",
    "print(f\"Original dataframe: {len(gdf):,} records\")\n",
    "print(f\"After removing empty geometries: {len(gdf_valid):,} records\")\n",
    "print(f\"Removed: {len(gdf) - len(gdf_valid):,} records ({(len(gdf) - len(gdf_valid))/len(gdf)*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5383d27a-9eb2-4698-81d0-ed5b2686f1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the size of gdf to make it easier to plot\n",
    "\n",
    "# Europe\n",
    "# gdf = gdf.cx[-11.83:25.5, 34.9:59]\n",
    "# USA\n",
    "# gdf = gdf.cx[-125:-66, 24:50]\n",
    "# WORLD\n",
    "# gdf = gdf.cx[-180:180, -90:90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "131528b2-76e7-496a-9052-28a1cd688a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sample_identifier', 'source_collection', 'geometry'], dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "437fc03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_color = [128, 128, 128, 255]  # Gray for unknown sources\n",
    "# Define color map \n",
    "color_map = {\n",
    "    \"SESAR\": [51, 102, 204, 255],       # Vibrant blue (#3366CC)\n",
    "    \"OPENCONTEXT\": [220, 57, 18, 255],  # Crimson red (#DC3912)\n",
    "    \"GEOME\": [16, 150, 24, 255],        # Forest green (#109618)\n",
    "    \"SMITHSONIAN\": [255, 153, 0, 255]   # Deep orange (#FF9900)\n",
    "}\n",
    "\n",
    "# Get selected collections\n",
    "selected_collections = ['SESAR', 'OPENCONTEXT', 'GEOME', 'SMITHSONIAN']\n",
    "\n",
    "def create_color_map_0(gdf, color_map, selected_collections=None, default_color=[128, 128, 128, 255]):\n",
    "    # Pre-compute colors for each point\n",
    "    colors = np.zeros((len(gdf), 4), dtype=np.uint8)\n",
    "    for i, source in enumerate(gdf['source_collection']):\n",
    "        if (selected_collections is None or source in selected_collections) and source in color_map:\n",
    "            colors[i] = color_map[source]\n",
    "        else:\n",
    "            colors[i] = default_color\n",
    "    return colors\n",
    "\n",
    "\n",
    "# function to create a color map with selected collections (which has default of all collections)\n",
    "# use faster vectorized approach\n",
    "def create_color_map(gdf, color_map, selected_collections=None, default_color=[128, 128, 128, 255]):\n",
    "    # Pre-compute colors for each point\n",
    "    colors = np.zeros((len(gdf), 4), dtype=np.uint8)\n",
    "    \n",
    "    # Create a mapping dictionary once\n",
    "    color_lookup = {cat: np.array(color_map.get(cat, default_color)) for cat in gdf['source_collection'].cat.categories}\n",
    "    \n",
    "    # Apply the mapping using categorical codes\n",
    "    for cat_code, cat in enumerate(gdf['source_collection'].cat.categories):\n",
    "        mask = gdf['source_collection'].cat.codes == cat_code\n",
    "        # Only apply color if the category is in selected_collections (if provided)\n",
    "        if selected_collections is None or cat in selected_collections:\n",
    "            colors[mask] = color_lookup.get(cat, default_color)\n",
    "        else:\n",
    "            colors[mask] = default_color\n",
    "    \n",
    "    return colors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d143d622",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# write this comparision as a test\n",
    "# pass arguments to the function\n",
    "\n",
    "def test_color_map():\n",
    "    # Test with full dataset (no selections)\n",
    "    colors0 = create_color_map_0(gdf_sample, color_map)\n",
    "    colors1 = create_color_map(gdf_sample, color_map)\n",
    "    assert np.array_equal(colors0, colors1), \"Full dataset color mapping failed\"\n",
    "\n",
    "    # Test with selected collections\n",
    "    selected_collections = ['SESAR', 'OPENCONTEXT']\n",
    "    colors0_selected = create_color_map_0(gdf_sample, color_map, selected_collections)\n",
    "    colors1_selected = create_color_map(gdf_sample, color_map, selected_collections)\n",
    "    assert np.array_equal(colors0_selected, colors1_selected), \"Selected collections color mapping failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c08994ff-6280-4c11-a7d0-609b358e5806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06da7f9421cd48f2b4189a491e2ed0ad",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "Map(custom_attribution='', layers=(BitmapTileLayer(data='https://tile.openstreetmap.org/{z}/{x}/{y}.png', max_â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lonboard import ScatterplotLayer, Map, BitmapTileLayer\n",
    "import numpy as np\n",
    "\n",
    "# First, ensure source_collection is categorical\n",
    "gdf['source_collection'] = gdf['source_collection'].astype('category')\n",
    "\n",
    "# Filter out null and empty geometries\n",
    "gdf_valid = gdf[~gdf.geometry.isna() & ~gdf.geometry.is_empty]\n",
    "\n",
    "# Get a sample if the dataset is too large\n",
    "gdf_sample = gdf_valid.sample(frac=0.1, random_state=42)  # Adjust number as needed\n",
    "\n",
    "# Create a color map for the sample\n",
    "colors = create_color_map(gdf_sample, color_map, selected_collections)\n",
    "\n",
    "\n",
    "# Create a base tile layer with OpenStreetMap\n",
    "base_layer = BitmapTileLayer(\n",
    "        data=\"https://tile.openstreetmap.org/{z}/{x}/{y}.png\",\n",
    "        tile_size=256,\n",
    "        max_requests=-1,\n",
    "        min_zoom=0,\n",
    "        max_zoom=19,\n",
    "    )\n",
    "\n",
    "satellite_layer = BitmapTileLayer(\n",
    "    data=\"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\",\n",
    "    tile_size=256,\n",
    "    min_zoom=0,\n",
    "    max_zoom=19\n",
    ")\n",
    "\n",
    "# Create a ScatterplotLayer with the pre-computed colors\n",
    "layer = ScatterplotLayer.from_geopandas(\n",
    "    gdf_sample,\n",
    "    get_fill_color=colors,  # Pass the numpy array of colors\n",
    "    get_radius=300,\n",
    "    radius_units='meters',  # Use pixels instead of meters\n",
    "    pickable=True\n",
    ")\n",
    "\n",
    "# Create and display the map\n",
    "m = Map([base_layer, layer], _height=800)\n",
    "# m = Map([satellite_layer, layer], _height=800)\n",
    "display(m)\n",
    "\n",
    "# example code to manipulate the map\n",
    "# layer.get_fill_color = [0, 50, 200, 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09a8395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's play with the map and layer to learn how to use it\n",
    "\n",
    "# layer.get_fill_color = [0, 50, 200, 200]\n",
    "# set layer fill color to the color map\n",
    "\n",
    "layer.get_fill_color = colors\n",
    "\n",
    "# Just update zoom\n",
    "# Correct way to update the view state\n",
    "new_view_state = {\n",
    "    \"longitude\": m.view_state.longitude,\n",
    "    \"latitude\": m.view_state.latitude,\n",
    "    \"zoom\": 6,  # Your new zoom level\n",
    "    \"pitch\": m.view_state.pitch,\n",
    "    \"bearing\": m.view_state.bearing\n",
    "}\n",
    "\n",
    "m.view_state = new_view_state\n",
    "\n",
    "\n",
    "# view_state has the following attributes: longitude, latitude, zoom, pitch, bearing\n",
    "# m.view_state = {\"zoom\": 10} \n",
    "\n",
    "# dynamically change layers in the map\n",
    "m.layers = [base_layer, layer]\n",
    "# m.layers = [satellite_layer, layer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee433e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the output widget code in cell with ID \"5d3f6ec5\"\n",
    "gdf_sample['source_collection']\n",
    "\n",
    "# construct checkboxes for each source collection\n",
    "source_collections = gdf_sample['source_collection'].unique()\n",
    "checkboxes = {source: widgets.Checkbox(value=False, description=source) for source in source_collections}\n",
    "\n",
    "# Create output widget\n",
    "output = widgets.Output()\n",
    "\n",
    "# Respond to checkbox changes - FIX HERE\n",
    "def on_checkbox_change(change):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        selected_collections = [source for source, checkbox in checkboxes.items() if checkbox.value]\n",
    "        # Print to the output widget instead of trying to set its value\n",
    "        print(f\"Selected collections: {', '.join(selected_collections)}\")\n",
    "        print(f\"Number of rows in selection: {gdf_sample['source_collection'].isin(selected_collections).sum()}\")\n",
    "\n",
    "        # now update the layer and the map\n",
    "        # Create a color map fo\n",
    "\n",
    "# Register the callback with all checkboxes\n",
    "for checkbox in checkboxes.values():\n",
    "    checkbox.observe(on_checkbox_change, names='value')\n",
    "\n",
    "# Display the checkboxes and output\n",
    "display(widgets.VBox(list(checkboxes.values())), output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8936d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_sample['source_collection'].isin(selected_collections).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ad0b71-5881-4144-946c-451c74ae58d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_sample['source_collection'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e699b66c",
   "metadata": {},
   "source": [
    "## Managing Environment with `pip-tools`\n",
    "\n",
    "`pip-tools` is used to manage Python package dependencies for reproducible environments. The typical workflow involves two main commands: `pip-compile` and `pip-sync`.\n",
    "\n",
    "1.  **Define Direct Dependencies (`requirements.in`)**:\n",
    "    *   List your project's top-level dependencies in a `requirements.in` file. You can specify version constraints if needed.\n",
    "    *   Example `requirements.in`:\n",
    "        ```\n",
    "        pandas>=1.0\n",
    "        geopandas\n",
    "        lonboard\n",
    "        # For local editable installs:\n",
    "        # -e /path/to/local/package\n",
    "        ```\n",
    "\n",
    "2.  **Compile Dependencies (`pip-compile`)**:\n",
    "    *   Run `pip-compile requirements.in` (or specify input and output files: `pip-compile requirements.in --output-file requirements.txt`).\n",
    "    *   This generates a `requirements.txt` file, which pins the versions of your direct dependencies and all their sub-dependencies. This file ensures that your environment is reproducible.\n",
    "\n",
    "3.  **Synchronize Environment (`pip-sync`)**:\n",
    "    *   Run `pip-sync requirements.txt` (or just `pip-sync` if `requirements.txt` is in the current directory).\n",
    "    *   This command modifies your current virtual environment to exactly match the packages and versions specified in `requirements.txt`. It will:\n",
    "        *   Install any missing packages.\n",
    "        *   Upgrade or downgrade existing packages to their pinned versions.\n",
    "        *   Uninstall any packages in the environment that are not listed in `requirements.txt`.\n",
    "\n",
    "**How to \"Install\" Packages with `pip-tools`**:\n",
    "\n",
    "`pip-tools` doesn't have a direct `install` subcommand like `pip install <package>`. To add or update packages:\n",
    "1.  Add or modify the package entry in your `requirements.in` file.\n",
    "2.  Run `pip-compile requirements.in` to update `requirements.txt`.\n",
    "3.  Run `pip-sync` to apply the changes to your virtual environment.\n",
    "\n",
    "This process ensures that your `requirements.txt` always reflects the complete, pinned set of dependencies for your project, leading to more stable and predictable environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b042fe-7f20-43bd-a5e0-a8d6552ab203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39c66d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create functions to make the map configurable and update based on user selections\n",
    "\n",
    "def update_layer_colors(gdf_data, selected_collections=None, radius=300, radius_units='meters'):\n",
    "    \"\"\"\n",
    "    Update the ScatterplotLayer with filtered data and colors based on selected collections\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf_data : GeoDataFrame\n",
    "        The geodataframe containing the data to plot\n",
    "    selected_collections : list, optional\n",
    "        List of collection names to highlight. If None, all collections are shown\n",
    "    radius : float, optional\n",
    "        Radius of the points\n",
    "    radius_units : str, optional\n",
    "        Units for the radius ('meters' or 'pixels')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    layer : ScatterplotLayer\n",
    "        Updated ScatterplotLayer with filtered data and colors\n",
    "    colors : numpy.ndarray\n",
    "        Array of colors for the points\n",
    "    \"\"\"\n",
    "    # If selected_collections is empty or None, use all collections\n",
    "    if not selected_collections:\n",
    "        selected_collections = gdf_data['source_collection'].unique()\n",
    "    \n",
    "    # Filter the data if needed\n",
    "    if len(selected_collections) < len(gdf_data['source_collection'].unique()):\n",
    "        filtered_data = gdf_data[gdf_data['source_collection'].isin(selected_collections)]\n",
    "    else:\n",
    "        filtered_data = gdf_data\n",
    "    \n",
    "    # Create colors based on the selected collections\n",
    "    colors = create_color_map(filtered_data, color_map, selected_collections)\n",
    "    \n",
    "    # Create the layer\n",
    "    layer = ScatterplotLayer.from_geopandas(\n",
    "        filtered_data,\n",
    "        get_fill_color=colors,\n",
    "        get_radius=radius,\n",
    "        radius_units=radius_units,\n",
    "        pickable=True\n",
    "    )\n",
    "    \n",
    "    return layer, colors, filtered_data\n",
    "\n",
    "def create_map(base_layer_type=\"osm\", layer=None, height=800):\n",
    "    \"\"\"\n",
    "    Create and return a map with the specified base layer and data layer\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_layer_type : str, optional\n",
    "        Type of base layer to use ('osm' or 'satellite')\n",
    "    layer : ScatterplotLayer, optional\n",
    "        Data layer to add to the map\n",
    "    height : int, optional\n",
    "        Height of the map in pixels\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    m : Map\n",
    "        Map object with the specified layers\n",
    "    \"\"\"\n",
    "    # Define base layers\n",
    "    osm_layer = BitmapTileLayer(\n",
    "        data=\"https://tile.openstreetmap.org/{z}/{x}/{y}.png\",\n",
    "        tile_size=256,\n",
    "        max_requests=-1,\n",
    "        min_zoom=0,\n",
    "        max_zoom=19,\n",
    "    )\n",
    "    \n",
    "    satellite_layer = BitmapTileLayer(\n",
    "        data=\"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\",\n",
    "        tile_size=256,\n",
    "        min_zoom=0,\n",
    "        max_zoom=19\n",
    "    )\n",
    "    \n",
    "    # Select the base layer\n",
    "    if base_layer_type.lower() == \"satellite\":\n",
    "        base = satellite_layer\n",
    "    else:\n",
    "        base = osm_layer\n",
    "    \n",
    "    # Create the map with appropriate layers\n",
    "    layers = [base]\n",
    "    if layer is not None:\n",
    "        layers.append(layer)\n",
    "    \n",
    "    m = Map(layers, _height=height)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52021c46",
   "metadata": {},
   "source": [
    "\n",
    "<a id='Interactive-Map'></a>\n",
    "## 5. Interactive Map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a51195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive widgets for map configuration\n",
    "from ipywidgets import widgets, interactive, Layout, HBox, VBox, Output\n",
    "\n",
    "# Create widgets for collection selection\n",
    "collection_checkboxes = {\n",
    "    collection: widgets.Checkbox(\n",
    "        value=True, \n",
    "        description=collection,\n",
    "        layout=Layout(width='auto')\n",
    "    ) for collection in gdf_sample['source_collection'].unique()\n",
    "}\n",
    "\n",
    "# Create a widget for base map selection\n",
    "base_map_dropdown = widgets.Dropdown(\n",
    "    options=['OpenStreetMap', 'Satellite'],\n",
    "    value='OpenStreetMap',\n",
    "    description='Base Map:',\n",
    "    layout=Layout(width='200px')\n",
    ")\n",
    "\n",
    "# Create a widget for point size\n",
    "point_size_slider = widgets.IntSlider(\n",
    "    value=300,\n",
    "    min=100,\n",
    "    max=1000,\n",
    "    step=50,\n",
    "    description='Point Size:',\n",
    "    layout=Layout(width='300px')\n",
    ")\n",
    "\n",
    "# Create a widget for the units\n",
    "radius_units_dropdown = widgets.Dropdown(\n",
    "    options=['meters', 'pixels'],\n",
    "    value='meters',\n",
    "    description='Units:',\n",
    "    layout=Layout(width='200px')\n",
    ")\n",
    "\n",
    "# Create a button to update the map\n",
    "update_button = widgets.Button(\n",
    "    description='Update Map',\n",
    "    button_style='primary',\n",
    "    layout=Layout(width='150px')\n",
    ")\n",
    "\n",
    "# Create an output widget for the map and statistics\n",
    "map_output = widgets.Output()\n",
    "stats_output = widgets.Output()\n",
    "\n",
    "# Function to update the map based on widget values\n",
    "def update_map(b):\n",
    "    with map_output:\n",
    "        map_output.clear_output(wait=True)\n",
    "        \n",
    "        # Get selected collections\n",
    "        selected_collections = [\n",
    "            collection for collection, checkbox in collection_checkboxes.items() \n",
    "            if checkbox.value\n",
    "        ]\n",
    "        \n",
    "        # Get base map type\n",
    "        base_layer_type = 'osm' if base_map_dropdown.value == 'OpenStreetMap' else 'satellite'\n",
    "        \n",
    "        # Update layer with selected collections and point size\n",
    "        layer, colors, filtered_data = update_layer_colors(\n",
    "            gdf_sample, \n",
    "            selected_collections, \n",
    "            radius=point_size_slider.value,\n",
    "            radius_units=radius_units_dropdown.value\n",
    "        )\n",
    "        \n",
    "        # Create and display the map\n",
    "        m = create_map(base_layer_type=base_layer_type, layer=layer)\n",
    "        display(m)\n",
    "        \n",
    "        # Update statistics\n",
    "        with stats_output:\n",
    "            stats_output.clear_output(wait=True)\n",
    "            print(f\"Selected collections: {', '.join(selected_collections)}\")\n",
    "            print(f\"Points displayed: {len(filtered_data):,} of {len(gdf_sample):,} ({len(filtered_data)/len(gdf_sample)*100:.1f}%)\")\n",
    "            print(f\"Points by collection:\")\n",
    "            for collection in selected_collections:\n",
    "                count = sum(filtered_data['source_collection'] == collection)\n",
    "                print(f\"  {collection}: {count:,} points\")\n",
    "\n",
    "# Connect the update function to the button\n",
    "update_button.on_click(update_map)\n",
    "\n",
    "# Create the layout for the widgets\n",
    "collection_box = VBox([widgets.HTML(\"<b>Data Collections:</b>\")] + list(collection_checkboxes.values()))\n",
    "config_box = VBox([\n",
    "    widgets.HTML(\"<b>Map Configuration:</b>\"),\n",
    "    base_map_dropdown,\n",
    "    point_size_slider,\n",
    "    radius_units_dropdown,\n",
    "    update_button\n",
    "])\n",
    "\n",
    "# Arrange the widgets in a horizontal layout\n",
    "control_panel = HBox([collection_box, config_box], layout=Layout(width='100%'))\n",
    "\n",
    "# Display the widgets and outputs\n",
    "display(control_panel)\n",
    "display(stats_output)\n",
    "display(map_output)\n",
    "\n",
    "# Initialize the map\n",
    "update_map(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c28258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a function to zoom to specific regions\n",
    "zoom_regions = {\n",
    "    'World': {'longitude': 0, 'latitude': 0, 'zoom': 1},\n",
    "    'North America': {'longitude': -100, 'latitude': 40, 'zoom': 3},\n",
    "    'Europe': {'longitude': 10, 'latitude': 50, 'zoom': 4},\n",
    "    'Asia': {'longitude': 100, 'latitude': 30, 'zoom': 3},\n",
    "    'Africa': {'longitude': 20, 'latitude': 0, 'zoom': 3},\n",
    "    'South America': {'longitude': -60, 'latitude': -20, 'zoom': 3},\n",
    "    'Australia': {'longitude': 135, 'latitude': -25, 'zoom': 4},\n",
    "}\n",
    "\n",
    "# Create a dropdown for region selection\n",
    "region_dropdown = widgets.Dropdown(\n",
    "    options=list(zoom_regions.keys()),\n",
    "    value='World',\n",
    "    description='Zoom to:',\n",
    "    layout=Layout(width='200px')\n",
    ")\n",
    "\n",
    "# Function to zoom the map to a region\n",
    "def zoom_to_region(change):\n",
    "    if not hasattr(zoom_to_region, 'current_map'):\n",
    "        return\n",
    "    \n",
    "    region = change['new']\n",
    "    view_state = zoom_regions[region].copy()\n",
    "    # Add missing view state properties\n",
    "    if 'pitch' not in view_state:\n",
    "        view_state['pitch'] = 0\n",
    "    if 'bearing' not in view_state:\n",
    "        view_state['bearing'] = 0\n",
    "    \n",
    "    zoom_to_region.current_map.view_state = view_state\n",
    "\n",
    "# Function to update the map based on widget values (updated version)\n",
    "def update_map(b):\n",
    "    with map_output:\n",
    "        map_output.clear_output(wait=True)\n",
    "        \n",
    "        # Get selected collections\n",
    "        selected_collections = [\n",
    "            collection for collection, checkbox in collection_checkboxes.items() \n",
    "            if checkbox.value\n",
    "        ]\n",
    "        \n",
    "        # Get base map type\n",
    "        base_layer_type = 'osm' if base_map_dropdown.value == 'OpenStreetMap' else 'satellite'\n",
    "        \n",
    "        # Update layer with selected collections and point size\n",
    "        layer, colors, filtered_data = update_layer_colors(\n",
    "            gdf_sample, \n",
    "            selected_collections, \n",
    "            radius=point_size_slider.value,\n",
    "            radius_units=radius_units_dropdown.value\n",
    "        )\n",
    "        \n",
    "        # Create and display the map\n",
    "        m = create_map(base_layer_type=base_layer_type, layer=layer)\n",
    "        display(m)\n",
    "        \n",
    "        # Store the map for later zoom operations\n",
    "        zoom_to_region.current_map = m\n",
    "        \n",
    "        # Update statistics\n",
    "        with stats_output:\n",
    "            stats_output.clear_output(wait=True)\n",
    "            print(f\"Selected collections: {', '.join(selected_collections)}\")\n",
    "            print(f\"Points displayed: {len(filtered_data):,} of {len(gdf_sample):,} ({len(filtered_data)/len(gdf_sample)*100:.1f}%)\")\n",
    "            print(f\"Points by collection:\")\n",
    "            for collection in selected_collections:\n",
    "                count = sum(filtered_data['source_collection'] == collection)\n",
    "                print(f\"  {collection}: {count:,} points ({count/len(filtered_data)*100:.1f}%)\")\n",
    "\n",
    "# Connect the region dropdown to the zoom function\n",
    "region_dropdown.observe(zoom_to_region, names='value')\n",
    "\n",
    "# Update the control panel to include the region dropdown\n",
    "config_box = VBox([\n",
    "    widgets.HTML(\"<b>Map Configuration:</b>\"),\n",
    "    base_map_dropdown,\n",
    "    point_size_slider,\n",
    "    radius_units_dropdown,\n",
    "    region_dropdown,\n",
    "    update_button\n",
    "])\n",
    "\n",
    "# Recreate the control panel\n",
    "control_panel = HBox([collection_box, config_box], layout=Layout(width='100%'))\n",
    "\n",
    "# Display the updated widgets and outputs\n",
    "display(control_panel)\n",
    "display(stats_output)\n",
    "display(map_output)\n",
    "\n",
    "# Initialize the map\n",
    "update_map(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9212f9",
   "metadata": {},
   "source": [
    "## Interactive iSamples Map\n",
    "\n",
    "This interactive map allows you to explore the iSamples dataset with the following features:\n",
    "\n",
    "1. **Collection Selection**: Choose which data collections to display\n",
    "2. **Base Map**: Switch between OpenStreetMap and satellite imagery\n",
    "3. **Point Size**: Adjust the size of the points on the map\n",
    "4. **Units**: Choose between meters and pixels for point sizing\n",
    "5. **Region Selection**: Quickly zoom to different regions of the world\n",
    "6. **Statistics**: View counts and percentages of displayed points\n",
    "\n",
    "The map is rendered using the Lonboard library, which provides fast visualization of large geospatial datasets directly in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e008c9b7",
   "metadata": {},
   "source": [
    "\n",
    "Yes, it is absolutely possible to embed human-readable descriptions of what columns mean directly within a Parquet or GeoParquet file. This is a key feature for making datasets self-documenting and easier to use.\n",
    "\n",
    "### Parquet and GeoParquet Metadata\n",
    "\n",
    "Both Parquet and GeoParquet formats allow for storing metadata at both the file level and the column level. This metadata is stored as key-value pairs. You can add a `description` key to the metadata of each column to hold a human-readable description.\n",
    "\n",
    "When you are creating or modifying a Parquet file, you can add this metadata. For example, using `pyarrow`, you can specify the schema with descriptions for each field.\n",
    "\n",
    "### Example with `pyarrow`\n",
    "\n",
    "Here is a conceptual example of how you might do this in Python with the `pyarrow` library:\n",
    "\n",
    "```python\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {'col1': [1, 2], 'col2': [3.4, 5.6]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create a schema with descriptions\n",
    "schema = pa.schema([\n",
    "    pa.field('col1', pa.int64(), metadata={\"description\": \"This is the first column.\"}),\n",
    "    pa.field('col2', pa.float64(), metadata={\"description\": \"This is the second column.\"})\n",
    "])\n",
    "\n",
    "# Create a PyArrow Table\n",
    "table = pa.Table.from_pandas(df, schema=schema)\n",
    "\n",
    "# Write to a Parquet file\n",
    "# pa.parquet.write_table(table, 'data_with_descriptions.parquet')\n",
    "```\n",
    "\n",
    "When another user or tool reads this Parquet file, they can inspect the schema and retrieve these descriptions to understand the meaning of each column without needing separate documentation. This is a powerful feature for data sharing and collaboration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8c6c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficiently generate a histogram for 'last_modified_time'\n",
    "# The challenge is the large number of rows and unique timestamps.\n",
    "# We can use Ibis to offload the heavy lifting to the backend.\n",
    "\n",
    "# 1. The column is already a timestamp, so we can use it directly.\n",
    "timestamp_col = table['last_modified_time']\n",
    "\n",
    "# 2. Extract the year from the timestamp to use as bins for our histogram.\n",
    "year_col = timestamp_col.year().name('year')\n",
    "\n",
    "# 3. Group by year and count the number of records in each year.\n",
    "# This is memory-efficient as only the aggregated result is pulled into pandas.\n",
    "histogram_data = table.group_by(year_col).agg(count=table.count()).order_by('year').execute()\n",
    "\n",
    "# 4. Plot the histogram using matplotlib.\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.bar(histogram_data['year'], histogram_data['count'], color='skyblue')\n",
    "plt.title('Histogram of Records by Last Modified Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Records')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "# Ensure x-axis labels are integers\n",
    "plt.xticks(histogram_data['year'].unique().astype(int))\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the aggregated data\n",
    "histogram_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f94442",
   "metadata": {},
   "source": [
    "### Interactive Filtering by Date\n",
    "\n",
    "Below is an example of using `ipywidgets` to create a slider for filtering the data based on the `last_modified_time`. This is much more efficient than loading the entire dataset into pandas, as it uses Ibis to perform the filtering and counting on the backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d55e818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get min and max years for the slider\n",
    "min_year = table['last_modified_time'].year().min().execute()\n",
    "max_year = table['last_modified_time'].year().max().execute()\n",
    "\n",
    "# Create a range slider for the years\n",
    "year_slider = widgets.IntRangeSlider(\n",
    "    value=[min_year, max_year],\n",
    "    min=min_year,\n",
    "    max=max_year,\n",
    "    step=1,\n",
    "    description='Filter by Year:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,  # Only trigger update on release\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d',\n",
    "    layout=Layout(width='500px')\n",
    ")\n",
    "\n",
    "# Create an output widget to display the count\n",
    "count_output = widgets.Output()\n",
    "\n",
    "# Function to handle slider changes\n",
    "def on_slider_change(change):\n",
    "    with count_output:\n",
    "        count_output.clear_output(wait=True)\n",
    "        min_val, max_val = change['new']\n",
    "        \n",
    "        # Filter the table based on the selected year range\n",
    "        filtered_table = table.filter(\n",
    "            (table['last_modified_time'].year() >= min_val) &\n",
    "            (table['last_modified_time'].year() <= max_val)\n",
    "        )\n",
    "        \n",
    "        # Get the count of rows in the filtered table\n",
    "        row_count = filtered_table.count().execute()\n",
    "        \n",
    "        print(f\"Number of records from {min_val} to {max_val}: {row_count:,}\")\n",
    "\n",
    "# Observe the slider for changes\n",
    "year_slider.observe(on_slider_change, names='value')\n",
    "\n",
    "# Display the widgets\n",
    "display(year_slider, count_output)\n",
    "\n",
    "# Trigger the initial display\n",
    "on_slider_change({'new': (min_year, max_year)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612be2a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ffc640",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pyarrow.parquet as pq\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Read the schema from the Parquet file\n",
    "schema = pq.read_schema(local_path)\n",
    "\n",
    "# Create the markdown table header\n",
    "md = \"| Column Name | Data Type | Description |\\n\"\n",
    "md += \"|---|---|---|\\n\"\n",
    "\n",
    "# Populate the table with schema information\n",
    "for field in schema:\n",
    "    # Extract description from metadata, if it exists\n",
    "    description = field.metadata.get(b'description', b'').decode('utf-8') if field.metadata else \"\"\n",
    "    md += f\"| {field.name} | {field.type} | {description} |\\n\"\n",
    "\n",
    "display(Markdown(md))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438162f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Define the path to the GeoParquet file\n",
    "LOCAL_PATH = \"/Users/raymondyee/Data/iSample/2025_04_21_16_23_46/isamples_export_2025_04_21_16_23_46_geo.parquet\"\n",
    "local_path = Path(LOCAL_PATH)\n",
    "\n",
    "# 1. Define your column descriptions here\n",
    "# The keys are the column names from your file. \n",
    "# Fill in the string values with the description for each column.\n",
    "\n",
    "column_descriptions = {\n",
    "    '@id': 'Unique identifier for the sample, often a URI.',\n",
    "    'sample_identifier': 'A unique identifier for the sample within its source collection.',\n",
    "    'label': 'The primary name or label assigned to the sample.',\n",
    "    'description': 'A free-text description of the sample.',\n",
    "    'source_collection': 'The collection or dataset from which the sample originates (e.g., SESAR, OPENCONTEXT).',\n",
    "    'has_sample_object_type': 'The type of object that was sampled (e.g., rock, water, artifact).',\n",
    "    'has_material_category': 'The category of material the sample is composed of (e.g., organic, inorganic).',\n",
    "    'has_context_category': 'The environmental or cultural context from which the sample was taken (e.g., marine, archaeological).',\n",
    "    'informal_classification': 'An informal or local classification of the sample.',\n",
    "    'keywords': 'A list of keywords associated with the sample.',\n",
    "    'produced_by': 'Information about the agent or process that produced the sample.',\n",
    "    'curation': 'Information about the curation and stewardship of the sample.',\n",
    "    'registrant': 'The person or organization that registered the sample.',\n",
    "    'related_resource': 'Links to related resources or publications.',\n",
    "    'sampling_purpose': 'The reason or purpose for which the sample was collected.',\n",
    "    'sample_location_longitude': 'The longitude of the sample location (WGS84).',\n",
    "    'sample_location_latitude': 'The latitude of the sample location (WGS84).',\n",
    "    'last_modified_names': 'The date the record was last modified.',\n",
    "    'geometry': 'The geographic coordinates of the sample location in WKB format.'\n",
    "}\n",
    "\n",
    "# 2. Code to generate a markdown documentation file\n",
    "\n",
    "# Read the schema from the Parquet file\n",
    "schema = pq.read_schema(local_path)\n",
    "\n",
    "# Start the markdown string with a header\n",
    "md_string = f\"# Schema for {local_path.name}\\n\\n\"\n",
    "md_string += \"| Column Name | Data Type | Description |\\n\"\n",
    "md_string += \"|---|---|---|\\n\"\n",
    "\n",
    "# Populate the table with schema information\n",
    "for field in schema:\n",
    "    col_name = field.name\n",
    "    col_type = str(field.type)\n",
    "    # Get description from our dictionary, or a placeholder if not present\n",
    "    col_desc = column_descriptions.get(col_name, \"*No description provided.*\").replace('|', '\\|') # Escape pipe characters\n",
    "    md_string += f\"| `{col_name}` | `{col_type}` | {col_desc} |\\n\"\n",
    "\n",
    "# Define the output path for the markdown file\n",
    "output_path = Path(str(local_path).replace('_geo.parquet', '_geo_schema.md'))\n",
    "\n",
    "# Write the markdown string to the file\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(md_string)\n",
    "\n",
    "print(f\"Successfully created schema documentation at: {output_path}\")\n",
    "\n",
    "# Display the generated markdown in the notebook for review\n",
    "print(\"\\n--- Schema Documentation ---\")\n",
    "display(Markdown(md_string))\n",
    "\n",
    "# TO DO: long term to do: possibly figure out how to embed human friendly descriptions into the geoparquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd635b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fd5776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3524135",
   "metadata": {},
   "source": [
    "\n",
    "That is an excellent question. While Ibis does not have a `query` method that works exactly like the pandas version, you can make your code more concise by passing a `lambda` function to the `filter` method. This avoids the need to repeat the table name.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38716461",
   "metadata": {},
   "source": [
    "\n",
    "### The EDA System: A Conceptual Roadmap\n",
    "\n",
    "The central idea is to create a \"control panel\" of widgets that are dynamically generated based on the schema of your Ibis table. This panel allows a user to build up a complex filter expression interactively, and then Ibis executes the final, filtered query.\n",
    "\n",
    "Hereâs a step-by-step approach to implementing your vision:\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 1: Schema Inspection and Widget Mapping**\n",
    "\n",
    "The foundation of the system is a function that inspects the Ibis table's schema and decides which widget is appropriate for each column.\n",
    "\n",
    "```python\n",
    "import ibis\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import VBox, HBox, Dropdown, Text, IntSlider, Output\n",
    "\n",
    "def generate_control_panel(table):\n",
    "    # Get the schema from the Ibis table\n",
    "    schema = table.schema()\n",
    "    \n",
    "    widget_map = {}\n",
    "    \n",
    "    for col_name, col_type in schema.items():\n",
    "        # A factory function decides which widget to create\n",
    "        widget = create_widget_for_column(table, col_name, col_type)\n",
    "        if widget:\n",
    "            widget_map[col_name] = widget\n",
    "            \n",
    "    # Arrange widgets in a layout\n",
    "    return VBox([HBox([widgets.Label(name), w]) for name, w in widget_map.items()]), widget_map\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 2: The Widget Factory (The \"Brains\")**\n",
    "\n",
    "This is the core logic you described. A function needs to intelligently create the right widget based on the column's type and cardinality.\n",
    "\n",
    "```python\n",
    "def create_widget_for_column(table, col_name, col_type):\n",
    "    # For nested/JSON-like columns\n",
    "    if isinstance(col_type, ibis.expr.datatypes.Struct):\n",
    "        # Create a text box to query a specific key within the struct\n",
    "        # This is a simple starting point; could be more advanced\n",
    "        return Text(description=\"Filter by key (e.g., key:value)\")\n",
    "\n",
    "    # For numeric columns\n",
    "    if col_type.is_numeric():\n",
    "        min_val = table[col_name].min().execute()\n",
    "        max_val = table[col_name].max().execute()\n",
    "        return IntSlider(min=min_val, max=max_val, value=min_val, description=f\"Range\")\n",
    "\n",
    "    # For string/categorical columns\n",
    "    if col_type.is_string():\n",
    "        # Check the number of unique values (cardinality)\n",
    "        cardinality = table[col_name].nunique().execute()\n",
    "        \n",
    "        if 1 < cardinality < 25: # Low cardinality -> Dropdown\n",
    "            options = table[col_name].value_counts().execute().index.tolist()\n",
    "            return Dropdown(options=[''] + options) # Add empty option for \"no filter\"\n",
    "        else: # High cardinality -> Text search\n",
    "            return Text(description=\"Contains text...\")\n",
    "            \n",
    "    return None # No widget for this type\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 3: Linking Widgets to Ibis (`observe`)**\n",
    "\n",
    "Once the widgets are displayed, you need to link their changes to an Ibis query. The `.observe()` method of `ipywidgets` is perfect for this. You'll build a list of filter expressions and re-run the query whenever a widget's value changes.\n",
    "\n",
    "```python\n",
    "# Assume `table` is your Ibis table\n",
    "controls_vbox, widget_map = generate_control_panel(table)\n",
    "output_area = Output() # An area to display the results\n",
    "\n",
    "def apply_filters(change):\n",
    "    # Start with the base table\n",
    "    filtered_table = table\n",
    "    \n",
    "    # Collect all active filter conditions\n",
    "    for col_name, widget in widget_map.items():\n",
    "        if widget.value: # Apply filter if widget has a value\n",
    "            # This logic would need to be more robust based on widget type\n",
    "            if isinstance(widget, Dropdown):\n",
    "                filtered_table = filtered_table.filter(lambda t: t[col_name] == widget.value)\n",
    "            elif isinstance(widget, Text):\n",
    "                 filtered_table = filtered_table.filter(lambda t: t[col_name].contains(widget.value))\n",
    "            # ... add logic for sliders, etc.\n",
    "\n",
    "    # Execute the query and display the results\n",
    "    with output_area:\n",
    "        output_area.clear_output()\n",
    "        # Display the filtered data (e.g., as a pandas DataFrame)\n",
    "        display(filtered_table.limit(100).execute())\n",
    "        # Also display the generated SQL to see what Ibis is doing!\n",
    "        print(\"Generated SQL:\")\n",
    "        print(ibis.to_sql(filtered_table.limit(100)))\n",
    "\n",
    "# Attach the observer to each widget\n",
    "for widget in widget_map.values():\n",
    "    widget.observe(apply_filters, names='value')\n",
    "\n",
    "# Display the UI\n",
    "display(controls_vbox, output_area)\n",
    "# Trigger the initial display\n",
    "apply_filters(None)\n",
    "```\n",
    "\n",
    "### Summary of Your Big Idea\n",
    "\n",
    "*   **You are right on track.** This is exactly how modern data apps are built. You're essentially creating a semantic layer (the widgets) on top of your data that translates user intent into high-performance queries.\n",
    "*   **Scalability is Key:** The beauty of this approach is that the expensive work (`value_counts`, `min`, `max`, and the final filtering) is all pushed down to the database via Ibis. Your Jupyter kernel remains light and responsive.\n",
    "*   **Interactivity:** For the high-cardinality search/autosuggest, you could make the `Text` widget's observer trigger a `... .like('%value%').value_counts()` query to populate a *separate* dropdown, creating a dynamic search experience.\n",
    "\n",
    "This is a substantial but very achievable project. Starting with the three steps outlined above will give you a solid foundation for a very powerful and reusable EDA tool.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7d6824",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c8f2f3-c03e-409f-b7e3-45fa0052f30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's concentrate on last_modified_time\n",
    "\n",
    "table['last_modified_time'].value_counts().execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14358259",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isamples-python-3.12.9",
   "language": "python",
   "name": "isamples-python-3.12.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69717ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "\n",
    "def in_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "def install_dependencies_from_pyproject():\n",
    "    # URL to raw pyproject.toml file in your GitHub repository\n",
    "    pyproject_url = \"https://raw.githubusercontent.com/rdhyee/isamples-python/exploratory/pyproject.toml\"\n",
    "    \n",
    "    with urlopen(pyproject_url) as response:\n",
    "        pyproject_content = response.read().decode()\n",
    "    \n",
    "    # Parse the TOML content\n",
    "    import toml\n",
    "    pyproject_data = toml.loads(pyproject_content)\n",
    "    \n",
    "    # Extract dependencies\n",
    "    dependencies = pyproject_data.get('tool', {}).get('poetry', {}).get('dependencies', {})\n",
    "    \n",
    "    # Install each dependency\n",
    "    for package, version in dependencies.items():\n",
    "        if isinstance(version, str):\n",
    "            subprocess.run(['pip', 'install', f\"{package}{version}\"])\n",
    "        elif isinstance(version, dict):\n",
    "            # Handle more complex version specifications\n",
    "            version_str = version.get('version', '')\n",
    "            subprocess.run(['pip', 'install', f\"{package}{version_str}\"])\n",
    "\n",
    "if in_colab():\n",
    "    # Install toml parser first\n",
    "    subprocess.run(['pip', 'install', 'toml'])\n",
    "    install_dependencies_from_pyproject()\n",
    "    # pip install git+https://github.com/rdhyee/isamples-python.git@exploratory#egg=isamples_client\n",
    "    subprocess.run(['pip', 'install', 'git+https://github.com/rdhyee/isamples-python.git@exploratory#egg=isamples_client'])\n",
    "\n",
    "    from google.colab import output\n",
    "    output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f775c5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680932\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to a database (in-memory for this example)\n",
    "con = duckdb.connect(database=':memory:', read_only=False)\n",
    "\n",
    "# Execute the SQL commands\n",
    "con.execute(\"SET VARIABLE parquet_path = 'https://z.rslv.xyz/10.5281/zenodo.15278210/isamples_export_2025_04_21_16_23_46_geo.parquet';\")\n",
    "con.execute(\"CREATE TEMP VIEW my_data AS SELECT(*) FROM read_parquet(getvariable('parquet_path'));\")\n",
    "result = con.execute(\"SELECT count(*) from my_data;\").fetchone()\n",
    "\n",
    "# Print the result\n",
    "print(result[0])\n",
    "\n",
    "# Close the connection\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdb3e80",
   "metadata": {},
   "source": [
    "## Why DuckDB + Remote Parquet is So Fast\n",
    "\n",
    "The previous cell demonstrates an incredibly efficient approach that leverages several key technologies:\n",
    "\n",
    "### 1. **HTTP Range Requests (Byte-Range Handling)**\n",
    "- `z.rslv.xyz` supports HTTP Range requests\n",
    "- DuckDB can request only the specific bytes it needs from the remote file\n",
    "- For a `COUNT(*)` operation, DuckDB only needs to read:\n",
    "  - Parquet file metadata (footer)\n",
    "  - Row group metadata \n",
    "  - NOT the actual data rows\n",
    "\n",
    "### 2. **Parquet Columnar Format Benefits**\n",
    "- Parquet stores metadata about row counts in each row group\n",
    "- DuckDB can sum these counts without reading data\n",
    "- For a ~300MB file, this might only require reading a few KB\n",
    "\n",
    "### 3. **DuckDB's Query Optimization**\n",
    "- Pushdown optimization: operations are pushed to the file level\n",
    "- Lazy evaluation: only reads what's absolutely necessary\n",
    "- Efficient metadata parsing\n",
    "\n",
    "This means a `COUNT(*)` on a 300MB remote file can complete in seconds rather than minutes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "770b35fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DuckDB Remote Parquet Performance Demo ===\n",
      "\n",
      "1. COUNT(*) - Metadata only\n",
      "   Result: 6,680,932 records\n",
      "   Time: 2.93 seconds\n",
      "   Data read: Minimal (just metadata)\n",
      "\n",
      "2. COUNT by source_collection - Lightweight aggregation\n",
      "   Result: 6,680,932 records\n",
      "   Time: 2.93 seconds\n",
      "   Data read: Minimal (just metadata)\n",
      "\n",
      "2. COUNT by source_collection - Lightweight aggregation\n",
      "   Results:\n",
      "     SESAR: 4,688,386\n",
      "     OPENCONTEXT: 1,064,831\n",
      "     GEOME: 605,554\n",
      "     SMITHSONIAN: 322,161\n",
      "   Time: 3.78 seconds\n",
      "   Data read: Only source_collection column + metadata\n",
      "\n",
      "3. Latitude statistics - Single column read\n",
      "   Results:\n",
      "     SESAR: 4,688,386\n",
      "     OPENCONTEXT: 1,064,831\n",
      "     GEOME: 605,554\n",
      "     SMITHSONIAN: 322,161\n",
      "   Time: 3.78 seconds\n",
      "   Data read: Only source_collection column + metadata\n",
      "\n",
      "3. Latitude statistics - Single column read\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8540cc3f99084881aaaebc800a6ae8e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Total records: 6,680,932\n",
      "   Non-null coordinates: 5,980,282\n",
      "   Latitude range: -89.983 to 89.981\n",
      "   Average latitude: 16.281\n",
      "   Time: 8.82 seconds\n",
      "   Data read: Only latitude column\n",
      "\n",
      "4. Geographic bounding box filter - Selective read\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff0e59637f4848ce880e6088e942b58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Records in continental US bounds: 1,153,603\n",
      "   Time: 12.05 seconds\n",
      "   Data read: Only lon/lat columns + pushdown filtering\n",
      "\n",
      "=== Key Insights ===\n",
      "• COUNT(*) is nearly instant - uses only Parquet metadata\n",
      "• Aggregations by categorical columns are very fast\n",
      "• Single-column operations read only that column\n",
      "• Filtering is pushed down to the file level\n",
      "• This approach scales to files much larger than available RAM\n",
      "\n",
      "This is why DuckDB + remote Parquet is perfect for exploratory data analysis!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import duckdb\n",
    "\n",
    "# Demonstrate different types of queries and their efficiency with remote Parquet\n",
    "con = duckdb.connect(database=':memory:', read_only=False)\n",
    "remote_url = 'https://z.rslv.xyz/10.5281/zenodo.15278210/isamples_export_2025_04_21_16_23_46_geo.parquet'\n",
    "con.execute(f\"SET VARIABLE parquet_path = '{remote_url}';\")\n",
    "con.execute(\"CREATE TEMP VIEW my_data AS SELECT(*) FROM read_parquet(getvariable('parquet_path'));\")\n",
    "\n",
    "print(\"=== DuckDB Remote Parquet Performance Demo ===\\n\")\n",
    "\n",
    "# Test 1: COUNT(*) - Only needs metadata\n",
    "print(\"1. COUNT(*) - Metadata only\")\n",
    "start_time = time.time()\n",
    "result = con.execute(\"SELECT count(*) from my_data;\").fetchone()\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"   Result: {result[0]:,} records\")\n",
    "print(f\"   Time: {elapsed:.2f} seconds\")\n",
    "print(f\"   Data read: Minimal (just metadata)\\n\")\n",
    "\n",
    "# Test 2: Count by groups - Still mostly metadata\n",
    "print(\"2. COUNT by source_collection - Lightweight aggregation\")\n",
    "start_time = time.time()\n",
    "result = con.execute(\"SELECT source_collection, count(*) FROM my_data GROUP BY source_collection ORDER BY count(*) DESC;\").fetchall()\n",
    "elapsed = time.time() - start_time\n",
    "print(\"   Results:\")\n",
    "for source, count in result:\n",
    "    print(f\"     {source}: {count:,}\")\n",
    "print(f\"   Time: {elapsed:.2f} seconds\")\n",
    "print(f\"   Data read: Only source_collection column + metadata\\n\")\n",
    "\n",
    "# Test 3: Simple column stats - Reads one column\n",
    "print(\"3. Latitude statistics - Single column read\")\n",
    "start_time = time.time()\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        count(*) as total,\n",
    "        count(sample_location_latitude) as non_null,\n",
    "        min(sample_location_latitude) as min_lat,\n",
    "        max(sample_location_latitude) as max_lat,\n",
    "        avg(sample_location_latitude) as avg_lat\n",
    "    FROM my_data;\n",
    "\"\"\").fetchone()\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"   Total records: {result[0]:,}\")\n",
    "print(f\"   Non-null coordinates: {result[1]:,}\")\n",
    "print(f\"   Latitude range: {result[2]:.3f} to {result[3]:.3f}\")\n",
    "print(f\"   Average latitude: {result[4]:.3f}\")\n",
    "print(f\"   Time: {elapsed:.2f} seconds\")\n",
    "print(f\"   Data read: Only latitude column\\n\")\n",
    "\n",
    "# Test 4: More complex query - Still efficient due to columnar format\n",
    "print(\"4. Geographic bounding box filter - Selective read\")\n",
    "start_time = time.time()\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT count(*) \n",
    "    FROM my_data \n",
    "    WHERE sample_location_longitude BETWEEN -125 AND -66\n",
    "      AND sample_location_latitude BETWEEN 24 AND 50;\n",
    "\"\"\").fetchone()\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"   Records in continental US bounds: {result[0]:,}\")\n",
    "print(f\"   Time: {elapsed:.2f} seconds\")\n",
    "print(f\"   Data read: Only lon/lat columns + pushdown filtering\\n\")\n",
    "\n",
    "con.close()\n",
    "\n",
    "print(\"=== Key Insights ===\")\n",
    "print(\"• COUNT(*) is nearly instant - uses only Parquet metadata\")\n",
    "print(\"• Aggregations by categorical columns are very fast\")\n",
    "print(\"• Single-column operations read only that column\")\n",
    "print(\"• Filtering is pushed down to the file level\")\n",
    "print(\"• This approach scales to files much larger than available RAM\")\n",
    "print(\"\\nThis is why DuckDB + remote Parquet is perfect for exploratory data analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e7ad132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Traditional vs DuckDB Approach Comparison ===\n",
      "\n",
      "Traditional Approach (e.g., pandas.read_parquet()):\n",
      "• Download entire file: 300 MB\n",
      "• Load into memory: ~300-600 MB (depending on data types)\n",
      "• Process in Python: Limited by single-core performance\n",
      "• Time for COUNT(*): 30-60 seconds + download time\n",
      "• Memory requirement: > 1GB\n",
      "\n",
      "DuckDB + Remote Parquet Approach:\n",
      "• Download for COUNT(*): < 1 KB (just metadata)\n",
      "• Memory usage: < 10 MB\n",
      "• Process with optimized engine: Multi-threaded, vectorized\n",
      "• Time for COUNT(*): 1-3 seconds\n",
      "• Memory requirement: Minimal\n",
      "\n",
      "=== When to Use Each Approach ===\n",
      "\n",
      "Use DuckDB + Remote Parquet when:\n",
      "✅ Doing exploratory analysis (counts, aggregations, sampling)\n",
      "✅ Working with large files that don't fit in memory\n",
      "✅ Need fast iteration on different queries\n",
      "✅ Bandwidth is limited\n",
      "✅ Working in cloud environments (Colab, Binder)\n",
      "\n",
      "Consider local download when:\n",
      "• Need to do complex row-by-row operations\n",
      "• Performing many different analyses on the same data\n",
      "• Have unreliable network connection\n",
      "• Need to use libraries that require full data in memory\n",
      "\n",
      "=== Best Practices for Large Remote Parquet Files ===\n",
      "1. Start with DuckDB for exploration and understanding\n",
      "2. Use COUNT(*), value_counts(), and aggregations to understand structure\n",
      "3. Filter data remotely before downloading subsets\n",
      "4. Cache filtered/sampled results locally for visualization\n",
      "5. Only download full dataset when absolutely necessary\n"
     ]
    }
   ],
   "source": [
    "# Compare with what traditional approaches would require\n",
    "print(\"=== Traditional vs DuckDB Approach Comparison ===\\n\")\n",
    "\n",
    "file_size_mb = 300  # Approximate size of the parquet file\n",
    "\n",
    "print(\"Traditional Approach (e.g., pandas.read_parquet()):\")\n",
    "print(f\"• Download entire file: {file_size_mb} MB\")\n",
    "print(\"• Load into memory: ~300-600 MB (depending on data types)\")\n",
    "print(\"• Process in Python: Limited by single-core performance\")\n",
    "print(\"• Time for COUNT(*): 30-60 seconds + download time\")\n",
    "print(\"• Memory requirement: > 1GB\")\n",
    "print()\n",
    "\n",
    "print(\"DuckDB + Remote Parquet Approach:\")\n",
    "print(\"• Download for COUNT(*): < 1 KB (just metadata)\")\n",
    "print(\"• Memory usage: < 10 MB\")\n",
    "print(\"• Process with optimized engine: Multi-threaded, vectorized\")\n",
    "print(\"• Time for COUNT(*): 1-3 seconds\")\n",
    "print(\"• Memory requirement: Minimal\")\n",
    "print()\n",
    "\n",
    "print(\"=== When to Use Each Approach ===\")\n",
    "print()\n",
    "print(\"Use DuckDB + Remote Parquet when:\")\n",
    "print(\"✅ Doing exploratory analysis (counts, aggregations, sampling)\")\n",
    "print(\"✅ Working with large files that don't fit in memory\")\n",
    "print(\"✅ Need fast iteration on different queries\")\n",
    "print(\"✅ Bandwidth is limited\")\n",
    "print(\"✅ Working in cloud environments (Colab, Binder)\")\n",
    "print()\n",
    "\n",
    "print(\"Consider local download when:\")\n",
    "print(\"• Need to do complex row-by-row operations\")\n",
    "print(\"• Performing many different analyses on the same data\")\n",
    "print(\"• Have unreliable network connection\")\n",
    "print(\"• Need to use libraries that require full data in memory\")\n",
    "print()\n",
    "\n",
    "print(\"=== Best Practices for Large Remote Parquet Files ===\")\n",
    "print(\"1. Start with DuckDB for exploration and understanding\")\n",
    "print(\"2. Use COUNT(*), value_counts(), and aggregations to understand structure\")\n",
    "print(\"3. Filter data remotely before downloading subsets\")\n",
    "print(\"4. Cache filtered/sampled results locally for visualization\")\n",
    "print(\"5. Only download full dataset when absolutely necessary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "336bda9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Efficient Data Preparation for Visualization ===\n",
      "\n",
      "1. Understanding data structure...\n",
      "1. Understanding data structure...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de318965e4de40829b1a5c13fd27eee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Total records: 6,680,932\n",
      "   Records with coordinates: 5,980,282 (89.5%)\n",
      "   Time: 13.08 seconds\n",
      "\n",
      "2. Creating stratified sample for visualization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "840b3d6068e84ab6a3699ba42f3c7879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sample size: 20,000 records\n",
      "   Columns: ['sample_identifier', 'source_collection', 'longitude', 'latitude', 'has_material_category', 'label']\n",
      "   Memory usage: ~6.7 MB\n",
      "   Time: 60.76 seconds\n",
      "   Data transferred: ~0.9 MB (estimated)\n",
      "\n",
      "   Sample distribution by source:\n",
      "     SESAR: 5,000\n",
      "     OPENCONTEXT: 5,000\n",
      "     SMITHSONIAN: 5,000\n",
      "     GEOME: 5,000\n",
      "\n",
      "3. Efficient caching strategy...\n",
      "   • Save sample as local Parquet file for reuse\n",
      "   • Use compressed format to minimize storage\n",
      "   • Include metadata about sampling method\n",
      "\n",
      "=== Key Takeaways ===\n",
      "• Remote querying allows efficient exploration without large downloads\n",
      "• Stratified sampling maintains data representativeness\n",
      "• 50K sample points are sufficient for most visualization needs\n",
      "• Transferring 50K records vs 6M records: ~40x less data transfer\n",
      "• This approach works well for both local analysis and cloud environments\n",
      "\n",
      "This sampled data would be perfect for Lonboard visualization!\n"
     ]
    }
   ],
   "source": [
    "# Practical example: Efficiently preparing data for visualization\n",
    "print(\"=== Efficient Data Preparation for Visualization ===\\n\")\n",
    "\n",
    "con = duckdb.connect(database=':memory:', read_only=False)\n",
    "remote_url = 'https://z.rslv.xyz/10.5281/zenodo.15278210/isamples_export_2025_04_21_16_23_46_geo.parquet'\n",
    "con.execute(f\"SET VARIABLE parquet_path = '{remote_url}';\")\n",
    "con.execute(\"CREATE TEMP VIEW my_data AS SELECT(*) FROM read_parquet(getvariable('parquet_path'));\")\n",
    "\n",
    "# Step 1: Understand the data structure\n",
    "print(\"1. Understanding data structure...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Get basic counts\n",
    "total_count = con.execute(\"SELECT count(*) FROM my_data\").fetchone()[0]\n",
    "geo_count = con.execute(\"\"\"\n",
    "    SELECT count(*) FROM my_data \n",
    "    WHERE sample_location_latitude IS NOT NULL \n",
    "    AND sample_location_longitude IS NOT NULL\n",
    "\"\"\").fetchone()[0]\n",
    "\n",
    "print(f\"   Total records: {total_count:,}\")\n",
    "print(f\"   Records with coordinates: {geo_count:,} ({geo_count/total_count*100:.1f}%)\")\n",
    "print(f\"   Time: {time.time() - start_time:.2f} seconds\\n\")\n",
    "\n",
    "# Step 2: Sample data efficiently for visualization\n",
    "print(\"2. Creating stratified sample for visualization...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Get sample that maintains source collection proportions\n",
    "# Fixed version - avoid correlated subqueries in LIMIT clause\n",
    "sample_query = \"\"\"\n",
    "    WITH collection_counts AS (\n",
    "        SELECT source_collection, count(*) as total_count\n",
    "        FROM my_data \n",
    "        WHERE sample_location_latitude IS NOT NULL \n",
    "        AND sample_location_longitude IS NOT NULL\n",
    "        GROUP BY source_collection\n",
    "    ),\n",
    "    collection_samples AS (\n",
    "        SELECT \n",
    "            source_collection,\n",
    "            CASE \n",
    "                WHEN total_count > 5000 THEN 5000\n",
    "                ELSE total_count \n",
    "            END as sample_size\n",
    "        FROM collection_counts\n",
    "    ),\n",
    "    numbered_data AS (\n",
    "        SELECT \n",
    "            sample_identifier,\n",
    "            source_collection,\n",
    "            sample_location_longitude as longitude,\n",
    "            sample_location_latitude as latitude,\n",
    "            has_material_category,\n",
    "            label,\n",
    "            row_number() OVER (PARTITION BY source_collection ORDER BY RANDOM()) as rn\n",
    "        FROM my_data \n",
    "        WHERE sample_location_latitude IS NOT NULL \n",
    "        AND sample_location_longitude IS NOT NULL\n",
    "    )\n",
    "    SELECT \n",
    "        nd.sample_identifier,\n",
    "        nd.source_collection,\n",
    "        nd.longitude,\n",
    "        nd.latitude,\n",
    "        nd.has_material_category,\n",
    "        nd.label\n",
    "    FROM numbered_data nd\n",
    "    INNER JOIN collection_samples cs ON nd.source_collection = cs.source_collection\n",
    "    WHERE nd.rn <= cs.sample_size\n",
    "    LIMIT 50000;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the sampling query\n",
    "sample_result = con.execute(sample_query).fetchdf()\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"   Sample size: {len(sample_result):,} records\")\n",
    "print(f\"   Columns: {list(sample_result.columns)}\")\n",
    "print(f\"   Memory usage: ~{sample_result.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB\")\n",
    "print(f\"   Time: {elapsed:.2f} seconds\")\n",
    "print(f\"   Data transferred: ~{len(sample_result) * 6 * 8 / 1024 / 1024:.1f} MB (estimated)\\n\")\n",
    "\n",
    "# Show sample distribution\n",
    "print(\"   Sample distribution by source:\")\n",
    "sample_dist = sample_result['source_collection'].value_counts()\n",
    "for source, count in sample_dist.items():\n",
    "    print(f\"     {source}: {count:,}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Step 3: Show how this could be saved for efficient reuse\n",
    "print(\"3. Efficient caching strategy...\")\n",
    "print(\"   • Save sample as local Parquet file for reuse\")\n",
    "print(\"   • Use compressed format to minimize storage\")\n",
    "print(\"   • Include metadata about sampling method\")\n",
    "\n",
    "# Example of saving (uncommented for demo)\n",
    "# sample_result.to_parquet('/tmp/isamples_visualization_sample.parquet', compression='snappy')\n",
    "\n",
    "con.close()\n",
    "\n",
    "print(\"\\n=== Key Takeaways ===\")\n",
    "print(\"• Remote querying allows efficient exploration without large downloads\")\n",
    "print(\"• Stratified sampling maintains data representativeness\")\n",
    "print(\"• 50K sample points are sufficient for most visualization needs\")\n",
    "print(\"• Transferring 50K records vs 6M records: ~40x less data transfer\")\n",
    "print(\"• This approach works well for both local analysis and cloud environments\")\n",
    "print(\"\\nThis sampled data would be perfect for Lonboard visualization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc7bfc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isamples-python-3.12.9",
   "language": "python",
   "name": "isamples-python-3.12.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose of this notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "\n",
    "def in_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "def install_dependencies_from_pyproject():\n",
    "    # URL to raw pyproject.toml file in your GitHub repository\n",
    "    pyproject_url = \"https://raw.githubusercontent.com/rdhyee/isamples-python/exploratory/pyproject.toml\"\n",
    "    \n",
    "    with urlopen(pyproject_url) as response:\n",
    "        pyproject_content = response.read().decode()\n",
    "    \n",
    "    # Parse the TOML content\n",
    "    import toml\n",
    "    pyproject_data = toml.loads(pyproject_content)\n",
    "    \n",
    "    # Extract dependencies\n",
    "    dependencies = pyproject_data.get('tool', {}).get('poetry', {}).get('dependencies', {})\n",
    "    \n",
    "    # Install each dependency\n",
    "    for package, version in dependencies.items():\n",
    "        if isinstance(version, str):\n",
    "            subprocess.run(['pip', 'install', f\"{package}{version}\"])\n",
    "        elif isinstance(version, dict):\n",
    "            # Handle more complex version specifications\n",
    "            version_str = version.get('version', '')\n",
    "            subprocess.run(['pip', 'install', f\"{package}{version_str}\"])\n",
    "\n",
    "if in_colab():\n",
    "    # Install toml parser first\n",
    "    subprocess.run(['pip', 'install', 'toml'])\n",
    "    install_dependencies_from_pyproject()\n",
    "    # pip install git+https://github.com/rdhyee/isamples-python.git@exploratory#egg=isamples_client\n",
    "    subprocess.run(['pip', 'install', 'git+https://github.com/rdhyee/isamples-python.git@exploratory#egg=isamples_client'])\n",
    "\n",
    "    from google.colab import output\n",
    "    output.enable_custom_widget_manager()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pybash macro\n",
    "# https://stackoverflow.com/a/67029719/7782\n",
    "from IPython import get_ipython\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "ipython = get_ipython()\n",
    "\n",
    "@register_cell_magic\n",
    "def pybash(line, cell):\n",
    "    ipython.run_cell_magic('bash', '', cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path as P\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This notebook is part of the iSamples repository. To ensure we can import the iSamples library\n",
    "directly from the source code in this repository, we manually manipulate sys.path. This allows us\n",
    "to include the iSamples library without needing to install it as a separate package. By adding the\n",
    "parent directory of the iSamples library to sys.path, we can import the library modules directly.\n",
    "\"\"\"\n",
    "\n",
    "# path of the current notebook\n",
    "lib_path = P.resolve(P.cwd() / '../../src')\n",
    "# check to see whether a directory which ends with \"isamples-python/src\" exists in the path\n",
    "if lib_path.exists() and not any([path.endswith(\"isamples-python/src\") for path in sys.path]):\n",
    "    sys.path.insert(0, str(lib_path))\n",
    "\n",
    "import isamples_client\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-28T13:01:34.491834Z",
     "start_time": "2023-10-28T13:01:34.342886Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import httpx\n",
    "import xarray\n",
    "import pysolr\n",
    "import multidict\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from urllib.parse import quote\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from isamples_client import IsbClient, MAJOR_FIELDS, FL_DEFAULT, FACET_FIELDS_DEFAULT, FACET_RANGE_FIELDS_DEFAULT, ISAMPLES_SOURCES\n",
    "\n",
    "# creating a subclass of IsbClient because we're still working out the best ways to interact with the API\n",
    "from isamples_client import IsbClient2\n",
    "\n",
    "from isamples_client import format_date_for_solr, create_date_range_query, filter_null_values\n",
    "from isamples_client import monkey_patch_select, SWITCH_TO_POST\n",
    "\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "# monkeypatch pysolr?\n",
    "monkey_patch_select(active=True)\n",
    "SWITCH_TO_POST = 10000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The overall iSamples API\n",
    "\n",
    "* https://central.isample.xyz/isamples_central/docs is the swagger UI\n",
    "* https://central.isample.xyz/isamples_central/openapi.json is the OpenAPI spec file for the iSamples API.\n",
    "\n",
    "There are Python libraries for enabling devs to interact with an API specified by an OpenAPI spec, but my current thought is that they don't make life any easier than to work with pieces of the API by hand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://central.isample.xyz/isamples_central/openapi.json is an OPENAPI 3.x spec\n",
    "\n",
    "OPENAPI_URL = 'https://central.isample.xyz/isamples_central/openapi.json'\n",
    "r = httpx.get(OPENAPI_URL)\n",
    "r.json()['paths'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# /thing/select: Solr-based select interface\n",
    "\n",
    "The most important part of the iSamples API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# focus on /thing/select endpoint\n",
    "r = httpx.get(OPENAPI_URL)\n",
    "r.json()['paths']['/thing/select']['get']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# documentation about Solr query language\n",
    "\n",
    "[The Standard Query Parser | Apache Solr Reference Guide 8.11](https://solr.apache.org/guide/8_11/the-standard-query-parser.html#standard-query-parser-parameters):\n",
    "\n",
    "> Solr’s default Query Parser is also known as the “lucene” parser.   \n",
    "> [....]   \n",
    "> q: Defines a query using standard query syntax. This parameter is mandatory\n",
    "\n",
    "Note [Differences between Lucene’s Classic Query Parser and Solr’s Standard Query Parser](https://solr.apache.org/guide/8_11/the-standard-query-parser.html#differences-between-lucenes-classic-query-parser-and-solrs-standard-query-parser)\n",
    "\n",
    "there are \"existence searches\" [The Standard Query Parser | Apache Solr Reference Guide 8.11](https://solr.apache.org/guide/8_11/the-standard-query-parser.html#existence-searches):\n",
    "\n",
    "> An existence search for a field matches all documents where a value exists for that field. To query for a field existing, simply use a wildcard instead of a term in the search.\n",
    ">\n",
    "> field:*\n",
    ">\n",
    "> A field will be considered to \"exist\" if it has any value, even values which are often considered \"not existent\". (e.g., NaN, \"\", etc.)\n",
    "\n",
    "Good tutorial on the query syntax of Solr (apart from the official documentation): [Solr Query Syntax and Examples](https://yonik.com/solr/query-syntax/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## why fq, rather than q, is the key parameter to vary\n",
    "\n",
    "We set `q=*:*` and vary `fq`.  By doing so, you can cache results by varying `fq`. Also changing `fq` doesn't change the score.  (A better explanation should be put here because the distinction between `q` and `fq` is something that is not obvious to people new to Solr. ([Difference between q and fq in Solr - Stack Overflow](https://stackoverflow.com/questions/20988516/difference-between-q-and-fq-in-solr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translate the UI from https://central.isample.xyz/isamples_central/ui into widgetized forms to formulate query\n",
    "\n",
    "* display number of hits\n",
    "* display facets\n",
    "\n",
    "map\n",
    "dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IsbClient2\n",
    "\n",
    "TO DO: fold IsbClient2 into IsbClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = IsbClient2()\n",
    "\n",
    "# get OpenContext sourced records\n",
    "fq = cli._fq_from_kwargs(source=('OPENCONTEXT',), collection_date_end=str(datetime.now().year))\n",
    "\n",
    "params = cli.default_search_params(fq=fq, fl=FL_DEFAULT, rows=100, **FACET_RANGE_FIELDS_DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "facets = cli.facets(params=params)\n",
    "facets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the /thing/select endpoint directly\n",
    "response = cli.search(params=params, thingselect=True)\n",
    "# print number of hits\n",
    "print (response['response']['numFound'])\n",
    "\n",
    "df = DataFrame(response)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['facet_fields', 'facet_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's the number of records that are geocoded in OpenContext\n",
    "\n",
    "# get OpenContext sourced records\n",
    "# fq=-lat:[* TO *] AND -long:[* TO *]&rows=0\n",
    "# fq = cli._fq_from_kwargs(source=('OPENCONTEXT',), collection_date_end=str(datetime.now().year))\n",
    "geodict = multidict.MultiDict({\n",
    "  '-producedBy_samplingSite_location_latitude':'[* TO *]', \n",
    "  '-producedBy_samplingSite_location_longitude': '[* TO *]'\n",
    "})\n",
    "\n",
    "fq = cli._fq_from_kwargs(source=('OPENCONTEXT',), collection_date_end=str(datetime.now().year), \n",
    "        _multi=geodict )\n",
    "\n",
    "params = cli.default_search_params(fq=fq, fl=FL_DEFAULT, rows=100, **FACET_RANGE_FIELDS_DEFAULT)\n",
    "\n",
    "# use the /thing/select endpoint directly\n",
    "response = cli.search(params=params, thingselect=True)\n",
    "# print number of hits\n",
    "print (response['response']['numFound'])\n",
    "results = islice(response, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli._fq_from_kwargs(source=('OPENCONTEXT',), collection_date_end=str(datetime.now().year), \n",
    "        producedBy_samplingSite_location_latitude='[* TO *]', producedBy_samplingSite_location_longitude='[* TO *]' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "# monkeypatch pysolr?\n",
    "monkey_patch_select(active=True)\n",
    "SWITCH_TO_POST = 100000\n",
    "\n",
    "cli = IsbClient2()\n",
    "# build fq: OpenContext source and search for bone\n",
    "fq = cli._fq_from_kwargs(source=('OPENCONTEXT',), searchText=\"bone\")\n",
    "params = cli.default_search_params(fq=fq, fl=FL_DEFAULT, rows=100, **FACET_RANGE_FIELDS_DEFAULT)\n",
    "\n",
    "# use pysolr to get the results\n",
    "response = cli.search(params=params)\n",
    "# print number of hits\n",
    "print (len(response))\n",
    "results = islice(response, 1000)\n",
    "\n",
    "df = DataFrame(results)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli.record_count(params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# goal: figure out how to get facet counts and pivoting\n",
    "\n",
    "cli = IsbClient2()\n",
    "# build fq: OpenContext source and search for bone\n",
    "fq = cli._fq_from_kwargs(source=('OPENCONTEXT',), searchText=\"bone\")\n",
    "params = cli.default_search_params(fq=fq, fl=FL_DEFAULT, rows=100, **FACET_RANGE_FIELDS_DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp0 = cli.search(params=params, thingselect=True)\n",
    "resp0.get(\"facet_counts\",{}).get(\"facet_fields\",{}).keys() #.get(field, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the data coming back and see how to make sense of them.\n",
    "# expect the columns in the DataFrame to be a proper subset of FL_DEFAULT\n",
    "\n",
    "\n",
    "def set_diff(a, b):\n",
    "    return set(a) - set(b), set(b) - set(a)\n",
    "    \n",
    "\n",
    "\n",
    "assert set(df.columns) - set(FL_DEFAULT) == set()\n",
    "\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can I get type information from the API?\n",
    "# it doesn't seem like /thing/select will return type information\n",
    "\n",
    "# save a copy of df to df0\n",
    "df0 = df.copy()\n",
    "\n",
    "# df.infer_objects().dtypes\n",
    "df = df.convert_dtypes()\n",
    "\n",
    "# some of the columns are datetimes\n",
    "for k in ['sourceUpdatedTime', 'producedBy_resultTime', 'producedBy_resultTimeRange']:\n",
    "    df[k] = pd.to_datetime(df[k], errors='coerce').dt.tz_localize(None)\n",
    "\n",
    "# spit out to Excel to look at the data in spreadsheet form\n",
    "df.to_excel('bone.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sourceUpdatedTime'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert the datetime64 column to just date\n",
    "df['sourceUpdatedTime'] = df['sourceUpdatedTime'].dt.date\n",
    "\n",
    "# Plot a histogram\n",
    "plt.figure(figsize=(10,6))\n",
    "df['sourceUpdatedTime'].hist(rwidth=0.9, bins=30)\n",
    "plt.title('Distribution of sourceUpdatedTime')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipydatagrid as ipg\n",
    "ipg.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the df into ipydatagrid\n",
    "from ipydatagrid import DataGrid\n",
    "\n",
    "dg = DataGrid(df, editable=True)\n",
    "dg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what type of events are supported by ipydatagrid\n",
    "# selection events, what rows are shown? what columns?\n",
    "dg.selection_mode, dg.selected_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_selection_from_dg(selection, total_rows, total_columns):\n",
    "    # Initialize counters\n",
    "    row_counts = {}\n",
    "    \n",
    "    # Process each selected cell\n",
    "    for cell in selection:\n",
    "        row_index = cell['r']\n",
    "        # Increment the row counter for each occurrence\n",
    "        if row_index in row_counts:\n",
    "            row_counts[row_index] += 1\n",
    "        else:\n",
    "            row_counts[row_index] = 1\n",
    "    \n",
    "    # Analyze the counts to determine full row selections\n",
    "    full_rows_selected = [row for row, count in row_counts.items() if count == total_columns]\n",
    "    \n",
    "    # Report findings\n",
    "    if full_rows_selected:\n",
    "        print(f\"Full rows selected: Rows {full_rows_selected}\")\n",
    "    else:\n",
    "        print(\"No full rows selected.\")\n",
    "\n",
    "# Assuming you have a DataFrame 'df' and a DataGrid 'dg' with selections as described\n",
    "total_rows, total_columns = df.shape  # As per your DataFrame's shape\n",
    "\n",
    "# Example selection (assuming this comes from dg.selected_cells)\n",
    "selected_cells =  dg.selected_cells\n",
    "# Analyze the selection\n",
    "analyze_selection_from_dg(selected_cells, total_rows, total_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Jupyter widgets to allow for change in searchText and display the number of results in a output widget\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "# monkeypatch pysolr?\n",
    "monkey_patch_select(active=True)\n",
    "SWITCH_TO_POST = 100000\n",
    "\n",
    "cli = IsbClient2()\n",
    "\n",
    "# build fq: OpenContext source and search for bone\n",
    "fq = cli._fq_from_kwargs(searchText=\"bone\")\n",
    "params = cli.default_search_params(fq=fq, fl=FL_DEFAULT, rows=10, **FACET_RANGE_FIELDS_DEFAULT)\n",
    "query = cli.search(params=params)\n",
    "num_hits = len(query)\n",
    "\n",
    "# Create a text input widget\n",
    "search_text = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type something',\n",
    "    description='Search:',\n",
    ")\n",
    "\n",
    "# add a date range widget\n",
    "\n",
    "producedby_range_slider = widgets.IntRangeSlider(\n",
    "    value=[1800, 2024],\n",
    "    min=1800,\n",
    "    max=2024,\n",
    "    step=1,\n",
    "    description='ProducedBy ResultTime:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d',\n",
    ")\n",
    "\n",
    "# Create an output widget\n",
    "output = widgets.Output()\n",
    "\n",
    "# Define a function to handle changes to the text input\n",
    "def on_text_change(change):\n",
    "    output.clear_output()  # Clear the previous results\n",
    "\n",
    "    # Get the new search text and range values\n",
    "    new_search_text = search_text.value\n",
    "    new_range = producedby_range_slider.value\n",
    "\n",
    "    fq = cli._fq_from_kwargs(searchText=new_search_text, collection_date_start=new_range[0], collection_date_end=new_range[1])\n",
    "    params = cli.default_search_params(fq=fq, fl=FL_DEFAULT, rows=10, **FACET_RANGE_FIELDS_DEFAULT)\n",
    "    query = cli.search(params=params)\n",
    "    num_hits = len(query)\n",
    "\n",
    "    with output:\n",
    "        print(f\"Number of hits: {num_hits}\")  # Display the new search text\n",
    "\n",
    "# Attach the event handler to the text input and range slider\n",
    "search_text.observe(on_text_change, names='value')\n",
    "producedby_range_slider.observe(on_text_change, names='value')\n",
    "\n",
    "# Display the widgets\n",
    "# align the widgets vertically\n",
    "display(widgets.VBox([producedby_range_slider, search_text, output]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out the call to iSamples using httpx to compare get vs post\n",
    "\n",
    "import httpx\n",
    "ISB_SERVER = \"https://central.isample.xyz/isamples_central/\"\n",
    "\n",
    "r = httpx.request('GET', f'{ISB_SERVER}/thing/select', params=params)\n",
    "r.json()['response']['numFound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a post request version\n",
    "\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "headers = {\n",
    "    \"Content-type\": \"application/x-www-form-urlencoded; charset=utf-8\"\n",
    "}\n",
    "\n",
    "params_encoded = urlencode(params)\n",
    "r = httpx.post(f'{ISB_SERVER}/thing/select', data=params_encoded, headers=headers)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(query.raw_response.keys()) == set(['responseHeader', 'response', 'nextCursorMark', 'facet_counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_keys(['facet_queries', 'facet_fields', 'facet_ranges', 'facet_intervals', 'facet_heatmaps'])\n",
    "query.raw_response['facet_counts'].keys()\n",
    "\n",
    "query.raw_response['facet_counts']['facet_fields'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.raw_response['facet_counts']['facet_fields']['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipytree import Tree, Node\n",
    "from ipyleaflet import Map, Marker\n",
    "from ipywidgets import HBox, link, Layout\n",
    "\n",
    "m = Map(center=[47.51, 4.04], zoom=4, layout=Layout(height='400px'))\n",
    "tree = Tree()\n",
    "tree.layout.width = '40%'\n",
    "box = HBox([tree, m])\n",
    "\n",
    "markers_node = Node('Markers')\n",
    "tree.add_node(markers_node)\n",
    "\n",
    "layers_node = Node('Layers', icon='map')\n",
    "tree.add_node(layers_node)\n",
    "\n",
    "cities = [\n",
    "    {'name': 'London', 'location': [51.5074, 0.1278]},\n",
    "    {'name': 'Paris', 'location': [48.8566, 2.3522]},\n",
    "    {'name': 'Barcelona', 'location': [41.31, 2.109]}\n",
    "]\n",
    "\n",
    "for city in cities:\n",
    "    marker = Marker(location=city.get('location'))\n",
    "    node = Node(city.get('name'), icon='map-marker')\n",
    "\n",
    "    link((marker, 'visible'), (node, 'selected'))\n",
    "\n",
    "    m.add_layer(marker)\n",
    "    markers_node.add_node(node)\n",
    "\n",
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query.raw_response.keys() --> dict_keys(['responseHeader', 'response', 'nextCursorMark', 'facet_counts'])\n",
    "query.raw_response['facet_counts']['facet_ranges'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys: dict_keys(['facet_queries', 'facet_fields', 'facet_ranges', 'facet_intervals', 'facet_heatmaps'])\n",
    "query.raw_response['facet_counts']['facet_ranges'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'responseHeader', 'index', 'schema', 'info'\n",
    "r = cli._request(\"thing/select/info\")\n",
    "r.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r['schema']['fields'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timeout internal server error -- skip trying to query thing/types right now. https://github.com/isamplesorg/isamples_inabox/issues/351\n",
    "if False:\n",
    "    r = cli._request(\"thing/types\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# types and classnames for all the fields on the system\n",
    "Counter([(x['type'], r['schema']['types'][x['type']]['className']) for x in r['schema']['fields'].values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g, I for Indexed, T for Tokenized, S for Stored, etc.\n",
    "r['info']['key']\n",
    "\n",
    "# ['fields', 'dynamicFields', 'uniqueKeyField', 'similarity', 'types']\n",
    "r['schema'].keys()\n",
    "\n",
    "# get the fields -- 78 of them\n",
    "print (\"number of fields\", len(r['schema']['fields'].keys()))\n",
    "\n",
    "field_names = cli.field_names()\n",
    "print(\"number of field names (another way to access)\", len(field_names))\n",
    "\n",
    "print (\"types for the major fields\")\n",
    "[(k,v['type'], r['schema']['types'][v['type']]['className'] ) for (k,v) in r['schema']['fields'].items() if k in MAJOR_FIELDS.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "url = 'https://central.isample.xyz/isamples_central/thing/select?q=*:*&fl=searchText%20authorizedBy%20producedBy_resultTimeRange%20hasContextCategory%20curation_accessContraints%20curation_description_text%20curation_label%20curation_location%20curation_responsibility%20description_text%20id%20informalClassification%20keywords%20label%20hasMaterialCategory%20producedBy_description_text%20producedBy_hasFeatureOfInterest%20producedBy_label%20producedBy_responsibility%20producedBy_resultTime%20producedBy_samplingSite_description_text%20producedBy_samplingSite_label%20producedBy_samplingSite_location_elevationInMeters%20producedBy_samplingSite_location_latitude%20producedBy_samplingSite_location_longitude%20producedBy_samplingSite_placeName%20registrant%20samplingPurpose%20source%20sourceUpdatedTime%20producedBy_samplingSite_location_rpt%20hasSpecimenCategory&fq=producedBy_resultTimeRange%3A%5B1800%20TO%202023%5D&fq=source%3A(%22OPENCONTEXT%22%20OR%20%22SESAR%22)&fq=-relation_target%3A*&facet.field=authorizedBy&facet.field=hasContextCategory&facet.field=hasMaterialCategory&facet.field=registrant&facet.field=source&facet.field=hasSpecimenCategory&facet.range=producedBy_resultTimeRange&facet.range.gap=%2B1YEARS&facet.range.start=1800-01-01T00:00:00Z&facet.range.end=2023-01-01T00:00:00Z&f.registrant.facet.sort=count&f.source.facet.sort=index&rows=20&facet.limit=-1&facet.sort=index&&start=0&facet=on&wt=json'\n",
    "\n",
    "parsed_url = urlparse(url)\n",
    "query_params = parse_qs(parsed_url.query)\n",
    "\n",
    "# The result is a dictionary where each key is associated with a list of values.\n",
    "# You can iterate over this dictionary to process your parameters as needed.\n",
    "for key, values in query_params.items():\n",
    "    print(f\"{key}: {values}\")\n",
    "\n",
    "# If you need each key to have a single value (taking the first value if multiple are present),\n",
    "# you can do the following:\n",
    "single_value_params = {key: values[0] for key, values in query_params.items()}\n",
    "print(single_value_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplest query -- default\n",
    "\n",
    "cli._request(\"thing/select\", params={'q': '*:*', 'start':0, 'rows': 10, \n",
    "        'fq': ['producedBy_resultTimeRange:[1800 TO 2023]', 'source:(OPENCONTEXT or SESAR)', '-relation_target:*'],\n",
    "        'facet.field': ['authorizedBy', 'hasContextCategory', 'hasMaterialCategory', 'registrant', 'source', 'hasSpecimenCategory'],\n",
    "        'facet': 'on',\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I *think* I had ChatGPT parse the parameters and give me the following interpretation:\n",
    "\n",
    "Let's break down these parameters, which are used for querying a Solr search engine. Solr is an open-source search platform that provides a wide range of capabilities for text search and faceted search, among other features.\n",
    "\n",
    "q: This parameter specifies the query. Here, *:* is a wildcard query, meaning it matches all documents in the Solr index.\n",
    "\n",
    "\n",
    "[fl](https://solr.apache.org/guide/8_11/common-query-parameters.html#fl-field-list-parameter): This stands for \"field list\". It specifies the fields to return in the result. In your query, a long list of fields like searchText, authorizedBy, producedBy_resultTimeRange, etc., are included. Only these fields will be returned for each document in the search results.\n",
    "\n",
    "fq: This is the \"filter query\". It filters the results returned by the main query (q) without influencing the score. Here, there are three filters applied:\n",
    "\n",
    "> producedBy_resultTimeRange:[1800 TO 2023] filters documents to those produced between the years 1800 and 2023.\n",
    "source:(OPENCONTEXT) filters documents where the source field matches \"OPENCONTEXT\".\n",
    "-relation_target:* excludes documents where the relation_target field exists.\n",
    "facet.field: Faceting is used to aggregate data based on a field. This parameter specifies the fields for which you want to see facet counts. Facets on fields like authorizedBy, hasContextCategory, etc., are requested.\n",
    "\n",
    "\n",
    "facet.range, facet.range.gap, facet.range.start, and facet.range.end: These parameters are used for range faceting. You are faceting on the producedBy_resultTimeRange field, starting from \"1800-01-01T00:00:00Z\" to \"2023-01-01T00:00:00Z\", with a gap of \"+1YEARS\". This means it will provide counts for each year in this range.\n",
    "\n",
    "f.registrant.facet.sort and f.source.facet.sort: These are sorting instructions for the facets. The registrant facet is sorted by count, and the source facet is sorted by index.\n",
    "\n",
    "rows: This specifies the number of documents to return. In your query, it's set to 20.\n",
    "\n",
    "facet.limit: This limits the number of facet values returned for each facet field. -1 means no limit.\n",
    "\n",
    "facet.sort: It dictates how to sort the facet fields. Here, it's sorted by index.\n",
    "\n",
    "start: This is the offset in the complete result set for pagination. It tells Solr where to start in the list of results (useful for paging through results).\n",
    "\n",
    "facet: When set to 'on', it enables faceting.\n",
    "\n",
    "wt: This stands for \"writer type\" and specifies the output format. Here, 'json' indicates that the response should be in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "\n",
    "\n",
    "url = \"https://central.isample.xyz/isamples_central/thing/select\"\n",
    "params = {\n",
    "    'q': '*:*',\n",
    "    'fl': 'searchText authorizedBy producedBy_resultTimeRange hasContextCategory curation_accessContraints curation_description_text curation_label curation_location curation_responsibility description_text id informalClassification keywords label hasMaterialCategory producedBy_description_text producedBy_hasFeatureOfInterest producedBy_label producedBy_responsibility producedBy_resultTime producedBy_samplingSite_description_text producedBy_samplingSite_label producedBy_samplingSite_location_elevationInMeters producedBy_samplingSite_location_latitude producedBy_samplingSite_location_longitude producedBy_samplingSite_placeName registrant samplingPurpose source sourceUpdatedTime producedBy_samplingSite_location_rpt hasSpecimenCategory',\n",
    "    'fq': ['producedBy_resultTimeRange:[1800 TO 2023]', 'source:(OPENCONTEXT)', '-relation_target:*'],\n",
    "    'facet.field': ['authorizedBy', 'hasContextCategory', 'hasMaterialCategory', 'registrant', 'source', 'hasSpecimenCategory'],\n",
    "    'facet.range': 'producedBy_resultTimeRange',\n",
    "    'facet.range.gap': '+1YEARS',\n",
    "    'facet.range.start': '1800-01-01T00:00:00Z',\n",
    "    'facet.range.end': '2023-01-01T00:00:00Z',\n",
    "    'f.registrant.facet.sort': 'count',\n",
    "    'f.source.facet.sort': 'index',\n",
    "    'rows': '20',\n",
    "    'facet.limit': '-1',\n",
    "    'facet.sort': 'index',\n",
    "    'start': '20',\n",
    "    'facet': 'on',\n",
    "    'wt': 'json'\n",
    "}\n",
    "headers = {\n",
    "    'Accept': 'application/json',\n",
    "    'User-Agent': 'raymondyee.net'\n",
    "}\n",
    "\n",
    "# keys in response: 'responseHeader', 'response', 'facet_counts'\n",
    "response = httpx.get(url, params=params, headers=headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get back parameters that went into the query and some basic metadata\n",
    "response.json()['responseHeader']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'numFound', 'start', 'numFoundExact', 'docs'\n",
    "response.json()['response'].keys()\n",
    "\n",
    "(response.json()['response']['numFound'], response.json()['response']['numFoundExact'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()['response']['docs'][0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plotting the collection dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "\n",
    "cli = IsbClient2()\n",
    "fq = cli._fq_from_kwargs(source=('OPENCONTEXT',))\n",
    "params = cli.default_search_params(fq=fq, fl=FL_DEFAULT, rows=100, **FACET_RANGE_FIELDS_DEFAULT)\n",
    "\n",
    "\n",
    "url = 'https://central.isample.xyz/isamples_central/thing/select/info'\n",
    "url = 'https://central.isample.xyz/isamples_central/thing/select?q=*:*&facet=true&facet.range=producedBy_resultTimeRange&facet.range.start=NOW/YEAR-200YEARS&facet.range.end=NOW/YEAR&facet.range.gap=YEAR'\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'accept': 'application/json'\n",
    "}\n",
    "\n",
    "response = httpx.get('https://central.isample.xyz/isamples_central/thing/select', headers=headers, params=params)\n",
    "\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()['facet_counts']['facet_ranges']['producedBy_resultTimeRange']['counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k = response.json()['facet_counts']['facet_ranges']['producedBy_resultTimeRange']['counts']\n",
    "dict(zip(k[::2], k[1::2]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming data is your response.json()['facet_counts']['facet_ranges']['producedBy_resultTimeRange']['counts']\n",
    "k = response.json()['facet_counts']['facet_ranges']['producedBy_resultTimeRange']['counts']\n",
    "data = dict(zip(k[::2], k[1::2]))\n",
    "\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(list(data.items()), columns=['Date', 'Count'])\n",
    "\n",
    "# Convert the 'Date' column to datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Extract the year from the date\n",
    "df['Year'] = df['Date'].dt.year\n",
    "\n",
    "# Count the occurrences of each year\n",
    "year_counts = df['Year'].value_counts().sort_index()\n",
    "\n",
    "# Plot the counts vs year\n",
    "year_counts.plot(kind='line')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count vs Year')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = response.json()['facet_counts']['facet_ranges']['producedBy_resultTimeRange']['counts']\n",
    "data = dict(zip(k[::2], k[1::2]))\n",
    "\n",
    "df = pd.DataFrame(list(data.items()), columns=['Date', 'Count'])\n",
    "df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# deal with log scale\n",
    "df = df.loc[df['Count'] != 0]\n",
    "\n",
    "# df['Count'] = df['Count'].replace(0, np.nan)\n",
    "# df['Count'] = df['Count'].fillna(0.1)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(df['Date'], df['Count'], color='green', alpha=0.5, s=10)\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count over Date')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "curl -X 'GET' \\\n",
    "  'https://central.isample.xyz/isamples_central/thing/select?facet=true&facet.mincount=0&facet.field=source' \\\n",
    "  -H 'accept: application/json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(field_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get OpenContext sourced records\n",
    "# fq = cli._fq_from_kwargs(source=('OPENCONTEXT', 'SESAR'), collection_date_end=str(datetime.now().year))\n",
    "fq = cli._fq_from_kwargs(collection_date_end=str(datetime.now().year))\n",
    "\n",
    "params = cli.default_search_params(fq=fq, fl=FL_DEFAULT, rows=100, **FACET_RANGE_FIELDS_DEFAULT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get counts of values grouping by three dimensions: source, hasMaterialCategory, and hasContextCategory\n",
    "dimensions = [\"source\", \"hasMaterialCategory\", \"hasContextCategory\"]\n",
    "xd = cli.pivot(params, dimensions)\n",
    "# print(xd.loc[\"geome\", \"organic material\", \"bacteria\"].sum())\n",
    "# Using the full URIs from the output\n",
    "print(xd.loc[\"geome\", \n",
    "            \"https://w3id.org/isample/vocabulary/material/1.0/organicmaterial\",\n",
    "            \"https://w3id.org/isample/biology/biosampledfeature/1.0/bacteria\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print unique values in each dimension to see what's available\n",
    "print(\"Available sources:\")\n",
    "print(xd.coords['source'].values)\n",
    "print(\"\\nAvailable material categories:\")\n",
    "print(xd.coords['hasMaterialCategory'].values)\n",
    "print(\"\\nAvailable context categories:\")\n",
    "print(xd.coords['hasContextCategory'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum by axis 2 (hasContextCategory) and print\n",
    "df = xd.sum(axis=2).to_pandas()\n",
    "# display transposed\n",
    "display(df.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xd.loc[\"sesar\", \"https://w3id.org/isample/vocabulary/material/1.0/rock\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten_material_category(uri):\n",
    "    \"\"\"Convert material category URI to short name\"\"\"\n",
    "    if not uri or not isinstance(uri, str):\n",
    "        return ''\n",
    "    return uri.split('/')[-1]\n",
    "\n",
    "def shorten_context_category(uri):\n",
    "    \"\"\"Convert context category URI to short name\"\"\"\n",
    "    if not uri or not isinstance(uri, str):\n",
    "        return ''\n",
    "    return uri.split('/')[-1]\n",
    "\n",
    "# Create new coordinates with shortened names\n",
    "xd_short = xd.copy()\n",
    "xd_short.coords['hasMaterialCategory'] = ('hasMaterialCategory', \n",
    "    [shorten_material_category(x) for x in xd.coords['hasMaterialCategory'].values])\n",
    "xd_short.coords['hasContextCategory'] = ('hasContextCategory',\n",
    "    [shorten_context_category(x) for x in xd.coords['hasContextCategory'].values])\n",
    "\n",
    "# Now you can use shorter names\n",
    "# print(xd_short.loc[\"geome\", \"organicmaterial\", \"bacteria\"].sum())\n",
    "print(f\"Number of matches: {int(xd_short.loc['geome', 'organicmaterial', 'bacteria'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Field names in solr\n",
    "for name in cli.field_names():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geoparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import time\n",
    "import ibis\n",
    "\n",
    "\n",
    "\n",
    "def print_timing(start_time, operation):\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Time for {operation}: {elapsed:.2f} seconds\")\n",
    "\n",
    "# Connect to an in-memory DuckDB instance\n",
    "start_time = time.time()\n",
    "con = duckdb.connect()\n",
    "print_timing(start_time, \"database connection\")\n",
    "\n",
    "# Load the GeoParquet file and create view\n",
    "start_time = time.time()\n",
    "geo_parquet_file = '/Users/raymondyee/Data/iSample/2025_02_20_10_30_49/isamples_export_2025_02_20_10_30_49_geo.parquet'\n",
    "con.sql(f\"CREATE VIEW geosamples AS SELECT * FROM read_parquet('{geo_parquet_file}')\")\n",
    "print_timing(start_time, \"creating view\")\n",
    "\n",
    "# Get column names dynamically\n",
    "start_time = time.time()\n",
    "columns = con.sql(\"SELECT column_name FROM information_schema.columns WHERE table_name = 'geosamples'\").fetchall()\n",
    "column_names = [col[0] for col in columns]\n",
    "print_timing(start_time, \"getting column names\")\n",
    "\n",
    "# Print available columns\n",
    "print(\"\\nAvailable columns:\")\n",
    "for col in column_names:\n",
    "    print(f\"- {col}\")\n",
    "\n",
    "# Get total row count\n",
    "start_time = time.time()\n",
    "row_count = con.sql(\"SELECT COUNT(*) FROM geosamples\").fetchone()[0]\n",
    "print(f\"\\nTotal number of rows: {row_count:,}\")\n",
    "print_timing(start_time, \"counting rows\")\n",
    "\n",
    "# Query the first 5 rows with all columns\n",
    "start_time = time.time()\n",
    "result = con.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM geosamples \n",
    "    LIMIT 5\n",
    "\"\"\").df()\n",
    "print_timing(start_time, \"querying sample rows\")\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import time\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Read only a sample of the data (e.g., 1% for better performance)\n",
    "geo_parquet_file = '/Users/raymondyee/Data/iSample/2025_02_20_10_30_49/isamples_export_2025_02_20_10_30_49_geo.parquet'\n",
    "gdf = gpd.read_parquet(geo_parquet_file, columns=['geometry'])  # Only read geometry column\n",
    "sample_size = len(gdf) // 100  # 1% of data\n",
    "gdf_sampled = gdf.sample(n=sample_size, random_state=42)\n",
    "\n",
    "print(f\"Data loading and sampling time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Create figure and axis with larger size and projection\n",
    "fig, ax = plt.subplots(figsize=(15, 10), \n",
    "                       subplot_kw={'projection': ccrs.Robinson()})\n",
    "\n",
    "# Add map features\n",
    "ax.add_feature(cfeature.LAND, facecolor='lightgray')\n",
    "ax.add_feature(cfeature.OCEAN, facecolor='lightblue')\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "ax.gridlines()\n",
    "\n",
    "# Plot with improved styling\n",
    "gdf_sampled.plot(\n",
    "    ax=ax,\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    alpha=0.5,\n",
    "    markersize=1,\n",
    "    color='red',\n",
    "    legend=True\n",
    ")\n",
    "\n",
    "# Add title\n",
    "plt.title(f'Sample of {sample_size:,} points from {len(gdf):,} total records')\n",
    "\n",
    "print(f\"Total execution time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import os\n",
    "\n",
    "# Change working directory to the location of the GeoParquet file\n",
    "os.chdir('/Users/raymondyee/Data/iSample/2025_02_20_10_30_49')\n",
    "\n",
    "# Initialize DuckDB connection\n",
    "conn = duckdb.connect(':memory:')  # or specify a database file\n",
    "\n",
    "# Install and load spatial extension\n",
    "conn.execute(\"INSTALL spatial;\")\n",
    "conn.execute(\"LOAD spatial;\")\n",
    "\n",
    "# Create temp view from parquet file\n",
    "conn.execute(\"\"\"\n",
    "    CREATE TEMP VIEW my_data AS \n",
    "    SELECT * FROM read_parquet('isamples_export_2025_02_20_10_30_49_geo.parquet')\n",
    "\"\"\")\n",
    "\n",
    "# Get count of rows\n",
    "result = conn.execute(\"SELECT COUNT(*) FROM my_data\").fetchall()\n",
    "\n",
    "# Print result\n",
    "print(f\"Total rows: {result[0][0]}\")\n",
    "\n",
    "# Close connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "# Change working directory to the location of the GeoParquet file\n",
    "os.chdir('/Users/raymondyee/Data/iSample/2025_02_20_10_30_49')\n",
    "\n",
    "# Create connection\n",
    "conn = duckdb.connect(':memory:')\n",
    "conn.execute(\"INSTALL spatial; LOAD spatial;\")\n",
    "\n",
    "# DuckDB can query Parquet directly - no need to create views\n",
    "query = conn.table('isamples_export_2025_02_20_10_30_49_geo.parquet')\n",
    "\n",
    "# Queries stay lazy until you need results\n",
    "print(f\"Total rows: {query.count()}\")\n",
    "\n",
    "# You can chain operations naturally\n",
    "filtered = query.filter(\"some_condition\").limit(5)\n",
    "\n",
    "# Only converts to DataFrame when you actually call .df()\n",
    "# filtered_df = filtered.df()  # This would materialize the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import duckdb\n",
    "from lonboard import viz\n",
    "\n",
    "# Change working directory to the location of the GeoParquet file\n",
    "os.chdir('/Users/raymondyee/Data/iSample/2025_02_20_10_30_49')\n",
    "\n",
    "# Initialize DuckDB connection\n",
    "conn = duckdb.connect(':memory:')\n",
    "\n",
    "# Install and load spatial extension\n",
    "conn.execute(\"INSTALL spatial;\")\n",
    "conn.execute(\"LOAD spatial;\")\n",
    "\n",
    "# First, let's check the geometry type and a sample\n",
    "result = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        ST_GeometryType(geometry) as geom_type,\n",
    "        ST_AsText(geometry) as wkt,\n",
    "        sample_location_longitude,\n",
    "        sample_location_latitude\n",
    "    FROM read_parquet('isamples_export_2025_02_20_10_30_49_geo.parquet') \n",
    "    WHERE geometry IS NOT NULL \n",
    "    LIMIT 1\n",
    "\"\"\").fetchall()\n",
    "print(result)\n",
    "\n",
    "# Create temp view using longitude/latitude to create geometry\n",
    "conn.execute(\"\"\"\n",
    "    CREATE TEMP VIEW my_data AS \n",
    "    SELECT \n",
    "        ST_AsWKB(ST_Point(sample_location_longitude, sample_location_latitude)) as geometry,\n",
    "        sample_identifier,\n",
    "        label,\n",
    "        description,\n",
    "        source_collection,\n",
    "        has_sample_object_type,\n",
    "        has_material_category,\n",
    "        has_context_category,\n",
    "        informal_classification,\n",
    "        keywords,\n",
    "        produced_by,\n",
    "        curation,\n",
    "        registrant,\n",
    "        related_resource,\n",
    "        sampling_purpose,\n",
    "        sample_location_longitude,\n",
    "        sample_location_latitude\n",
    "    FROM read_parquet('isamples_export_2025_02_20_10_30_49_geo.parquet')\n",
    "    WHERE sample_location_longitude IS NOT NULL \n",
    "    AND sample_location_latitude IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "# Check coordinate bounds\n",
    "bounds = conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        MIN(sample_location_longitude) as min_lon,\n",
    "        MAX(sample_location_longitude) as max_lon,\n",
    "        MIN(sample_location_latitude) as min_lat,\n",
    "        MAX(sample_location_latitude) as max_lat,\n",
    "        COUNT(*) as point_count\n",
    "    FROM my_data\n",
    "\"\"\").fetchall()\n",
    "print(\"\\nCoordinate bounds and point count:\")\n",
    "print(bounds)\n",
    "\n",
    "# Query and visualize with map configuration\n",
    "result = conn.sql(\"SELECT * FROM my_data\")\n",
    "viz(\n",
    "    result,\n",
    "    map_kwargs={\n",
    "        \"zoom\": 1,  # Global view\n",
    "        \"center\": {\"lat\": 0, \"lon\": 0}  # Center at equator\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isamples-python-3.12.9",
   "language": "python",
   "name": "isamples-python-3.12.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

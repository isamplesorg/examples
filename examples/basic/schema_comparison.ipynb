{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iSamples Parquet Schema Comparison\n",
    "\n",
    "**Goal**: Understand the tradeoffs among five parquet formats for iSamples data.\n",
    "\n",
    "| Format | Philosophy | Sources | Relationships |\n",
    "|--------|-----------|---------|---------------|\n",
    "| **Export** | Sample-centric (flat) | All 4 sources | Nested STRUCTs |\n",
    "| **Zenodo Narrow** | Graph (nodes + edges) | All 4 sources | Separate `_edge_` rows |\n",
    "| **Zenodo Wide** | Entity-centric | All 4 sources | `p__*` arrays â†’ row_ids |\n",
    "| **Eric's Narrow** | Graph (nodes + edges) | OpenContext only | Separate `_edge_` rows |\n",
    "| **Eric's Wide** | Entity-centric | OpenContext only | `p__*` arrays â†’ row_ids |\n",
    "\n",
    "**Key insight**: There is no universal best format. Each optimizes for different query patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## Portability\n",
    "\n",
    "This notebook works in multiple environments:\n",
    "\n",
    "| Environment | Behavior |\n",
    "|-------------|----------|\n",
    "| **Raymond's laptop** | Uses local files in `~/Data/iSample/` |\n",
    "| **mybinder.org** | Downloads to `/tmp/pqgfiles/` cache |\n",
    "| **Other users** | Downloads to `~/Data/iSample/pqg_cache/` |\n",
    "\n",
    "**Configuration options** (in cell 2):\n",
    "- `CACHE_DIR`: Override with `ISAMPLES_CACHE_DIR` env var\n",
    "- `USE_REMOTE=True`: Skip downloads, query remote parquet via HTTP (slower but no disk)\n",
    "- `DOWNLOAD_MISSING=False`: Error instead of downloading missing files\n",
    "\n",
    "---\n",
    "\n",
    "## Data Source Coverage\n",
    "\n",
    "| Format | Sources | Description |\n",
    "|--------|---------|-------------|\n",
    "| **Export, Zenodo Narrow, Zenodo Wide** | SESAR, OpenContext, GEOME, Smithsonian | Full iSamples (~6.7M samples) |\n",
    "| **Eric's Narrow, Eric's Wide** | OpenContext only | Subset (~1.1M samples) |\n",
    "\n",
    "This allows fair comparisons:\n",
    "- **Apples-to-apples**: Export vs Zenodo Narrow vs Zenodo Wide (same data)\n",
    "- **Structure comparison**: Eric's Narrow vs Eric's Wide (same data, different structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION - Edit these paths for your environment\n",
    "# =============================================================================\n",
    "\n",
    "# Cache directory for downloaded files (used when local paths don't exist)\n",
    "# - On mybinder.org: uses /tmp/pqgfiles\n",
    "# - Locally: uses ~/Data/iSample/pqg_cache (or override with ISAMPLES_CACHE_DIR env var)\n",
    "CACHE_DIR = Path(os.environ.get('ISAMPLES_CACHE_DIR', \n",
    "                                '/tmp/pqgfiles' if Path('/tmp').exists() and not Path.home().joinpath('Data/iSample').exists()\n",
    "                                else Path.home() / 'Data/iSample/pqg_cache'))\n",
    "\n",
    "# Local paths (Raymond's setup) - these are checked first\n",
    "# Updated 2026-01-09: zenodo_wide now points to January 9 conversion\n",
    "# which fixes issue #8 ([null] array bug in p__* columns)\n",
    "LOCAL_PATHS = {\n",
    "    'export': Path.home() / 'Data/iSample/2025_04_21_16_23_46/isamples_export_2025_04_21_16_23_46_geo.parquet',\n",
    "    'zenodo_narrow': Path.home() / 'Data/iSample/pqg_refining/zenodo_narrow_2025-12-12.parquet',\n",
    "    'zenodo_wide': Path.home() / 'Data/iSample/pqg_refining/zenodo_wide_2026-01-09.parquet',\n",
    "    'eric_narrow': Path.home() / 'Data/iSample/pqg_refining/oc_isamples_pqg.parquet',\n",
    "    'eric_wide': Path.home() / 'Data/iSample/pqg_refining/oc_isamples_pqg_wide.parquet',\n",
    "}\n",
    "\n",
    "# Remote URLs - fallback when local files don't exist\n",
    "# Updated 2026-01-09: R2 bucket contains January 9 wide conversion (fixes issue #8)\n",
    "URLS = {\n",
    "    'export': 'https://zenodo.org/records/15278211/files/isamples_export_2025_04_21_16_23_46_geo.parquet',\n",
    "    'zenodo_narrow': 'https://pub-a18234d962364c22a50c787b7ca09fa5.r2.dev/isamples_202512_narrow.parquet',\n",
    "    'zenodo_wide': 'https://pub-a18234d962364c22a50c787b7ca09fa5.r2.dev/isamples_202601_wide.parquet',\n",
    "    'eric_narrow': 'https://storage.googleapis.com/opencontext-parquet/oc_isamples_pqg.parquet',\n",
    "    'eric_wide': 'https://storage.googleapis.com/opencontext-parquet/oc_isamples_pqg_wide.parquet',\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# PATH RESOLUTION - Automatically finds or downloads files\n",
    "# =============================================================================\n",
    "\n",
    "def resolve_path(name: str, local_paths: dict, urls: dict, cache_dir: Path, \n",
    "                 download: bool = True, use_remote: bool = False) -> Path:\n",
    "    \"\"\"\n",
    "    Resolve file path: check local first, then cache, optionally download.\n",
    "    \n",
    "    Args:\n",
    "        name: File identifier (e.g., 'export', 'zenodo_wide')\n",
    "        local_paths: Dict of local file paths to check first\n",
    "        urls: Dict of remote URLs for downloading\n",
    "        cache_dir: Directory for cached downloads\n",
    "        download: If True, download missing files to cache\n",
    "        use_remote: If True, return URL for DuckDB remote access (no download)\n",
    "    \n",
    "    Returns:\n",
    "        Path to local file, or URL string if use_remote=True\n",
    "    \"\"\"\n",
    "    # Option 1: Local file exists\n",
    "    if name in local_paths and local_paths[name].exists():\n",
    "        return local_paths[name]\n",
    "    \n",
    "    # Option 2: Return URL for remote access (DuckDB can read directly)\n",
    "    if use_remote and name in urls:\n",
    "        return urls[name]\n",
    "    \n",
    "    # Option 3: Check cache\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    cached_file = cache_dir / f\"{name}.parquet\"\n",
    "    \n",
    "    if cached_file.exists():\n",
    "        return cached_file\n",
    "    \n",
    "    # Option 4: Download to cache\n",
    "    if download and name in urls:\n",
    "        url = urls[name]\n",
    "        print(f\"Downloading {name} from {url}...\")\n",
    "        print(f\"  -> {cached_file}\")\n",
    "        \n",
    "        # Download with progress\n",
    "        def progress_hook(block_num, block_size, total_size):\n",
    "            downloaded = block_num * block_size\n",
    "            if total_size > 0:\n",
    "                pct = min(100, downloaded * 100 // total_size)\n",
    "                mb = downloaded / 1e6\n",
    "                total_mb = total_size / 1e6\n",
    "                print(f\"\\r  Progress: {pct}% ({mb:.1f}/{total_mb:.1f} MB)\", end='', flush=True)\n",
    "        \n",
    "        urllib.request.urlretrieve(url, cached_file, reporthook=progress_hook)\n",
    "        print()  # newline after progress\n",
    "        return cached_file\n",
    "    \n",
    "    # No file available\n",
    "    raise FileNotFoundError(f\"File '{name}' not found locally and download=False\")\n",
    "\n",
    "# =============================================================================\n",
    "# RESOLVE ALL PATHS\n",
    "# =============================================================================\n",
    "\n",
    "# Set to True to skip downloads and use DuckDB's remote parquet reading\n",
    "# (Slower queries but no disk usage - good for quick exploration)\n",
    "USE_REMOTE = False\n",
    "\n",
    "# Set to False to skip downloading missing files (will error if not found)\n",
    "DOWNLOAD_MISSING = True\n",
    "\n",
    "print(f\"Cache directory: {CACHE_DIR}\")\n",
    "print(f\"Use remote: {USE_REMOTE}, Download missing: {DOWNLOAD_MISSING}\\n\")\n",
    "\n",
    "PATHS = {}\n",
    "for name in ['export', 'zenodo_narrow', 'zenodo_wide', 'eric_narrow', 'eric_wide']:\n",
    "    try:\n",
    "        path = resolve_path(name, LOCAL_PATHS, URLS, CACHE_DIR, \n",
    "                           download=DOWNLOAD_MISSING, use_remote=USE_REMOTE)\n",
    "        PATHS[name] = path\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"âš ï¸ {name}: {e}\")\n",
    "        PATHS[name] = None\n",
    "\n",
    "# =============================================================================\n",
    "# VERIFY FILES\n",
    "# =============================================================================\n",
    "\n",
    "def get_file_info(path):\n",
    "    \"\"\"Get file info - works for both local paths and URLs.\"\"\"\n",
    "    if path is None:\n",
    "        return 'âŒ', 'Not available'\n",
    "    if isinstance(path, str) and path.startswith('http'):\n",
    "        return 'ğŸŒ', 'Remote URL'\n",
    "    if Path(path).exists():\n",
    "        size_mb = Path(path).stat().st_size / 1e6\n",
    "        return 'âœ…', f'{size_mb:.1f} MB'\n",
    "    return 'âŒ', 'Not found'\n",
    "\n",
    "print(\"=== Full iSamples (all sources) ===\")\n",
    "for name in ['export', 'zenodo_narrow', 'zenodo_wide']:\n",
    "    status, info = get_file_info(PATHS.get(name))\n",
    "    source = \"local\" if PATHS.get(name) and Path(PATHS[name]).exists() and PATHS[name] in LOCAL_PATHS.values() else \"cache/remote\"\n",
    "    print(f'{status} {name}: {info} ({source})')\n",
    "\n",
    "print(\"\\n=== OpenContext only (Eric's) ===\")\n",
    "for name in ['eric_narrow', 'eric_wide']:\n",
    "    status, info = get_file_info(PATHS.get(name))\n",
    "    source = \"local\" if PATHS.get(name) and Path(PATHS[name]).exists() and PATHS[name] in LOCAL_PATHS.values() else \"cache/remote\"\n",
    "    print(f'{status} {name}: {info} ({source})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for timing queries\n",
    "import statistics\n",
    "\n",
    "def timed_query(con, sql, name=\"Query\"):\n",
    "    \"\"\"Execute query and return (result_df, elapsed_ms)\"\"\"\n",
    "    start = time.time()\n",
    "    result = con.sql(sql).fetchdf()\n",
    "    elapsed = (time.time() - start) * 1000\n",
    "    print(f\"{name}: {elapsed:.1f}ms, {len(result):,} rows\")\n",
    "    return result, elapsed\n",
    "\n",
    "def timed_query_multirun(con, sql, name=\"Query\", runs=3):\n",
    "    \"\"\"Execute query multiple times and return (result_df, mean_ms, stddev_ms)\"\"\"\n",
    "    times = []\n",
    "    result = None\n",
    "    for i in range(runs):\n",
    "        start = time.time()\n",
    "        result = con.sql(sql).fetchdf()\n",
    "        elapsed = (time.time() - start) * 1000\n",
    "        times.append(elapsed)\n",
    "    \n",
    "    mean_ms = statistics.mean(times)\n",
    "    stddev_ms = statistics.stdev(times) if len(times) > 1 else 0\n",
    "    print(f\"{name}: {mean_ms:.1f}ms Â± {stddev_ms:.1f}ms (n={runs}), {len(result):,} rows\")\n",
    "    return result, mean_ms, stddev_ms\n",
    "\n",
    "# Create connection\n",
    "con = duckdb.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Schema Inspection\n",
    "\n",
    "Understanding what columns exist and their types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to check if path is available (works for Path objects and URL strings)\n",
    "def path_available(path):\n",
    "    \"\"\"Check if a path is available (local file exists or is a URL).\"\"\"\n",
    "    if path is None:\n",
    "        return False\n",
    "    if isinstance(path, str) and path.startswith('http'):\n",
    "        return True  # URLs are assumed available\n",
    "    return Path(path).exists()\n",
    "\n",
    "# Get schema for each format\n",
    "schemas = {}\n",
    "for name, path in PATHS.items():\n",
    "    if path_available(path):\n",
    "        result = con.sql(f\"DESCRIBE SELECT * FROM read_parquet('{path}')\").fetchdf()\n",
    "        schemas[name] = result\n",
    "        print(f\"\\n=== {name.upper()} ({len(result)} columns) ===\")\n",
    "        # Show just first 15 columns to keep output manageable\n",
    "        print(result[['column_name', 'column_type']].head(15).to_string())\n",
    "        if len(result) > 15:\n",
    "            print(f\"  ... and {len(result) - 15} more columns\")\n",
    "    else:\n",
    "        print(f\"\\n=== {name.upper()} ===\")\n",
    "        print(f\"  âš ï¸ Not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare column counts and key structural differences (computed from schemas)\n",
    "def check_schema_features(schema_df):\n",
    "    \"\"\"Analyze schema DataFrame for structural features.\"\"\"\n",
    "    if schema_df is None or len(schema_df) == 0:\n",
    "        return {'columns': 0, 'has_edge_cols': False, 'has_p__cols': False, \n",
    "                'has_nested_structs': False, 'has_otype': False}\n",
    "    \n",
    "    cols = set(schema_df['column_name'].tolist())\n",
    "    types = dict(zip(schema_df['column_name'], schema_df['column_type']))\n",
    "    \n",
    "    return {\n",
    "        'columns': len(schema_df),\n",
    "        'has_edge_cols': all(c in cols for c in ['s', 'p', 'o']),\n",
    "        'has_p__cols': any(c.startswith('p__') for c in cols),\n",
    "        'has_nested_structs': any('STRUCT' in str(t) for t in types.values()),\n",
    "        'has_otype': 'otype' in cols,\n",
    "    }\n",
    "\n",
    "# Compute features for each format\n",
    "format_order = ['export', 'zenodo_narrow', 'zenodo_wide', 'eric_narrow', 'eric_wide']\n",
    "features = {name: check_schema_features(schemas.get(name)) for name in format_order}\n",
    "\n",
    "# Build comparison table\n",
    "comparison = pd.DataFrame([\n",
    "    {\n",
    "        'Format': name.replace('_', ' ').title(),\n",
    "        'Data': 'Full' if name in ['export', 'zenodo_narrow', 'zenodo_wide'] else 'OC only',\n",
    "        'Columns': features[name]['columns'],\n",
    "        'Edge cols (s,p,o)': 'âœ“' if features[name]['has_edge_cols'] else '',\n",
    "        'p__* cols': 'âœ“' if features[name]['has_p__cols'] else '',\n",
    "        'Nested STRUCTs': 'âœ“' if features[name]['has_nested_structs'] else '',\n",
    "        'otype col': 'âœ“' if features[name]['has_otype'] else '',\n",
    "    }\n",
    "    for name in format_order\n",
    "])\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Row Count Analysis\n",
    "\n",
    "Understanding what's IN each format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total row counts\n",
    "row_counts = {}\n",
    "print(\"=== Full iSamples ===\")\n",
    "for name in ['export', 'zenodo_narrow', 'zenodo_wide']:\n",
    "    path = PATHS.get(name)\n",
    "    if path_available(path):\n",
    "        count = con.sql(f\"SELECT COUNT(*) FROM read_parquet('{path}')\").fetchone()[0]\n",
    "        row_counts[name] = count\n",
    "        print(f\"{name}: {count:,} rows\")\n",
    "    else:\n",
    "        print(f\"{name}: âš ï¸ Not available\")\n",
    "\n",
    "print(\"\\n=== OpenContext only ===\")\n",
    "for name in ['eric_narrow', 'eric_wide']:\n",
    "    path = PATHS.get(name)\n",
    "    if path_available(path):\n",
    "        count = con.sql(f\"SELECT COUNT(*) FROM read_parquet('{path}')\").fetchone()[0]\n",
    "        row_counts[name] = count\n",
    "        print(f\"{name}: {count:,} rows\")\n",
    "    else:\n",
    "        print(f\"{name}: âš ï¸ Not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For PQG formats: breakdown by otype\n",
    "for name in ['zenodo_narrow', 'zenodo_wide', 'eric_narrow', 'eric_wide']:\n",
    "    path = PATHS.get(name)\n",
    "    if path_available(path):\n",
    "        print(f\"=== {name.upper()}: Rows by otype ===\")\n",
    "        result = con.sql(f\"\"\"\n",
    "            SELECT otype, COUNT(*) as cnt \n",
    "            FROM read_parquet('{path}')\n",
    "            GROUP BY otype ORDER BY cnt DESC\n",
    "        \"\"\").fetchdf()\n",
    "        print(result.to_string())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Export: breakdown by source_collection\n",
    "print(\"=== EXPORT: Rows by source_collection ===\")\n",
    "if path_available(PATHS.get('export')):\n",
    "    result = con.sql(f\"\"\"\n",
    "        SELECT source_collection, COUNT(*) as cnt \n",
    "        FROM read_parquet('{PATHS['export']}')\n",
    "        GROUP BY source_collection ORDER BY cnt DESC\n",
    "    \"\"\").fetchdf()\n",
    "    print(result.to_string())\n",
    "else:\n",
    "    print(\"âš ï¸ Export file not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Query Benchmark Suite\n",
    "\n",
    "Testing common query patterns across all three formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Map Visualization: Get All Coordinates\n",
    "\n",
    "**Use case**: Render points on a Cesium/Leaflet map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT: Direct column access\n",
    "print(\"=== EXPORT (full iSamples) ===\")\n",
    "export_coords, export_coords_time = timed_query(con, f\"\"\"\n",
    "    SELECT sample_location_latitude as lat, sample_location_longitude as lon\n",
    "    FROM read_parquet('{PATHS['export']}')\n",
    "    WHERE sample_location_latitude IS NOT NULL\n",
    "\"\"\", \"All coordinates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIDE formats: Filter by otype\n",
    "print(\"=== ZENODO WIDE (full iSamples) ===\")\n",
    "zenodo_wide_coords, zenodo_wide_coords_time = timed_query(con, f\"\"\"\n",
    "    SELECT latitude as lat, longitude as lon\n",
    "    FROM read_parquet('{PATHS['zenodo_wide']}')\n",
    "    WHERE otype = 'GeospatialCoordLocation' AND latitude IS NOT NULL\n",
    "\"\"\", \"All coordinates\")\n",
    "\n",
    "print(\"\\n=== ERIC WIDE (OpenContext only) ===\")\n",
    "eric_wide_coords, eric_wide_coords_time = timed_query(con, f\"\"\"\n",
    "    SELECT latitude as lat, longitude as lon\n",
    "    FROM read_parquet('{PATHS['eric_wide']}')\n",
    "    WHERE otype = 'GeospatialCoordLocation' AND latitude IS NOT NULL\n",
    "\"\"\", \"All coordinates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NARROW formats: Filter by otype  \n",
    "print(\"=== ZENODO NARROW (full iSamples) ===\")\n",
    "zenodo_narrow_coords, zenodo_narrow_coords_time = timed_query(con, f\"\"\"\n",
    "    SELECT latitude as lat, longitude as lon\n",
    "    FROM read_parquet('{PATHS['zenodo_narrow']}')\n",
    "    WHERE otype = 'GeospatialCoordLocation' AND latitude IS NOT NULL\n",
    "\"\"\", \"All coordinates\")\n",
    "\n",
    "print(\"\\n=== ERIC NARROW (OpenContext only) ===\")\n",
    "eric_narrow_coords, eric_narrow_coords_time = timed_query(con, f\"\"\"\n",
    "    SELECT latitude as lat, longitude as lon\n",
    "    FROM read_parquet('{PATHS['eric_narrow']}')\n",
    "    WHERE otype = 'GeospatialCoordLocation' AND latitude IS NOT NULL\n",
    "\"\"\", \"All coordinates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary - Map query comparison\n",
    "print(\"=== MAP QUERY SUMMARY ===\")\n",
    "print(\"\\nFull iSamples (apples-to-apples comparison):\")\n",
    "print(f\"  Export:        {export_coords_time:6.1f}ms ({len(export_coords):,} points)\")\n",
    "print(f\"  Zenodo Wide:   {zenodo_wide_coords_time:6.1f}ms ({len(zenodo_wide_coords):,} points)\")\n",
    "print(f\"  Zenodo Narrow: {zenodo_narrow_coords_time:6.1f}ms ({len(zenodo_narrow_coords):,} points)\")\n",
    "\n",
    "print(\"\\nOpenContext only (Eric's files):\")\n",
    "print(f\"  Eric Wide:     {eric_wide_coords_time:6.1f}ms ({len(eric_wide_coords):,} points)\")\n",
    "print(f\"  Eric Narrow:   {eric_narrow_coords_time:6.1f}ms ({len(eric_narrow_coords):,} points)\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Key insight: Export returns coords directly; PQG formats need otype filter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Faceted Search: Count by Material Category\n",
    "\n",
    "**Use case**: Show facet counts in a search UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT: Unnest nested struct array\n",
    "# SQL Complexity: 1 subquery, 0 JOINs - simple unnest\n",
    "print(\"=== EXPORT (full iSamples) ===\")\n",
    "export_facets, export_facets_time = timed_query(con, f\"\"\"\n",
    "    SELECT \n",
    "        mat.identifier as material,\n",
    "        COUNT(*) as cnt\n",
    "    FROM (\n",
    "        SELECT unnest(has_material_category) as mat\n",
    "        FROM read_parquet('{PATHS['export']}')\n",
    "        WHERE has_material_category IS NOT NULL AND len(has_material_category) > 0\n",
    "    )\n",
    "    GROUP BY mat.identifier\n",
    "    ORDER BY cnt DESC\n",
    "    LIMIT 10\n",
    "\"\"\", \"Material facets\")\n",
    "print(export_facets.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIDE formats: JOIN via p__has_material_category\n",
    "# SQL Complexity: 2 CTEs, 1 JOIN - requires row_id lookup\n",
    "print(\"=== ZENODO WIDE (full iSamples) ===\")\n",
    "zenodo_wide_facets, zenodo_wide_facets_time = timed_query(con, f\"\"\"\n",
    "    WITH samples AS (\n",
    "        SELECT unnest(p__has_material_category) as concept_rowid\n",
    "        FROM read_parquet('{PATHS['zenodo_wide']}')\n",
    "        WHERE otype = 'MaterialSampleRecord' \n",
    "          AND p__has_material_category IS NOT NULL\n",
    "    ),\n",
    "    concepts AS (\n",
    "        SELECT row_id, label\n",
    "        FROM read_parquet('{PATHS['zenodo_wide']}')\n",
    "        WHERE otype = 'IdentifiedConcept'\n",
    "    )\n",
    "    SELECT c.label as material, COUNT(*) as cnt\n",
    "    FROM samples s\n",
    "    JOIN concepts c ON s.concept_rowid = c.row_id\n",
    "    GROUP BY c.label\n",
    "    ORDER BY cnt DESC\n",
    "    LIMIT 10\n",
    "\"\"\", \"Material facets\")\n",
    "print(zenodo_wide_facets.to_string())\n",
    "\n",
    "print(\"\\n=== ERIC WIDE (OpenContext only) ===\")\n",
    "eric_wide_facets, eric_wide_facets_time = timed_query(con, f\"\"\"\n",
    "    WITH samples AS (\n",
    "        SELECT unnest(p__has_material_category) as concept_rowid\n",
    "        FROM read_parquet('{PATHS['eric_wide']}')\n",
    "        WHERE otype = 'MaterialSampleRecord' \n",
    "          AND p__has_material_category IS NOT NULL\n",
    "    ),\n",
    "    concepts AS (\n",
    "        SELECT row_id, label\n",
    "        FROM read_parquet('{PATHS['eric_wide']}')\n",
    "        WHERE otype = 'IdentifiedConcept'\n",
    "    )\n",
    "    SELECT c.label as material, COUNT(*) as cnt\n",
    "    FROM samples s\n",
    "    JOIN concepts c ON s.concept_rowid = c.row_id\n",
    "    GROUP BY c.label\n",
    "    ORDER BY cnt DESC\n",
    "    LIMIT 10\n",
    "\"\"\", \"Material facets\")\n",
    "print(eric_wide_facets.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NARROW formats: Follow edges with predicate='has_material_category'\n",
    "# SQL Complexity: 2 CTEs, 1 JOIN - requires edge traversal\n",
    "print(\"=== ZENODO NARROW (full iSamples) ===\")\n",
    "zenodo_narrow_facets, zenodo_narrow_facets_time = timed_query(con, f\"\"\"\n",
    "    WITH edges AS (\n",
    "        SELECT s as sample_rowid, unnest(o) as concept_rowid\n",
    "        FROM read_parquet('{PATHS['zenodo_narrow']}')\n",
    "        WHERE otype = '_edge_' AND p = 'has_material_category'\n",
    "    ),\n",
    "    concepts AS (\n",
    "        SELECT row_id, label\n",
    "        FROM read_parquet('{PATHS['zenodo_narrow']}')\n",
    "        WHERE otype = 'IdentifiedConcept'\n",
    "    )\n",
    "    SELECT c.label as material, COUNT(*) as cnt\n",
    "    FROM edges e\n",
    "    JOIN concepts c ON e.concept_rowid = c.row_id\n",
    "    GROUP BY c.label\n",
    "    ORDER BY cnt DESC\n",
    "    LIMIT 10\n",
    "\"\"\", \"Material facets\")\n",
    "print(zenodo_narrow_facets.to_string())\n",
    "\n",
    "print(\"\\n=== ERIC NARROW (OpenContext only) ===\")\n",
    "eric_narrow_facets, eric_narrow_facets_time = timed_query(con, f\"\"\"\n",
    "    WITH edges AS (\n",
    "        SELECT s as sample_rowid, unnest(o) as concept_rowid\n",
    "        FROM read_parquet('{PATHS['eric_narrow']}')\n",
    "        WHERE otype = '_edge_' AND p = 'has_material_category'\n",
    "    ),\n",
    "    concepts AS (\n",
    "        SELECT row_id, label\n",
    "        FROM read_parquet('{PATHS['eric_narrow']}')\n",
    "        WHERE otype = 'IdentifiedConcept'\n",
    "    )\n",
    "    SELECT c.label as material, COUNT(*) as cnt\n",
    "    FROM edges e\n",
    "    JOIN concepts c ON e.concept_rowid = c.row_id\n",
    "    GROUP BY c.label\n",
    "    ORDER BY cnt DESC\n",
    "    LIMIT 10\n",
    "\"\"\", \"Material facets\")\n",
    "print(eric_narrow_facets.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facet query summary\n",
    "print(\"=== FACET QUERY SUMMARY ===\")\n",
    "print(\"\\nFull iSamples (apples-to-apples):\")\n",
    "print(f\"  Export:        {export_facets_time:6.1f}ms (SQL: 1 subquery, 0 JOINs)\")\n",
    "print(f\"  Zenodo Wide:   {zenodo_wide_facets_time:6.1f}ms (SQL: 2 CTEs, 1 JOIN)\")\n",
    "print(f\"  Zenodo Narrow: {zenodo_narrow_facets_time:6.1f}ms (SQL: 2 CTEs, 1 JOIN)\")\n",
    "\n",
    "print(\"\\nOpenContext only (Eric's files):\")\n",
    "print(f\"  Eric Wide:     {eric_wide_facets_time:6.1f}ms\")\n",
    "print(f\"  Eric Narrow:   {eric_narrow_facets_time:6.1f}ms\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Key insight: Export is simplest (no JOINs), but PQG returns human-readable labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Entity Listing: Get All Unique Agents\n",
    "\n",
    "**Use case**: Populate a dropdown, show \"who collected samples\"\n",
    "\n",
    "**Key tradeoff**: Export cannot do this efficiently!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIDE formats: Direct query on Agent rows\n",
    "# SQL Complexity: 0 CTEs, 0 JOINs - simple otype filter\n",
    "print(\"=== ZENODO WIDE (full iSamples) ===\")\n",
    "zenodo_wide_agents, zenodo_wide_agents_time = timed_query(con, f\"\"\"\n",
    "    SELECT name, role, COUNT(*) as cnt\n",
    "    FROM read_parquet('{PATHS['zenodo_wide']}')\n",
    "    WHERE otype = 'Agent'\n",
    "    GROUP BY name, role\n",
    "    ORDER BY cnt DESC\n",
    "    LIMIT 10\n",
    "\"\"\", \"All agents\")\n",
    "print(zenodo_wide_agents.to_string())\n",
    "\n",
    "print(\"\\n=== ERIC WIDE (OpenContext only) ===\")\n",
    "eric_wide_agents, eric_wide_agents_time = timed_query(con, f\"\"\"\n",
    "    SELECT name, role, COUNT(*) as cnt\n",
    "    FROM read_parquet('{PATHS['eric_wide']}')\n",
    "    WHERE otype = 'Agent'\n",
    "    GROUP BY name, role\n",
    "    ORDER BY cnt DESC\n",
    "    LIMIT 10\n",
    "\"\"\", \"All agents\")\n",
    "print(eric_wide_agents.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NARROW formats: Same approach - otype filter\n",
    "# SQL Complexity: 0 CTEs, 0 JOINs - simple otype filter\n",
    "print(\"=== ZENODO NARROW (full iSamples) ===\")\n",
    "zenodo_narrow_agents, zenodo_narrow_agents_time = timed_query(con, f\"\"\"\n",
    "    SELECT name, role, COUNT(*) as cnt\n",
    "    FROM read_parquet('{PATHS['zenodo_narrow']}')\n",
    "    WHERE otype = 'Agent'\n",
    "    GROUP BY name, role\n",
    "    ORDER BY cnt DESC\n",
    "    LIMIT 10\n",
    "\"\"\", \"All agents\")\n",
    "print(zenodo_narrow_agents.to_string())\n",
    "\n",
    "print(\"\\n=== ERIC NARROW (OpenContext only) ===\")\n",
    "eric_narrow_agents, eric_narrow_agents_time = timed_query(con, f\"\"\"\n",
    "    SELECT name, role, COUNT(*) as cnt\n",
    "    FROM read_parquet('{PATHS['eric_narrow']}')\n",
    "    WHERE otype = 'Agent'\n",
    "    GROUP BY name, role\n",
    "    ORDER BY cnt DESC\n",
    "    LIMIT 10\n",
    "\"\"\", \"All agents\")\n",
    "print(eric_narrow_agents.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT: Must scan all samples and extract from nested structs\n",
    "# SQL Complexity: 1 subquery, 0 JOINs - but FULL TABLE SCAN required\n",
    "# This is MUCH slower because agents are embedded in every sample row\n",
    "print(\"=== EXPORT (full iSamples) ===\")\n",
    "export_agents, export_agents_time = timed_query(con, f\"\"\"\n",
    "    SELECT \n",
    "        resp.name as name,\n",
    "        resp.role as role,\n",
    "        COUNT(*) as cnt\n",
    "    FROM (\n",
    "        SELECT unnest(produced_by.responsibility) as resp\n",
    "        FROM read_parquet('{PATHS['export']}')\n",
    "        WHERE produced_by IS NOT NULL \n",
    "          AND produced_by.responsibility IS NOT NULL\n",
    "    )\n",
    "    GROUP BY resp.name, resp.role\n",
    "    ORDER BY cnt DESC\n",
    "    LIMIT 10\n",
    "\"\"\", \"All agents (from nested)\")\n",
    "print(export_agents.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent listing summary\n",
    "print(\"=== ENTITY LISTING SUMMARY ===\")\n",
    "print(\"\\nFull iSamples (apples-to-apples):\")\n",
    "print(f\"  Zenodo Wide:   {zenodo_wide_agents_time:6.1f}ms (SQL: 0 JOINs, otype filter)\")\n",
    "print(f\"  Zenodo Narrow: {zenodo_narrow_agents_time:6.1f}ms (SQL: 0 JOINs, otype filter)\")\n",
    "print(f\"  Export:        {export_agents_time:6.1f}ms (SQL: 0 JOINs, FULL SCAN)\")\n",
    "\n",
    "print(\"\\nOpenContext only (Eric's files):\")\n",
    "print(f\"  Eric Wide:     {eric_wide_agents_time:6.1f}ms\")\n",
    "print(f\"  Eric Narrow:   {eric_narrow_agents_time:6.1f}ms\")\n",
    "\n",
    "print(\"\\nâš ï¸ Export is 10-100x SLOWER for entity listing!\")\n",
    "print(\"   Reason: Agents are embedded in every sample row, requiring full scan\")\n",
    "print(\"   PQG: Agents are separate rows, filtered by otype = 'Agent'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Reverse Lookup: Samples by Agent\n",
    "\n",
    "**Use case**: \"Show me all samples collected by Agent X\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, pick an agent name that exists in all formats\n",
    "# Using a common agent from the data\n",
    "AGENT_NAME = 'Vance Vredenburg'  # Adjust based on your data\n",
    "\n",
    "print(f\"Looking for samples by: {AGENT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT: Filter on nested struct\n",
    "print(\"=== EXPORT ===\")\n",
    "export_by_agent, export_time = timed_query(con, f\"\"\"\n",
    "    SELECT sample_identifier, label\n",
    "    FROM read_parquet('{PATHS['export']}')\n",
    "    WHERE list_contains(\n",
    "        [r.name FOR r IN produced_by.responsibility],\n",
    "        '{AGENT_NAME}'\n",
    "    )\n",
    "    LIMIT 10\n",
    "\"\"\", f\"Samples by {AGENT_NAME}\")\n",
    "print(export_by_agent.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIDE: Find agent row_id, then find samples with that row_id in p__responsibility\n",
    "# Note: Agent may not exist in Eric's OC-only data, so use Zenodo Wide for full coverage\n",
    "print(\"=== ZENODO WIDE (full iSamples) ===\")\n",
    "zenodo_wide_by_agent, zenodo_wide_by_agent_time = timed_query(con, f\"\"\"\n",
    "    WITH agent AS (\n",
    "        SELECT row_id \n",
    "        FROM read_parquet('{PATHS['zenodo_wide']}')\n",
    "        WHERE otype = 'Agent' AND name = '{AGENT_NAME}'\n",
    "        LIMIT 1\n",
    "    ),\n",
    "    events AS (\n",
    "        SELECT w.row_id as event_id\n",
    "        FROM read_parquet('{PATHS['zenodo_wide']}') w, agent\n",
    "        WHERE w.otype = 'SamplingEvent' \n",
    "          AND list_contains(w.p__responsibility, agent.row_id)\n",
    "    )\n",
    "    SELECT s.sample_identifier, s.label\n",
    "    FROM read_parquet('{PATHS['zenodo_wide']}') s, events\n",
    "    WHERE s.otype = 'MaterialSampleRecord'\n",
    "      AND list_contains(s.p__produced_by, events.event_id)\n",
    "    LIMIT 10\n",
    "\"\"\", f\"Samples by {AGENT_NAME}\")\n",
    "print(zenodo_wide_by_agent.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n=== REVERSE LOOKUP SUMMARY ===\")\n",
    "print(f\"Export:      {export_time:.1f}ms ({len(export_by_agent)} rows)\")\n",
    "print(f\"Zenodo Wide: {zenodo_wide_by_agent_time:.1f}ms ({len(zenodo_wide_by_agent)} rows)\")\n",
    "print(\"\\nNote: Export's nested list_contains is efficient for this pattern\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Sample Detail: Get Full Info for One Sample\n",
    "\n",
    "**Use case**: User clicks on a sample, show all details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a sample identifier\n",
    "SAMPLE_ID = con.sql(f\"\"\"\n",
    "    SELECT sample_identifier FROM read_parquet('{PATHS['export']}')\n",
    "    WHERE sample_identifier IS NOT NULL LIMIT 1\n",
    "\"\"\").fetchone()[0]\n",
    "print(f\"Sample: {SAMPLE_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT: Everything on one row\n",
    "print(\"=== EXPORT ===\")\n",
    "start = time.time()\n",
    "result = con.sql(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM read_parquet('{PATHS['export']}')\n",
    "    WHERE sample_identifier = '{SAMPLE_ID}'\n",
    "\"\"\").fetchdf()\n",
    "export_time = (time.time() - start) * 1000\n",
    "print(f\"Time: {export_time:.1f}ms\")\n",
    "print(f\"Columns returned: {len(result.columns)}\")\n",
    "print(result.T)  # Transpose for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZENODO WIDE: Need to JOIN related entities\n",
    "print(\"=== ZENODO WIDE ===\")\n",
    "start = time.time()\n",
    "# This is more complex - would need multiple JOINs to get full picture\n",
    "result = con.sql(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM read_parquet('{PATHS['zenodo_wide']}')\n",
    "    WHERE sample_identifier = '{SAMPLE_ID}'\n",
    "\"\"\").fetchdf()\n",
    "zenodo_wide_detail_time = (time.time() - start) * 1000\n",
    "print(f\"Time: {zenodo_wide_detail_time:.1f}ms\")\n",
    "print(f\"Rows returned: {len(result)}\")\n",
    "if len(result) > 0:\n",
    "    print(f\"Columns returned: {len(result.columns)}\")\n",
    "    print(\"Note: This only returns the sample row, not related entities\")\n",
    "    print(result[['sample_identifier', 'label']].T)\n",
    "else:\n",
    "    print(\"Note: Sample not found (may be from GEOME source, not in this sample_identifier format)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Storage Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File sizes and efficiency\n",
    "def get_file_size_mb(path):\n",
    "    \"\"\"Get file size - returns None for URLs (size unknown without HEAD request).\"\"\"\n",
    "    if path is None:\n",
    "        return None\n",
    "    if isinstance(path, str) and path.startswith('http'):\n",
    "        return None  # Can't easily get URL size\n",
    "    p = Path(path)\n",
    "    if p.exists():\n",
    "        return p.stat().st_size / 1e6\n",
    "    return None\n",
    "\n",
    "storage = []\n",
    "for name in ['export', 'zenodo_narrow', 'zenodo_wide', 'eric_narrow', 'eric_wide']:\n",
    "    path = PATHS.get(name)\n",
    "    if path_available(path):\n",
    "        size_mb = get_file_size_mb(path)\n",
    "        rows = row_counts.get(name, 0)\n",
    "        cols = len(schemas.get(name, []))\n",
    "        bytes_per_row = (size_mb * 1e6) / rows if (size_mb and rows > 0) else None\n",
    "        data_scope = 'Full' if name in ['export', 'zenodo_narrow', 'zenodo_wide'] else 'OC only'\n",
    "        is_remote = isinstance(path, str) and path.startswith('http')\n",
    "        storage.append({\n",
    "            'Format': name.replace('_', ' ').title(),\n",
    "            'Data': data_scope,\n",
    "            'Size (MB)': f'{size_mb:.1f}' if size_mb else 'Remote',\n",
    "            'Rows': f'{rows:,}',\n",
    "            'Columns': cols,\n",
    "            'Bytes/Row': f'{bytes_per_row:.1f}' if bytes_per_row else 'N/A',\n",
    "        })\n",
    "\n",
    "pd.DataFrame(storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Benchmark Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T23:45:11.827621Z",
     "iopub.status.busy": "2025-12-10T23:45:11.827559Z",
     "iopub.status.idle": "2025-12-10T23:45:11.830598Z",
     "shell.execute_reply": "2025-12-10T23:45:11.830215Z"
    }
   },
   "source": [
    "### Benchmark Results Summary\n",
    "\n",
    "**Data Coverage Verification:**\n",
    "- âœ… Export, Zenodo Narrow, Zenodo Wide all contain **6,680,932 samples** from all 4 sources\n",
    "- âœ… Eric's Narrow/Wide contain OpenContext subset (~1.1M samples)\n",
    "\n",
    "| Query Pattern | Best For | SQL Complexity | Notes |\n",
    "|--------------|----------|----------------|-------|\n",
    "| **Map (all coords)** | Export â‰ˆ Zenodo Wide | Simple SELECT | Both ~30ms for 6M points |\n",
    "| **Facets (material counts)** | Export | 1 subquery vs 2 CTEs + JOIN | Export has URIs, PQG has labels |\n",
    "| **Entity listing (agents)** | PQG formats | 0 JOINs (otype filter) | Export requires full scan |\n",
    "| **Reverse lookup by agent** | Export | list_contains() | Only works if agent exists |\n",
    "| **Sample detail (one row)** | Export | Simple WHERE | All data on single row |\n",
    "\n",
    "**Key tradeoffs:**\n",
    "- **Export**: Best for UI (map + facets + detail) but slow for entity listing\n",
    "- **PQG Wide**: Good balance - entities queryable, reasonable JOIN complexity\n",
    "- **PQG Narrow**: Most flexible but slower (92M rows including edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusions: When to Use Each Format\n",
    "\n",
    "### Export Format\n",
    "**Best for:**\n",
    "- UI queries (map, search, facets)\n",
    "- Sample-centric analysis\n",
    "- When you don't need to query entities independently\n",
    "\n",
    "**Avoid when:**\n",
    "- You need to list all agents/sites/concepts\n",
    "- You need graph traversal flexibility\n",
    "- You need incremental updates\n",
    "\n",
    "### Wide Format\n",
    "**Best for:**\n",
    "- Entity-centric queries (\"all agents\", \"all sites\")\n",
    "- Analytical dashboards\n",
    "- When you need both samples AND other entity types\n",
    "\n",
    "**Avoid when:**\n",
    "- Pure sample queries (Export is faster)\n",
    "- Complex multi-hop traversals (Narrow is more natural)\n",
    "\n",
    "### Narrow Format\n",
    "**Best for:**\n",
    "- Archival/preservation (full fidelity)\n",
    "- Graph algorithms\n",
    "- Relationship exploration\n",
    "- When you need to traverse in any direction\n",
    "\n",
    "**Avoid when:**\n",
    "- Interactive UI (too slow)\n",
    "- Simple sample queries (overkill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Insights\n",
    "\n",
    "### What Export Gains\n",
    "1. **No JOINs** - Everything on one row\n",
    "2. **Pre-extracted coords** - `sample_location_latitude/longitude` at top level\n",
    "3. **Fewer rows** - 6.7M vs 19.5M vs 92M\n",
    "\n",
    "### What Export Loses\n",
    "1. **Entity independence** - Can't query agents without scanning all samples\n",
    "2. **Graph flexibility** - Can't traverse in arbitrary directions\n",
    "3. **Incremental updates** - Must regenerate entire file\n",
    "\n",
    "### The `list_contains()` Problem\n",
    "Both Wide (p__* arrays) and Export (nested structs) suffer from O(n) scans when searching within arrays. Neither has index support in DuckDB/Parquet.\n",
    "\n",
    "### Recommendation for Eric's UI\n",
    "For the iSamples Central UI requirements:\n",
    "- **Start with Export format** - fastest for map + facets + click-to-detail\n",
    "- **Pre-compute H3 aggregations** - for initial map render\n",
    "- **Pre-compute facet counts** - avoid runtime aggregation\n",
    "- **Keep Wide/Narrow for advanced queries** - entity exploration, graph traversal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualization with Lonboard\n",
    "\n",
    "Now let's visualize the coordinate data we queried earlier using **Lonboard** - a high-performance WebGL-based mapping library for Jupyter.\n",
    "\n",
    "**Key considerations for 6M+ points:**\n",
    "- Use sampling to avoid memory issues\n",
    "- Color by source collection for insight\n",
    "- Compare visualization speed across formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import visualization libraries\n",
    "try:\n",
    "    from lonboard import Map, ScatterplotLayer\n",
    "    import geopandas as gpd\n",
    "    from shapely.geometry import Point\n",
    "    import numpy as np\n",
    "    LONBOARD_AVAILABLE = True\n",
    "    print(\"âœ… Lonboard available for visualization\")\n",
    "except ImportError as e:\n",
    "    LONBOARD_AVAILABLE = False\n",
    "    print(f\"âš ï¸ Lonboard not available: {e}\")\n",
    "    print(\"   Install with: pip install lonboard geopandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a sample of points from EXPORT format (includes source_collection for coloring)\n",
    "if LONBOARD_AVAILABLE:\n",
    "    SAMPLE_SIZE = 50000  # Adjust based on your system's memory\n",
    "    \n",
    "    print(f\"Querying {SAMPLE_SIZE:,} random samples from Export format...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    # Export has source_collection and pre-extracted coords - ideal for visualization\n",
    "    viz_data = con.sql(f\"\"\"\n",
    "        SELECT \n",
    "            sample_location_longitude as lon,\n",
    "            sample_location_latitude as lat,\n",
    "            source_collection,\n",
    "            sample_identifier\n",
    "        FROM read_parquet('{PATHS['export']}')\n",
    "        WHERE sample_location_latitude IS NOT NULL\n",
    "        USING SAMPLE {SAMPLE_SIZE}\n",
    "    \"\"\").fetchdf()\n",
    "    \n",
    "    query_time = (time.time() - start) * 1000\n",
    "    print(f\"Query time: {query_time:.1f}ms, {len(viz_data):,} points\")\n",
    "    \n",
    "    # Show distribution by source\n",
    "    print(\"\\nSample distribution by source:\")\n",
    "    print(viz_data['source_collection'].value_counts().to_string())\n",
    "else:\n",
    "    print(\"Skipping visualization (Lonboard not available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Lonboard visualization with color by source collection\n",
    "if LONBOARD_AVAILABLE and len(viz_data) > 0:\n",
    "    from IPython.display import display\n",
    "    \n",
    "    # Define colors for each source collection\n",
    "    SOURCE_COLORS = {\n",
    "        'SESAR': [255, 99, 71, 200],      # Tomato red\n",
    "        'OPENCONTEXT': [65, 105, 225, 200], # Royal blue  \n",
    "        'GEOME': [50, 205, 50, 200],       # Lime green\n",
    "        'SMITHSONIAN': [255, 215, 0, 200], # Gold\n",
    "    }\n",
    "    DEFAULT_COLOR = [128, 128, 128, 200]  # Gray for unknown\n",
    "    \n",
    "    # Create geometry from coordinates\n",
    "    geometry = gpd.points_from_xy(viz_data['lon'], viz_data['lat'])\n",
    "    gdf = gpd.GeoDataFrame(viz_data, geometry=geometry, crs=\"EPSG:4326\")\n",
    "    \n",
    "    # Create color array (RGBA as uint8)\n",
    "    colors = np.array([\n",
    "        SOURCE_COLORS.get(src, DEFAULT_COLOR) \n",
    "        for src in viz_data['source_collection']\n",
    "    ], dtype=np.uint8)\n",
    "    \n",
    "    # Create ScatterplotLayer\n",
    "    layer = ScatterplotLayer.from_geopandas(\n",
    "        gdf,\n",
    "        get_fill_color=colors,\n",
    "        get_radius=3000,  # meters\n",
    "        radius_min_pixels=2,\n",
    "        radius_max_pixels=10,\n",
    "        opacity=0.8,\n",
    "        pickable=True,\n",
    "    )\n",
    "    \n",
    "    # Create map\n",
    "    m = Map(layer)\n",
    "    \n",
    "    print(f\"ğŸ—ºï¸ Visualizing {len(gdf):,} points colored by source:\")\n",
    "    for src, color in SOURCE_COLORS.items():\n",
    "        count = (viz_data['source_collection'] == src).sum()\n",
    "        if count > 0:\n",
    "            print(f\"   {src}: {count:,} points\")\n",
    "    \n",
    "    # Display the map explicitly\n",
    "    display(m)\n",
    "else:\n",
    "    print(\"No data to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Enhanced Popup with Sample Details\n",
    "\n",
    "The basic visualization only shows `sample_identifier` and `source_collection`. This enhanced version extracts nested fields so clicking a point shows:\n",
    "- **materials**: Material categories (anthropogenicmetal, rock, etc.)\n",
    "- **context**: Sampled feature context (pasthumanoccupationsite, etc.)\n",
    "- **object_type**: Sample object type\n",
    "- **site_name**: Sampling site name\n",
    "- **keywords**: Associated keywords\n",
    "- **description**: Full sample description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced visualization with full sample details in popup\n",
    "if LONBOARD_AVAILABLE:\n",
    "    from IPython.display import display\n",
    "    \n",
    "    SAMPLE_SIZE = 10000  # Smaller sample for richer data\n",
    "    \n",
    "    print(f'Querying {SAMPLE_SIZE:,} samples with full details...')\n",
    "    \n",
    "    # Enhanced query extracting nested fields for popup display\n",
    "    enhanced_data = con.sql(f\"\"\"\n",
    "        SELECT \n",
    "            sample_identifier,\n",
    "            label,\n",
    "            LEFT(description, 150) as description,\n",
    "            source_collection,\n",
    "            sample_location_latitude as lat,\n",
    "            sample_location_longitude as lon,\n",
    "            -- Extract and join material categories\n",
    "            array_to_string(\n",
    "                list_transform(has_material_category, x -> split_part(x.identifier, '/', -1)),\n",
    "                ', '\n",
    "            ) as materials,\n",
    "            -- Extract context categories\n",
    "            array_to_string(\n",
    "                list_transform(has_context_category, x -> split_part(x.identifier, '/', -1)),\n",
    "                ', '\n",
    "            ) as context,\n",
    "            -- Extract object types\n",
    "            array_to_string(\n",
    "                list_transform(has_sample_object_type, x -> split_part(x.identifier, '/', -1)),\n",
    "                ', '\n",
    "            ) as object_type,\n",
    "            -- Sampling site info\n",
    "            produced_by.sampling_site.label as site_name,\n",
    "            -- Keywords\n",
    "            array_to_string(\n",
    "                list_transform(keywords, x -> x.keyword),\n",
    "                ', '\n",
    "            ) as keywords,\n",
    "            -- Curation\n",
    "            curation.label as curation,\n",
    "            -- Registrant\n",
    "            registrant.name as registrant\n",
    "        FROM read_parquet('{PATHS[\"export\"]}')\n",
    "        WHERE sample_location_latitude IS NOT NULL\n",
    "        USING SAMPLE {SAMPLE_SIZE}\n",
    "    \"\"\").fetchdf()\n",
    "    \n",
    "    print(f'Got {len(enhanced_data):,} samples with full details')\n",
    "    print(f'Columns available in popup: {list(enhanced_data.columns)}')\n",
    "    \n",
    "    # Create geometry\n",
    "    geometry = gpd.points_from_xy(enhanced_data['lon'], enhanced_data['lat'])\n",
    "    enhanced_gdf = gpd.GeoDataFrame(enhanced_data, geometry=geometry, crs='EPSG:4326')\n",
    "    \n",
    "    # Color by source\n",
    "    colors = np.array([\n",
    "        SOURCE_COLORS.get(src, DEFAULT_COLOR) \n",
    "        for src in enhanced_data['source_collection']\n",
    "    ], dtype=np.uint8)\n",
    "    \n",
    "    # Create layer with pickable=True for click popups\n",
    "    enhanced_layer = ScatterplotLayer.from_geopandas(\n",
    "        enhanced_gdf,\n",
    "        get_fill_color=colors,\n",
    "        get_radius=3000,\n",
    "        radius_min_pixels=2,\n",
    "        radius_max_pixels=10,\n",
    "        opacity=0.8,\n",
    "        pickable=True,  # Enable click to see all properties\n",
    "    )\n",
    "    \n",
    "    enhanced_map = Map(enhanced_layer)\n",
    "    \n",
    "    print('\\nğŸ—ºï¸ Click any point to see full sample details!')\n",
    "    print('   Properties shown: label, materials, context, site_name, keywords, etc.')\n",
    "    \n",
    "    display(enhanced_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Visualizing from Wide Format\n",
    "\n",
    "The PQG Wide format stores coordinates in `GeospatialCoordLocation` rows with `otype` filter.\n",
    "The `n` column contains the source collection (named graph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize from WIDE format (uses `n` column for source, `otype` for filtering)\n",
    "if LONBOARD_AVAILABLE:\n",
    "    from IPython.display import display\n",
    "    \n",
    "    print(f\"Querying {SAMPLE_SIZE:,} samples from Wide format...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    # Wide format uses `n` for named graph (source collection) and otype filter\n",
    "    wide_viz_data = con.sql(f\"\"\"\n",
    "        SELECT \n",
    "            longitude as lon,\n",
    "            latitude as lat,\n",
    "            n as source_collection,  -- Named graph contains source\n",
    "            pid as sample_identifier\n",
    "        FROM read_parquet('{PATHS['zenodo_wide']}')\n",
    "        WHERE otype = 'GeospatialCoordLocation' \n",
    "          AND latitude IS NOT NULL\n",
    "        USING SAMPLE {SAMPLE_SIZE}\n",
    "    \"\"\").fetchdf()\n",
    "    \n",
    "    query_time = (time.time() - start) * 1000\n",
    "    print(f\"Query time: {query_time:.1f}ms, {len(wide_viz_data):,} points\")\n",
    "    \n",
    "    # Create geometry and colors\n",
    "    geometry = gpd.points_from_xy(wide_viz_data['lon'], wide_viz_data['lat'])\n",
    "    wide_gdf = gpd.GeoDataFrame(wide_viz_data, geometry=geometry, crs=\"EPSG:4326\")\n",
    "    \n",
    "    colors = np.array([\n",
    "        SOURCE_COLORS.get(src, DEFAULT_COLOR) \n",
    "        for src in wide_viz_data['source_collection']\n",
    "    ], dtype=np.uint8)\n",
    "    \n",
    "    # Create layer and map\n",
    "    wide_layer = ScatterplotLayer.from_geopandas(\n",
    "        wide_gdf,\n",
    "        get_fill_color=colors,\n",
    "        get_radius=3000,\n",
    "        radius_min_pixels=2,\n",
    "        radius_max_pixels=10,\n",
    "        opacity=0.8,\n",
    "        pickable=True,\n",
    "    )\n",
    "    \n",
    "    wide_map = Map(wide_layer)\n",
    "    \n",
    "    print(f\"\\nğŸ—ºï¸ Wide format: {len(wide_gdf):,} points\")\n",
    "    print(wide_viz_data['source_collection'].value_counts().to_string())\n",
    "    \n",
    "    # Display the map explicitly\n",
    "    display(wide_map)\n",
    "else:\n",
    "    print(\"Skipping (Lonboard not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Visualization Tips\n",
    "\n",
    "**Memory Management for 6M+ Points:**\n",
    "- Use `USING SAMPLE N` to limit points (shown above)\n",
    "- Or use `LIMIT` with `ORDER BY RANDOM()` for reproducible sampling\n",
    "- For full dataset: consider H3 hexbin aggregation first\n",
    "\n",
    "**Format Comparison for Visualization:**\n",
    "\n",
    "| Format | Query | Source Color | Notes |\n",
    "|--------|-------|--------------|-------|\n",
    "| **Export** | Direct `lat/lon` columns | `source_collection` | Fastest, simplest |\n",
    "| **Wide** | Filter `otype='GeospatialCoordLocation'` | `n` (named graph) | Slightly slower |\n",
    "| **Narrow** | Same as Wide | Same as Wide | Slowest (most rows) |\n",
    "\n",
    "**Color Scheme Used:**\n",
    "- ğŸ”´ SESAR (geological): Tomato red\n",
    "- ğŸ”µ OPENCONTEXT (archaeological): Royal blue  \n",
    "- ğŸŸ¢ GEOME (biological): Lime green\n",
    "- ğŸŸ¡ SMITHSONIAN (museum): Gold\n",
    "\n",
    "**Next Steps:**\n",
    "- See `geoparquet.ipynb` for more advanced memory-efficient strategies\n",
    "- See `isample-archive.ipynb` for remote parquet visualization patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Browser Visualization with Cesium\n",
    "\n",
    "For web-based 3D globe visualization, use **CesiumJS** with **DuckDB-WASM**. This enables:\n",
    "- No server required - runs entirely in browser\n",
    "- 3D globe with terrain\n",
    "- Click-to-query sample details\n",
    "- Works with remote parquet files via HTTP range requests\n",
    "\n",
    "**Reference implementations:**\n",
    "- `isamplesorg.github.io/tutorials/parquet_cesium_isamples_wide.qmd` - Quarto tutorial with live demo\n",
    "- Remote parquet URLs work directly in browser:\n",
    "  ```javascript\n",
    "  const db = await AsyncDuckDB.create();\n",
    "  await db.open({path: ':memory:'});\n",
    "  const result = await db.query(`\n",
    "    SELECT latitude, longitude, pid\n",
    "    FROM read_parquet('https://pub-a18234d962364c22a50c787b7ca09fa5.r2.dev/isamples_202601_wide.parquet')\n",
    "    WHERE otype = 'GeospatialCoordLocation'\n",
    "    LIMIT 10000\n",
    "  `);\n",
    "  ```\n",
    "\n",
    "**Lonboard vs Cesium:**\n",
    "\n",
    "| Feature | Lonboard (Jupyter) | Cesium (Browser) |\n",
    "|---------|-------------------|------------------|\n",
    "| Environment | Jupyter notebooks | Web pages/Quarto |\n",
    "| Rendering | 2D WebGL | 3D Globe |\n",
    "| Best for | Data exploration | Public demos |\n",
    "| Max points | ~500K comfortable | ~100K with clustering |\n",
    "| Interactivity | Pan/zoom, hover | Click, terrain, 3D |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Focus Sites: PKAP and Poggio Civitate\n",
    "\n",
    "To make the 6M+ sample dataset more tangible, let's explore two well-documented OpenContext archaeological sites:\n",
    "\n",
    "| Site | Location | Coordinates | Scale |\n",
    "|------|----------|-------------|-------|\n",
    "| **PKAP** | Pyla-Koutsopetria, Cyprus | 34.99Â°N, 33.71Â°E | 544 locations, 15K+ events |\n",
    "| **Poggio Civitate** | Murlo, Tuscany, Italy | 43.15Â°N, 11.40Â°E | 11K+ locations, 30K events |\n",
    "\n",
    "These sites demonstrate:\n",
    "- How coordinates cluster around archaeological excavations\n",
    "- The relationship between samples, events, and locations\n",
    "- Real-world query patterns for site-specific analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define focus sites\n",
    "FOCUS_SITES = {\n",
    "    'PKAP': {\n",
    "        'name': 'Pyla-Koutsopetria Archaeological Project',\n",
    "        'location': 'Cyprus',\n",
    "        'lat': 34.987406,\n",
    "        'lon': 33.708047,\n",
    "        'radius_deg': 0.05,  # ~5km bounding box\n",
    "    },\n",
    "    'Poggio': {\n",
    "        'name': 'Poggio Civitate',\n",
    "        'location': 'Murlo, Tuscany, Italy', \n",
    "        'lat': 43.15,\n",
    "        'lon': 11.40,\n",
    "        'radius_deg': 0.1,  # ~10km bounding box\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_site_bbox(site):\n",
    "    \"\"\"Get bounding box for a focus site.\"\"\"\n",
    "    return {\n",
    "        'min_lat': site['lat'] - site['radius_deg'],\n",
    "        'max_lat': site['lat'] + site['radius_deg'],\n",
    "        'min_lon': site['lon'] - site['radius_deg'],\n",
    "        'max_lon': site['lon'] + site['radius_deg'],\n",
    "    }\n",
    "\n",
    "# Display site info\n",
    "for key, site in FOCUS_SITES.items():\n",
    "    bbox = get_site_bbox(site)\n",
    "    print(f\"ğŸ“ {key}: {site['name']}\")\n",
    "    print(f\"   Location: {site['location']}\")\n",
    "    print(f\"   Center: {site['lat']:.4f}Â°N, {site['lon']:.4f}Â°E\")\n",
    "    print(f\"   Bbox: [{bbox['min_lat']:.2f}, {bbox['min_lon']:.2f}] to [{bbox['max_lat']:.2f}, {bbox['max_lon']:.2f}]\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1 Query Site Data from Export Format\n",
    "\n",
    "The Export format makes spatial queries simple - just filter on lat/lon columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query samples from each focus site using Export format\n",
    "site_data = {}\n",
    "\n",
    "for key, site in FOCUS_SITES.items():\n",
    "    bbox = get_site_bbox(site)\n",
    "    \n",
    "    print(f\"=== {key}: {site['name']} ===\")\n",
    "    start = time.time()\n",
    "    \n",
    "    df = con.sql(f\"\"\"\n",
    "        SELECT \n",
    "            sample_identifier,\n",
    "            label,\n",
    "            sample_location_latitude as lat,\n",
    "            sample_location_longitude as lon,\n",
    "            source_collection\n",
    "        FROM read_parquet('{PATHS['export']}')\n",
    "        WHERE sample_location_latitude BETWEEN {bbox['min_lat']} AND {bbox['max_lat']}\n",
    "          AND sample_location_longitude BETWEEN {bbox['min_lon']} AND {bbox['max_lon']}\n",
    "    \"\"\").fetchdf()\n",
    "    \n",
    "    elapsed = (time.time() - start) * 1000\n",
    "    site_data[key] = df\n",
    "    \n",
    "    print(f\"Found {len(df):,} samples in {elapsed:.1f}ms\")\n",
    "    print(f\"Coordinate range: [{df['lat'].min():.4f}, {df['lon'].min():.4f}] to [{df['lat'].max():.4f}, {df['lon'].max():.4f}]\")\n",
    "    print(f\"Unique locations: {df.groupby(['lat', 'lon']).ngroups:,}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.2 Visualize PKAP (Cyprus)\n",
    "\n",
    "Zoomed view of the Pyla-Koutsopetria Archaeological Project survey area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize PKAP site with ENHANCED popup data\nif LONBOARD_AVAILABLE and 'PKAP' in FOCUS_SITES:\n    from IPython.display import display\n\n    site = FOCUS_SITES['PKAP']\n    bbox = get_site_bbox(site)\n\n    # Query with enhanced fields for rich popup\n    pkap_enhanced = con.sql(f\"\"\"\n        SELECT\n            sample_identifier,\n            label,\n            LEFT(description, 150) as description,\n            source_collection,\n            sample_location_latitude as lat,\n            sample_location_longitude as lon,\n            -- Extract material categories\n            array_to_string(\n                list_transform(has_material_category, x -> split_part(x.identifier, '/', -1)),\n                ', '\n            ) as materials,\n            -- Extract context categories\n            array_to_string(\n                list_transform(has_context_category, x -> split_part(x.identifier, '/', -1)),\n                ', '\n            ) as context,\n            -- Extract object types\n            array_to_string(\n                list_transform(has_sample_object_type, x -> split_part(x.identifier, '/', -1)),\n                ', '\n            ) as object_type,\n            -- Sampling site info\n            produced_by.sampling_site.label as site_name,\n            -- Keywords\n            array_to_string(\n                list_transform(keywords, x -> x.keyword),\n                ', '\n            ) as keywords\n        FROM read_parquet('{PATHS['export']}')\n        WHERE sample_location_latitude BETWEEN {bbox['min_lat']} AND {bbox['max_lat']}\n          AND sample_location_longitude BETWEEN {bbox['min_lon']} AND {bbox['max_lon']}\n    \"\"\").fetchdf()\n\n    # Create geometry\n    geometry = gpd.points_from_xy(pkap_enhanced['lon'], pkap_enhanced['lat'])\n    pkap_gdf = gpd.GeoDataFrame(pkap_enhanced, geometry=geometry, crs=\"EPSG:4326\")\n\n    # Single color for site-specific view (blue)\n    colors = np.full((len(pkap_gdf), 4), [65, 105, 225, 200], dtype=np.uint8)\n\n    # Create layer with pickable=True for enhanced popup\n    pkap_layer = ScatterplotLayer.from_geopandas(\n        pkap_gdf,\n        get_fill_color=colors,\n        get_radius=50,  # smaller radius for zoomed view\n        radius_min_pixels=3,\n        radius_max_pixels=8,\n        opacity=0.8,\n        pickable=True,\n    )\n\n    # Create map centered on site\n    pkap_map = Map(pkap_layer)\n    pkap_map.set_view_state(latitude=site['lat'], longitude=site['lon'], zoom=14)\n\n    print(f\"ğŸ—ºï¸ PKAP: {len(pkap_gdf):,} samples at {pkap_gdf.groupby(['lat', 'lon']).ngroups:,} unique locations\")\n    print(f\"   Center: {site['lat']:.4f}Â°N, {site['lon']:.4f}Â°E\")\n    print(f\"   Click any point to see: materials, context, object_type, site_name, keywords\")\n\n    display(pkap_map)\nelse:\n    print(\"PKAP data not available or Lonboard not installed\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.3 Visualize Poggio Civitate (Tuscany)\n",
    "\n",
    "Zoomed view of the Poggio Civitate excavation site in Murlo, Italy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize Poggio Civitate site with ENHANCED popup data\nif LONBOARD_AVAILABLE and 'Poggio' in FOCUS_SITES:\n    from IPython.display import display\n\n    site = FOCUS_SITES['Poggio']\n    bbox = get_site_bbox(site)\n\n    # Query with enhanced fields for rich popup\n    poggio_enhanced = con.sql(f\"\"\"\n        SELECT\n            sample_identifier,\n            label,\n            LEFT(description, 150) as description,\n            source_collection,\n            sample_location_latitude as lat,\n            sample_location_longitude as lon,\n            -- Extract material categories\n            array_to_string(\n                list_transform(has_material_category, x -> split_part(x.identifier, '/', -1)),\n                ', '\n            ) as materials,\n            -- Extract context categories\n            array_to_string(\n                list_transform(has_context_category, x -> split_part(x.identifier, '/', -1)),\n                ', '\n            ) as context,\n            -- Extract object types\n            array_to_string(\n                list_transform(has_sample_object_type, x -> split_part(x.identifier, '/', -1)),\n                ', '\n            ) as object_type,\n            -- Sampling site info\n            produced_by.sampling_site.label as site_name,\n            -- Keywords\n            array_to_string(\n                list_transform(keywords, x -> x.keyword),\n                ', '\n            ) as keywords\n        FROM read_parquet('{PATHS['export']}')\n        WHERE sample_location_latitude BETWEEN {bbox['min_lat']} AND {bbox['max_lat']}\n          AND sample_location_longitude BETWEEN {bbox['min_lon']} AND {bbox['max_lon']}\n    \"\"\").fetchdf()\n\n    # Create geometry\n    geometry = gpd.points_from_xy(poggio_enhanced['lon'], poggio_enhanced['lat'])\n    poggio_gdf = gpd.GeoDataFrame(poggio_enhanced, geometry=geometry, crs=\"EPSG:4326\")\n\n    # Single color for site-specific view (tomato red)\n    colors = np.full((len(poggio_gdf), 4), [255, 99, 71, 200], dtype=np.uint8)\n\n    # Create layer with pickable=True for enhanced popup\n    poggio_layer = ScatterplotLayer.from_geopandas(\n        poggio_gdf,\n        get_fill_color=colors,\n        get_radius=20,  # even smaller for dense site\n        radius_min_pixels=2,\n        radius_max_pixels=6,\n        opacity=0.7,\n        pickable=True,\n    )\n\n    # Create map centered on site\n    poggio_map = Map(poggio_layer)\n    poggio_map.set_view_state(latitude=site['lat'], longitude=site['lon'], zoom=15)\n\n    print(f\"ğŸ—ºï¸ Poggio Civitate: {len(poggio_gdf):,} samples at {poggio_gdf.groupby(['lat', 'lon']).ngroups:,} unique locations\")\n    print(f\"   Center: {site['lat']:.4f}Â°N, {site['lon']:.4f}Â°E\")\n    print(f\"   Click any point to see: materials, context, object_type, site_name, keywords\")\n\n    display(poggio_map)\nelse:\n    print(\"Poggio Civitate data not available or Lonboard not installed\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.4 Site-Specific Material Analysis\n",
    "\n",
    "What materials were found at each site? This demonstrates practical site-level queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Material categories at each site\n",
    "# Load official iSamples vocabulary labels from material_hierarchy.json\n",
    "# Source: https://github.com/isamplesorg/isamples_inabox/blob/develop/isb_web/static/controlled_vocabulary/material_hierarchy.json\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "VOCAB_PATH = Path.home() / 'C/src/iSamples/isamples_inabox/isb_web/static/controlled_vocabulary/material_hierarchy.json'\n",
    "\n",
    "def extract_labels_from_hierarchy(node, result=None):\n",
    "    \"\"\"Recursively extract URI -> label mappings from vocabulary hierarchy.\"\"\"\n",
    "    if result is None:\n",
    "        result = {}\n",
    "    \n",
    "    for uri, data in node.items():\n",
    "        if isinstance(data, dict):\n",
    "            if 'label' in data and 'en' in data['label']:\n",
    "                # Store both 0.9 and 1.0 versions (data uses 1.0)\n",
    "                uri_1_0 = uri.replace('/0.9/', '/1.0/')\n",
    "                result[uri_1_0] = data['label']['en']\n",
    "                result[uri] = data['label']['en']\n",
    "            if 'children' in data:\n",
    "                for child in data['children']:\n",
    "                    extract_labels_from_hierarchy(child, result)\n",
    "    return result\n",
    "\n",
    "# Load vocabulary and build lookup\n",
    "if VOCAB_PATH.exists():\n",
    "    with open(VOCAB_PATH) as f:\n",
    "        vocab_hierarchy = json.load(f)\n",
    "    URI_TO_LABEL = extract_labels_from_hierarchy(vocab_hierarchy)\n",
    "    print(f\"Loaded {len(URI_TO_LABEL)} material labels from vocabulary file\")\n",
    "else:\n",
    "    print(f\"âš ï¸ Vocabulary file not found: {VOCAB_PATH}\")\n",
    "    URI_TO_LABEL = {}\n",
    "\n",
    "def get_material_label(uri):\n",
    "    \"\"\"Get human-readable label for a material URI.\"\"\"\n",
    "    return URI_TO_LABEL.get(uri, uri.split('/')[-1])\n",
    "\n",
    "# Query and display materials for each site\n",
    "for key, site in FOCUS_SITES.items():\n",
    "    bbox = get_site_bbox(site)\n",
    "    \n",
    "    print(f\"\\n=== {key}: Material Categories ===\")\n",
    "    \n",
    "    result = con.sql(f\"\"\"\n",
    "        SELECT \n",
    "            mat.identifier as uri,\n",
    "            COUNT(*) as cnt\n",
    "        FROM (\n",
    "            SELECT unnest(has_material_category) as mat\n",
    "            FROM read_parquet('{PATHS['export']}')\n",
    "            WHERE sample_location_latitude BETWEEN {bbox['min_lat']} AND {bbox['max_lat']}\n",
    "              AND sample_location_longitude BETWEEN {bbox['min_lon']} AND {bbox['max_lon']}\n",
    "              AND has_material_category IS NOT NULL\n",
    "        )\n",
    "        GROUP BY mat.identifier\n",
    "        ORDER BY cnt DESC\n",
    "        LIMIT 8\n",
    "    \"\"\").fetchdf()\n",
    "    \n",
    "    # Add friendly label from vocabulary\n",
    "    result['material'] = result['uri'].apply(get_material_label)\n",
    "    \n",
    "    # Display with friendly labels\n",
    "    print(result[['material', 'cnt']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.5 Data Format Question: Should PQG Include Labels?\n",
    "\n",
    "**Current state:**\n",
    "- **Export format**: `has_material_category` only contains `identifier` (URI), no label\n",
    "- **Zenodo Wide/Narrow**: `IdentifiedConcept.label` = URI (not human-readable)\n",
    "- **Eric's Wide**: `IdentifiedConcept.label` = human-readable (vocabulary lookup applied)\n",
    "\n",
    "**The question:** Should Zenodo Wide/Narrow `IdentifiedConcept` rows include:\n",
    "1. Just the URI (current) - requires external vocabulary lookup\n",
    "2. Just the label - loses precise identifier\n",
    "3. Both URI and label - redundant but self-contained\n",
    "\n",
    "**Tradeoffs:**\n",
    "\n",
    "| Approach | File Size | Query Simplicity | Vocabulary Updates |\n",
    "|----------|-----------|------------------|-------------------|\n",
    "| URI only | Smaller | Need JOIN to vocab | Easy to re-label |\n",
    "| Label only | Smaller | Direct display | Stuck with old labels |\n",
    "| Both | Larger | Best of both | Must regenerate |\n",
    "\n",
    "**Recommendation:** Include both `pid` (URI) and `label` (human-readable) in `IdentifiedConcept` rows. The ~50K concept rows are tiny compared to 6M+ samples, so the size increase is negligible.\n",
    "\n",
    "**Vocabulary source:** \n",
    "- Local: `~/C/src/iSamples/isamples_inabox/isb_web/static/controlled_vocabulary/material_hierarchy.json`\n",
    "- GitHub: https://github.com/isamplesorg/isamples_inabox/blob/develop/isb_web/static/controlled_vocabulary/material_hierarchy.json"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 12. Eric's OpenContext vs Zenodo: Data Quality Analysis\n\nEric's OpenContext parquet files were generated directly from OpenContext data, while Zenodo files were exported via the iSamples Central API. This section compares the two to understand what data was lost or gained in each conversion pipeline.\n\n**Key question**: Can we merge the best of both sources?\n\nSee [pqg issue #13](https://github.com/isamplesorg/pqg/issues/13) for the full analysis and recommendations.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Compare Eric's OpenContext files with Zenodo (filtered to OpenContext)\nprint(\"=\" * 70)\nprint(\"ERIC vs ZENODO: Row Counts (Zenodo filtered to n='OPENCONTEXT')\")\nprint(\"=\" * 70)\n\ncomparison_data = []\nfor name in ['eric_wide', 'zenodo_wide', 'eric_narrow', 'zenodo_narrow']:\n    path = PATHS.get(name)\n    if path_available(path):\n        if 'zenodo' in name:\n            # Filter to OpenContext only for fair comparison\n            count = con.sql(f\"\"\"\n                SELECT COUNT(*) FROM read_parquet('{path}')\n                WHERE n = 'OPENCONTEXT'\n            \"\"\").fetchone()[0]\n        else:\n            count = con.sql(f\"SELECT COUNT(*) FROM read_parquet('{path}')\").fetchone()[0]\n        comparison_data.append({'Format': name, 'Rows': count})\n        print(f\"{name}: {count:,} rows\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"ENTITY TYPE COMPARISON (Wide Format)\")\nprint(\"=\" * 70)\n\n# Compare otypes between Eric and Zenodo\neric_otypes = con.sql(f\"\"\"\n    SELECT otype, COUNT(*) as eric_cnt\n    FROM read_parquet('{PATHS['eric_wide']}')\n    GROUP BY otype\n\"\"\").fetchdf()\n\nzenodo_otypes = con.sql(f\"\"\"\n    SELECT otype, COUNT(*) as zenodo_cnt\n    FROM read_parquet('{PATHS['zenodo_wide']}')\n    WHERE n = 'OPENCONTEXT'\n    GROUP BY otype\n\"\"\").fetchdf()\n\n# Merge and compare\notype_comparison = eric_otypes.merge(zenodo_otypes, on='otype', how='outer').fillna(0)\notype_comparison['eric_cnt'] = otype_comparison['eric_cnt'].astype(int)\notype_comparison['zenodo_cnt'] = otype_comparison['zenodo_cnt'].astype(int)\notype_comparison['diff'] = otype_comparison['eric_cnt'] - otype_comparison['zenodo_cnt']\notype_comparison = otype_comparison.sort_values('eric_cnt', ascending=False)\nprint(otype_comparison.to_string(index=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Key field population comparison\nprint(\"=\" * 70)\nprint(\"KEY FIELD POPULATION: Eric vs Zenodo (OpenContext)\")\nprint(\"=\" * 70)\n\n# Project field (SamplingEvent)\nprint(\"\\n--- Project field (SamplingEvent) ---\")\neric_projects = con.sql(f\"\"\"\n    SELECT project, COUNT(*) as cnt\n    FROM read_parquet('{PATHS['eric_wide']}')\n    WHERE otype = 'SamplingEvent' AND project IS NOT NULL\n    GROUP BY project ORDER BY cnt DESC LIMIT 5\n\"\"\").fetchdf()\nprint(\"Eric (top 5 projects):\")\nprint(eric_projects.to_string(index=False))\n\nzenodo_project_null = con.sql(f\"\"\"\n    SELECT COUNT(*) FROM read_parquet('{PATHS['zenodo_wide']}')\n    WHERE otype = 'SamplingEvent' AND n = 'OPENCONTEXT' AND project IS NULL\n\"\"\").fetchone()[0]\nzenodo_project_total = con.sql(f\"\"\"\n    SELECT COUNT(*) FROM read_parquet('{PATHS['zenodo_wide']}')\n    WHERE otype = 'SamplingEvent' AND n = 'OPENCONTEXT'\n\"\"\").fetchone()[0]\nprint(f\"\\nZenodo: {zenodo_project_null:,} / {zenodo_project_total:,} have NULL project ({100*zenodo_project_null/zenodo_project_total:.0f}%)\")\n\n# Coordinates on MaterialSampleRecord\nprint(\"\\n--- Coordinates on MaterialSampleRecord ---\")\neric_coords = con.sql(f\"\"\"\n    SELECT COUNT(*) FROM read_parquet('{PATHS['eric_wide']}')\n    WHERE otype = 'MaterialSampleRecord' AND latitude IS NOT NULL \n      AND CAST(latitude AS VARCHAR) != 'nan'\n\"\"\").fetchone()[0]\neric_total = con.sql(f\"\"\"\n    SELECT COUNT(*) FROM read_parquet('{PATHS['eric_wide']}')\n    WHERE otype = 'MaterialSampleRecord'\n\"\"\").fetchone()[0]\nprint(f\"Eric: {eric_coords:,} / {eric_total:,} have coordinates ({100*eric_coords/eric_total:.1f}%)\")\n\nzenodo_coords = con.sql(f\"\"\"\n    SELECT COUNT(*) FROM read_parquet('{PATHS['zenodo_wide']}')\n    WHERE otype = 'MaterialSampleRecord' AND n = 'OPENCONTEXT' \n      AND latitude IS NOT NULL\n\"\"\").fetchone()[0]\nzenodo_total = con.sql(f\"\"\"\n    SELECT COUNT(*) FROM read_parquet('{PATHS['zenodo_wide']}')\n    WHERE otype = 'MaterialSampleRecord' AND n = 'OPENCONTEXT'\n\"\"\").fetchone()[0]\nprint(f\"Zenodo: {zenodo_coords:,} / {zenodo_total:,} have coordinates ({100*zenodo_coords/zenodo_total:.1f}%)\")\n\n# IdentifiedConcept comparison\nprint(\"\\n--- IdentifiedConcept labels ---\")\nprint(\"Eric (domain-specific labels):\")\neric_concepts = con.sql(f\"\"\"\n    SELECT label, COUNT(*) as cnt\n    FROM read_parquet('{PATHS['eric_wide']}')\n    WHERE otype = 'IdentifiedConcept'\n    GROUP BY label ORDER BY cnt DESC LIMIT 5\n\"\"\").fetchdf()\nprint(eric_concepts.to_string(index=False))\n\nprint(\"\\nZenodo (URI-based labels):\")\nzenodo_concepts = con.sql(f\"\"\"\n    SELECT label, COUNT(*) as cnt\n    FROM read_parquet('{PATHS['zenodo_wide']}')\n    WHERE otype = 'IdentifiedConcept'\n    GROUP BY label ORDER BY cnt DESC LIMIT 5\n\"\"\").fetchdf()\nprint(zenodo_concepts.to_string(index=False) if len(zenodo_concepts) > 0 else \"(no OC-specific concepts)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 12.1 Summary: Eric vs Zenodo\n\n| Aspect | Eric's Files | Zenodo Files |\n|--------|--------------|--------------|\n| **MaterialSampleRecord** | 1,110,412 | 1,064,831 |\n| **SamplingEvent** | 1,110,412 | 1,059,103 |\n| **GeospatialCoordLocation** | 199,147 | 1,059,025 |\n| **IdentifiedConcept** | 25,929 | 0 (OC-specific) |\n| **Agent** | 577 | 0 |\n\n**Field Population:**\n\n| Field | Eric | Zenodo |\n|-------|------|--------|\n| `project` (SamplingEvent) | 100% populated | 0% (all NULL) |\n| Coords on MaterialSampleRecord | 0% | 99.5% |\n| `last_modified_time` | 100% | 0% |\n| `alternate_identifiers` | 100% | 0% |\n\n**Key Differences:**\n\n1. **Coordinate Strategy**: Eric stores coords only on GeospatialCoordLocation (~5.8 samples share each location). Zenodo propagates coords to MaterialSampleRecord (1:1 mapping).\n\n2. **Project Data**: Eric preserves project names (\"Ã‡atalhÃ¶yÃ¼k Zooarchaeology\", \"Petra Great Temple\"). Zenodo lost this during Central API export.\n\n3. **IdentifiedConcept**: Eric has domain-specific labels (\"Element :: Scapula\"). Zenodo has URI strings.\n\n**Recommendation**: Merge the best of both - use Eric's project/concept/agent data with Zenodo's coordinate propagation.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. PQG Library: Querying with Narrow Format\n",
    "\n",
    "The pqg library provides **full query support for narrow format** files:\n",
    "- `PQG` class: Load graph, traverse relations, get nodes\n",
    "- `TypedEdgeQueries`: Query edges by the 14 edge types\n",
    "- Edge type utilities: Inference, discovery, validation\n",
    "\n",
    "**Narrow vs Wide Format**:\n",
    "| Feature | Narrow | Wide |\n",
    "|---------|--------|------|\n",
    "| Edge storage | Explicit `_edge_` rows | `p__*` columns |\n",
    "| File size | ~844 MB | ~282 MB |\n",
    "| PQG query support | Full | Validation only |\n",
    "| Best for | Graph traversal | Entity analytics |\n",
    "\n",
    "See [pqg issue #11](https://github.com/isamplesorg/pqg/issues/11) for unified API proposal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add pqg library to path\n",
    "# NOTE: pqg requires linkml-runtime. Install with: pip install -e ~/C/src/iSamples/pqg\n",
    "import sys\n",
    "sys.path.insert(0, str(Path.home() / 'C/src/iSamples/pqg'))\n",
    "\n",
    "try:\n",
    "    from pqg import PQG, ISamplesEdgeType, EDGE_TYPE_CONSTRAINTS\n",
    "    from pqg import get_edge_types_by_subject, get_edge_types_by_object, infer_edge_type\n",
    "    from pqg.typed_edges import TypedEdgeQueries\n",
    "    from pqg.schemas import get_schema_from_parquet, SchemaFormat\n",
    "    PQG_AVAILABLE = True\n",
    "    print(f'Loaded pqg with {len(list(ISamplesEdgeType))} edge types')\n",
    "except ImportError as e:\n",
    "    PQG_AVAILABLE = False\n",
    "    print(f'pqg library not available: {e}')\n",
    "    print('To fix: pip install -e ~/C/src/iSamples/pqg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the NARROW format parquet file into PQG\n",
    "if not PQG_AVAILABLE:\n",
    "    print('PQG not available; install pqg first')\n",
    "else:\n",
    "    import duckdb\n",
    "    from pathlib import Path\n",
    "    \n",
    "    narrow_path = PATHS.get('zenodo_narrow')\n",
    "    narrow_path = Path(narrow_path) if narrow_path else None\n",
    "    if narrow_path and narrow_path.exists():\n",
    "        # Detect format first (pqg expects a string path/URL)\n",
    "        schema, detected_format = get_schema_from_parquet(str(narrow_path))\n",
    "        print(f'File: {narrow_path.name}')\n",
    "        print(f'Detected format: {detected_format.value}')\n",
    "        \n",
    "        # Load into PQG (requires connection + source path)\n",
    "        pqg_con = duckdb.connect()\n",
    "        graph = PQG(pqg_con, str(narrow_path))\n",
    "        \n",
    "        if hasattr(graph, 'loadMetadata'):\n",
    "            graph.loadMetadata()\n",
    "        elif hasattr(graph, 'load_metadata'):\n",
    "            graph.load_metadata()\n",
    "        \n",
    "        print('\\nGraph loaded!')\n",
    "        types = getattr(graph, '_types', {})\n",
    "        print(f\"Entity types: {list(types.keys()) or '(loaded from parquet metadata)'}\")\n",
    "    else:\n",
    "        print(f'Narrow format file not found locally: {narrow_path}')\n",
    "        print('Download from:', URLS.get('zenodo_narrow'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 14 edge types in iSamples schema\n",
    "if PQG_AVAILABLE:\n",
    "    print('THE 14 ISAMPLES EDGE TYPES')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    current_subject = None\n",
    "    for i, et in enumerate(ISamplesEdgeType, 1):\n",
    "        if et.subject_type != current_subject:\n",
    "            current_subject = et.subject_type\n",
    "            print(f'\\n{current_subject}:')\n",
    "        \n",
    "        constraints = EDGE_TYPE_CONSTRAINTS.get(et.value, {})\n",
    "        multivalued = '*' if constraints.get('multivalued', False) else '1'\n",
    "        print(f'  {i:2d}. --{et.predicate}-->[{multivalued}] {et.object_type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer edge type from subject-predicate-object triple\n",
    "if PQG_AVAILABLE:\n",
    "    print('EDGE TYPE INFERENCE')\n",
    "    print('-' * 50)\n",
    "    \n",
    "    test_cases = [\n",
    "        ('MaterialSampleRecord', 'produced_by', 'SamplingEvent'),\n",
    "        ('SamplingEvent', 'sample_location', 'GeospatialCoordLocation'),\n",
    "        ('SamplingEvent', 'responsibility', 'Agent'),\n",
    "        ('MaterialSampleCuration', 'responsibility', 'Agent'),  # Same predicate!\n",
    "    ]\n",
    "    \n",
    "    for s, p, o in test_cases:\n",
    "        edge_type = infer_edge_type(s, p, o)\n",
    "        print(f'({s}, {p}, {o})')\n",
    "        print(f'  -> {edge_type.name if edge_type else \"None\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find edge types by subject or object type\n",
    "if PQG_AVAILABLE:\n",
    "    print('EDGE TYPES BY SUBJECT')\n",
    "    print('-' * 50)\n",
    "    \n",
    "    for subject in ['MaterialSampleRecord', 'SamplingEvent']:\n",
    "        types = get_edge_types_by_subject(subject)\n",
    "        print(f'\\n{subject} ({len(types)} edges):')\n",
    "        for et in types:\n",
    "            print(f'  -> {et.predicate} -> {et.object_type}')\n",
    "    \n",
    "    print('\\n' + '=' * 50)\n",
    "    print('EDGE TYPES BY OBJECT')\n",
    "    print('-' * 50)\n",
    "    \n",
    "    for obj in ['Agent', 'GeospatialCoordLocation']:\n",
    "        types = get_edge_types_by_object(obj)\n",
    "        print(f'\\n{obj} ({len(types)} incoming edges):')\n",
    "        for et in types:\n",
    "            print(f'  <- {et.subject_type} via {et.predicate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PQG.getRelations() to traverse the graph\n",
    "if PQG_AVAILABLE and 'graph' in dir():\n",
    "    print('GRAPH TRAVERSAL (PQG.getRelations)')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    # Sample relations\n",
    "    print('\\nSample relations (first 10):')\n",
    "    for i, (s, p, o) in enumerate(graph.getRelations(maxrows=10)):\n",
    "        print(f'  {s[:35]}... --{p}-> {o[:35]}...')\n",
    "    \n",
    "    # Filter by predicate\n",
    "    print('\\nRelations with predicate \"produced_by\" (first 5):')\n",
    "    for i, (s, p, o) in enumerate(graph.getRelations(predicate='produced_by', maxrows=5)):\n",
    "        print(f'  {s[:40]} -> {o[:40]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TypedEdgeQueries to query by edge type\n",
    "if PQG_AVAILABLE and 'graph' in dir():\n",
    "    print('TYPED EDGE QUERIES')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    typed = TypedEdgeQueries(graph)\n",
    "    \n",
    "    # Sample edges of specific types\n",
    "    for edge_type in [ISamplesEdgeType.MSR_PRODUCED_BY, \n",
    "                      ISamplesEdgeType.EVENT_SAMPLE_LOCATION]:\n",
    "        print(f'\\n{edge_type.name}:')\n",
    "        count = 0\n",
    "        for s_pid, p, o_pids, n, et in typed.get_edges_by_type(edge_type, limit=3):\n",
    "            print(f'  {s_pid[:40]} -> {o_pids[0][:30] if o_pids else \"?\"}...')\n",
    "            count += 1\n",
    "        print(f'  (showing {count} of many)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary: pqg API capabilities\n",
    "if PQG_AVAILABLE:\n",
    "    print('PQG LIBRARY SUMMARY')\n",
    "    print('=' * 60)\n",
    "    print('''\n",
    "Format Detection:\n",
    "  get_schema_from_parquet(path) -> (schema, format)\n",
    "\n",
    "Narrow Format Query APIs:\n",
    "  PQG class:\n",
    "    - getNode(pid) -> dict\n",
    "    - getRelations(predicate=None, maxrows=0) -> Iterator\n",
    "    - getNodeIds(pid) -> Set[str]\n",
    "  \n",
    "  TypedEdgeQueries:\n",
    "    - get_edges_by_type(edge_type, limit) -> Iterator\n",
    "    - get_typed_relations() -> Iterator\n",
    "    - get_edge_type_statistics() -> List[(EdgeType, count)]\n",
    "\n",
    "Edge Type Utilities (format-independent):\n",
    "  - ISamplesEdgeType enum: 14 edge types\n",
    "  - infer_edge_type(s, p, o) -> EdgeType\n",
    "  - get_edge_types_by_subject(otype) -> List[EdgeType]\n",
    "  - get_edge_types_by_object(otype) -> List[EdgeType]\n",
    "\n",
    "Wide Format: Validation only (see issue #11 for unified API)\n",
    "''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isamples-python-3.12.9",
   "language": "python",
   "name": "isamples-python-3.12.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}